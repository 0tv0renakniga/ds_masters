
\documentclass{article}

% Required packages
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{tikz}
\usepackage{array}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{subcaption}

% Set page geometry
\geometry{a4paper, margin=1in}

% Configure listings for Python
\lstset{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  numbers=left,
  numberstyle=\tiny\color{gray},
  frame=single,
  breaklines=true,
  breakatwhitespace=true,
  captionpos=b,
  tabsize=4,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  commentstyle=\color{gray}\textit,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{red}
}

\begin{document}

\pagestyle{fancy}
\chead{DSC 255: Machine Learning Fundamentals (Spring 2025)}
\lhead{Homework 7}
\rhead{Randall Rogers}

\subsection*{Solution 1}
\noindent\rule{\textwidth}{0.4pt}\\

% -----------------------------------------------------------------
\subsubsection*{Step 1}
\parbox{\textwidth}{
The general form for a linear predictor is the following:

$$f(x) = w^{T}x + b$$\\

We are given three linear functions for each class:

$$\text{Class 1: } w_1=(1,1), b_1 = 0 \rightarrow f_{1}(x) = x_{1}\cdot(w^1_1)+x_{2}\cdot(w^1_2)+b_1 = x_{1}\cdot(1)+x_{2}\cdot(1)+0 = x_{1} + x_{2} $$\\
$$\text{Class 2: } w_2=(1,0), b_2 = 1 \rightarrow f_{2}(x) = x_{1}\cdot(w^2_1)+x_{2}\cdot(w^2_2)+b_2 = x_{1}\cdot(1)+x_{2}\cdot(0)+1 = x_{1} + 1 $$\\
$$\text{Class 3: } w_3=(0,1), b_3 = -1 \rightarrow f_{3}(x) = x_{1}\cdot(w^3_1)+x_{2}\cdot(w^3_2)+b_3 = x_{2}\cdot(0)+x_{1}\cdot(1)-1 = x_{2} - 1$$\\

Hence, for each class, we have the following:

$$
\begin{aligned}
f_{1}(x) &= x_{1}+x_{2},\\
f_{2}(x) &= x_{1}+1,\\
f_{3}(x) &= x_{2}-1.
\end{aligned}
$$
}
% -----------------------------------------------------------------
\subsubsection*{Step 2}
\parbox{\textwidth}{
Compare $f_{1}$, $f_{2}$, and $f_{3}$ at the intersection points of the decison boundaries:

\begin{center}
$$
\begin{aligned}
\text{(1 vs 2)} &: \; f_{1}=f_{2} \;\rightarrow\; x_{1}+x_{2} = x_1 + 1 \;\rightarrow\; x_{2} = 1 \;\rightarrow\; (0,1)\\
\text{(1 vs 3)} &: \; f_{1}=f_{2} \;\rightarrow\; x_{1}+x_{2} = x_2 - 1 \;\rightarrow\; x_{1} = -1 \;\rightarrow\; (-1,0) \\
\text{(2 vs 3)} &: \; f_{2}=f_{3} \;\rightarrow\; x_{1}+1 = x_{2}-1 \;\rightarrow\; x_{2} = x_{1}+2 \;\rightarrow\; (0,2) \text{ and }  (-2,0)\\
\end{aligned}
$$
\end{center}

Hence, we have the following decision boundaries:
\begin{center}
$$
\begin{aligned}
\text{Decision Boundary between Class 1 and Class 2: }& \quad x_{2}=1 \\
\text{Decision Boundary between Class 1 and Class 3: }& \quad x_{1}=-1 \\
\text{Decision Boundary between Class 2 and Class 3: }& \quad x_{2}=x_{1}+2 
\end{aligned}
$$
\end{center}
}

\subsubsection*{Step 3}
\parbox{\textwidth}{
Note: \,$x_{2}=1$, \,$x_{1}=-1$, and \,$x_{2}=x_{1}+2$ all pass
through the common point $(-1,1)$,and slice the plane into 6 wedges.\\

Now, select six test points for each wedge, and evaluate $f_1$, $f_2$, and $f_3$ to determine the classification.\\

$$
\begin{array}{c|c|c|c|c|c}
\text{Wedge label} & \text{Test point} & f_{1}(x) & f_{2}(x) & f_{3}(x) & \text{Classification}\\ \hline
A & ( 1, 4)  &  5 &  2 &  3 & \text{Class 1}\\ %  x_{1}>-1, x_{2}>1,  below red
B & ( 1, 2)  &  3 &  2 &  1 & \text{Class 1} \\ %  x_{1}>-1, 1<x_{2}<x_{1}+2
C & ( 1, -1) &  0 &  2 & -2 & \text{Class 2}\\ %  x_{1}>-1, x_{2}<1
D & (-2,-1)  & -4 & -1 & -3 & \text{Class 2}\\ %  x_{1}<-1, x_{2}<1, below red
E & (-3, 0)  & -3 & -2 & -1 & \text{Class 3}\\ % x_{1}<-1, x_{2}<1,
F & (-3, 3)  &  0 & -2 &  2 & \text{Class 3}\\ %  x_{1}<-1, x_{2}>1
\end{array}
$$

}

\subsubsection*{Step 4}
\parbox{\textwidth}{
Visualize points and classify each wedge.
}
\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=0.8]

% Axes
\draw[->] (-6,0) -- (4,0) node[right] {$x_{1}$};
\draw[->] (0,-6) -- (0,5) node[above] {$x_{2}$};

% Decision boundaries
\draw[blue,thick] (-6,1) -- (4,1)   node[right] {$x_{2}=1$};          % 1 vs 2
\draw[green!50!black,thick] (-1,-5) -- (-1,5) node[above left] {$x_{1}=-1$}; % 1 vs 3
\draw[red,thick] (-6,-4) -- (3,5)   node[above] {$x_{2}=x_{1}+2$};     % 2 vs 3

% Intersection point
\fill (-1,1) circle (0.05);

% Class labels in each wedge
\node at ( 2.4, 4) {\large (A) Class 1};
\node at ( 2.4, 2) {\large (B) Class 1};

\node at ( 2.4,-1) {\large (C) Class 2};
\node at (-2.5,-3) {\large (D) Class 2};

\node at (-3.4, 0.5) {\large (E) Class 3};
\node at (-3.4, 3.5) {\large (F) Class 3};

% (Optional) test points used to verify labels
\fill ( 1,4)   circle (0.06);
\fill ( 1,2) circle (0.06);
\fill ( 1,-1)   circle (0.06);
\fill (-2,-2)  circle (0.06);
\fill (-3,0) circle (0.06);
\fill (-3,3)   circle (0.06);

\end{tikzpicture}
\caption{All six wedges produced by the three pairwise boundaries, with the winning class shown in each.}
\end{figure}

% -----------------------------------------------------------------
\subsubsection*{\normalfont}
{
$\therefore$ the decision boundaries and the regions that belong to each class can be seen in \textit{Figure 1}.
}\\
\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 2}
\noindent\rule{\textwidth}{0.4pt}\\

% -----------------------------------------------------------------
\subsubsection*{Step 1}
\parbox{\textwidth}{
In order to determine which option will best satisfy the statistical learning framework, we will evaluate each option.
\begin{itemize}
    \item Option A: A web search on \textit{American plants} will yeild subset of plant images outside of California.
    \item Option B: Plant photos from a similar region to that of California, will yeild similar flora but not identiencal to Californian flora.
    \item Option C: Plant photos from the favorite Californian city will yeild photos of plants will have a higher bias towards urban Californian flora.
\end{itemize} 
}
\subsubsection*{\normalfont}{$\therefore$ Option B will best satisfy the statistical learning framework, since it will introduce the least amount of bias towards Californian plant identification.}\\

\noindent\rule{\textwidth}{0.4pt}\\

\newpage
\subsection*{Solution 3}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}
\parbox{\textwidth}{
We will explore a few potential reasons why the SVM classifier performs well despite the large number of features.
\begin{itemize}
\item If the classes the SVM is predicting are sufficently different, the SVM will be able to find a good decision boundary that separates the data.
      As a consquence of the difference between the classes, the SVM will have a large margin. This means that the SVM can find a good decision boundary even with a large number of features and minimal training points.

\item The generalizaton error can be  expressed as:
      \[
        \text{generalisation error} \;\approx\; 
        \sqrt{\dfrac{R^{2}/\gamma^{2}}{n}}
      \]
      So we can see that as the margin grows largers, the generalization error decreases. This means that the SVM can find a good decision boundary even with a large number of features and minimal training points.

\end{itemize}
}

\subsubsection*{\normalfont}{$\therefore$ a wide margin for the SVM is a key reason the 1000 traing points are sufficent reguardless of the one million features in the data.}

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 4 (a)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{\normalfont}{The class priors changed: fewer \textit{sports} articles, more \textit{politics} Within each topic (class), the way the text is written is assumed unchanged, so only the frequency of each label moves.
}
\subsubsection*{\normalfont}{$\therefore$ this would be an example of a label shift}

\noindent\rule{\textwidth}{0.4pt}\\

\subsection*{Solution 4 (b)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{\normalfont}{The vocabulary has drifted: new proper nouns appear and old ones vanish. This alters the distribution of the features within each topic, while the overall mix of topics (sports, politics, business,etc.) stays the same.}

\subsubsection*{\normalfont}{$\therefore$ this is an example of a covariate shift}

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 5}
\noindent\rule{\textwidth}{0.4pt}\\


\subsubsection*{Python Code}

\begin{lstlisting}
## import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils import shuffle

## load data0.txt
data = np.loadtxt('data0.txt', dtype=int)
x = data[:, :2]
y = data[:, 2]

## get number of unique classes
k = len(np.unique(y))

## define multi_perceptron function
def multi_perceptron(w, b, x):
    predictions = np.dot(w, x) + b
    best_prediction = predictions.argmax()
    return(best_prediction)
    
## define fit_multi_perceptron function
def fit_multi_perceptron(x, y, k,track_updates, set_seed):
    if set_seed:
        x, y = shuffle(x, y,random_state=42)
    else:
        x, y = shuffle(x, y)
    w = np.zeros((k, x.shape[1]))
    b = np.zeros(k)
    updates = 0
    max_updates = 1000

    def make_prediction(w, b, x, y, updates):
        error = False    
        for xi, yi in zip(x, y):
            predicted_y = multi_perceptron(w, b, xi) 
            if predicted_y != yi:
                w[yi] += xi
                b[yi] += 1
                w[predicted_y] -= xi
                b[predicted_y] -= 1
                updates += 1
                error = True  
        if updates <= max_updates:
            if error:
                return make_prediction(w, b, x, y, updates) 
            else:
                return (w, b, updates)
        else:
            print("Did not converge after {max_iterations} iterations")
    if track_updates:
        w, b, updates = make_prediction(w, b, x, y, updates)
        return (w, b, updates)
    else:
        w, b, updates = make_prediction(w, b, x, y, updates)
        return (w, b)
\end{lstlisting}
\subsubsection*{Plots}
\begin{figure}[H]
\includegraphics[width=1\textwidth]{hw7_5_b.png} 
\caption{Multiclass Perceptron decision boundaries.}
\end{figure}
\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 6}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Plots}
\begin{figure}[H]
\includegraphics[width=1\textwidth]{hw7_6_a.png} 
\caption{SVM C value comparison.}
\end{figure}
\subsubsection*{C Value Comparison Comments}
\parbox{\textwidth}{
  Observations across C values:
  \begin{itemize}
    \item As C increases the model partitions the data more effectively
    \item Since there are a limited amount of features in this dataset (\textit{data0.txt}), we don't have to worry about minimizing the margin, since the computational cost won't skyrocket with this dataset.
  \end{itemize}
}
\noindent\rule{\textwidth}{0.4pt}\\


\end{document}


