{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Neural net experiments</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss 0.61841 errors 16\n",
      "Iteration 2000: loss 0.61774 errors 16\n",
      "Iteration 3000: loss 0.61751 errors 16\n",
      "Iteration 4000: loss 0.61740 errors 16\n",
      "Iteration 5000: loss 0.61734 errors 16\n",
      "Iteration 1000: loss 0.39306 errors 12\n",
      "Iteration 2000: loss 0.38810 errors 12\n",
      "Iteration 3000: loss 0.38611 errors 12\n",
      "Iteration 4000: loss 0.38494 errors 12\n",
      "Iteration 5000: loss 0.38414 errors 11\n",
      "Iteration 6000: loss 0.38357 errors 12\n",
      "Iteration 7000: loss 0.38313 errors 12\n",
      "Iteration 8000: loss 0.38283 errors 12\n",
      "Iteration 9000: loss 0.38261 errors 12\n",
      "Iteration 10000: loss 0.38244 errors 12\n",
      "Iteration 11000: loss 0.38232 errors 12\n",
      "Iteration 12000: loss 0.38223 errors 12\n",
      "Iteration 1000: loss 0.52880 errors 16\n",
      "Iteration 2000: loss 0.52534 errors 16\n",
      "Iteration 3000: loss 0.52391 errors 16\n",
      "Iteration 4000: loss 0.52305 errors 16\n",
      "Iteration 5000: loss 0.52245 errors 16\n",
      "Iteration 6000: loss 0.52201 errors 16\n",
      "Iteration 7000: loss 0.52168 errors 16\n",
      "Iteration 8000: loss 0.52142 errors 16\n",
      "Iteration 9000: loss 0.52121 errors 16\n",
      "Iteration 10000: loss 0.52104 errors 16\n",
      "Iteration 11000: loss 0.52090 errors 16\n",
      "Iteration 12000: loss 0.52079 errors 16\n",
      "Iteration 13000: loss 0.52069 errors 16\n",
      "Iteration 1000: loss 0.61724 errors 16\n",
      "Iteration 2000: loss 0.61724 errors 16\n",
      "Iteration 1000: loss 0.57893 errors 16\n",
      "Iteration 2000: loss 0.57107 errors 16\n",
      "Iteration 3000: loss 0.56500 errors 16\n",
      "Iteration 4000: loss 0.56073 errors 16\n",
      "Iteration 5000: loss 0.55784 errors 16\n",
      "Iteration 6000: loss 0.55567 errors 16\n",
      "Iteration 7000: loss 0.55407 errors 16\n",
      "Iteration 8000: loss 0.55287 errors 16\n",
      "Iteration 9000: loss 0.55194 errors 16\n",
      "Iteration 10000: loss 0.55111 errors 16\n",
      "Iteration 11000: loss 0.55034 errors 16\n",
      "Iteration 12000: loss 0.54961 errors 16\n",
      "Iteration 13000: loss 0.54894 errors 16\n",
      "Iteration 14000: loss 0.54830 errors 16\n",
      "Iteration 15000: loss 0.54770 errors 16\n",
      "Iteration 16000: loss 0.54714 errors 16\n",
      "Iteration 17000: loss 0.54661 errors 16\n",
      "Iteration 18000: loss 0.54610 errors 16\n",
      "Iteration 19000: loss 0.54562 errors 16\n",
      "Iteration 20000: loss 0.54517 errors 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1628232/3336676992.py:46: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  xx, yy = np.meshgrid(np.arange(x_min, x_max, delta), np.arange(y_min, y_max, delta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best for data1.txt H=2: error=10, iterations=185\n",
      "Iteration 1000: loss 0.53264 errors 16\n",
      "Iteration 2000: loss 0.52905 errors 16\n",
      "Iteration 3000: loss 0.52738 errors 16\n",
      "Iteration 4000: loss 0.52639 errors 16\n",
      "Iteration 5000: loss 0.52573 errors 16\n",
      "Iteration 6000: loss 0.52527 errors 16\n",
      "Iteration 7000: loss 0.52494 errors 16\n",
      "Iteration 8000: loss 0.52468 errors 16\n",
      "Iteration 9000: loss 0.52448 errors 16\n",
      "Iteration 10000: loss 0.52433 errors 16\n",
      "Iteration 11000: loss 0.52420 errors 16\n",
      "Iteration 12000: loss 0.52410 errors 16\n",
      "Iteration 13000: loss 0.52401 errors 16\n",
      "Iteration 1000: loss 0.39353 errors 12\n",
      "Iteration 2000: loss 0.38142 errors 13\n",
      "Iteration 3000: loss 0.36019 errors 8\n",
      "Iteration 4000: loss 0.34463 errors 8\n",
      "Iteration 5000: loss 0.32585 errors 6\n",
      "Iteration 6000: loss 0.30225 errors 5\n",
      "Iteration 7000: loss 0.27058 errors 4\n",
      "Iteration 8000: loss 0.24378 errors 4\n",
      "Iteration 9000: loss 0.22141 errors 4\n",
      "Iteration 10000: loss 0.20394 errors 4\n",
      "Iteration 11000: loss 0.18539 errors 2\n",
      "Iteration 12000: loss 0.17113 errors 1\n",
      "Iteration 13000: loss 0.15988 errors 1\n",
      "Iteration 14000: loss 0.14999 errors 1\n",
      "Iteration 15000: loss 0.13895 errors 1\n",
      "Iteration 16000: loss 0.12873 errors 0\n",
      "Iteration 17000: loss 0.12028 errors 0\n",
      "Iteration 18000: loss 0.11311 errors 0\n",
      "Iteration 19000: loss 0.10689 errors 0\n",
      "Iteration 20000: loss 0.10126 errors 0\n",
      "Iteration 1000: loss 0.41573 errors 12\n",
      "Iteration 2000: loss 0.41050 errors 11\n",
      "Iteration 3000: loss 0.40918 errors 11\n",
      "Iteration 4000: loss 0.40865 errors 12\n",
      "Iteration 5000: loss 0.40839 errors 12\n",
      "Iteration 6000: loss 0.40824 errors 12\n",
      "Iteration 7000: loss 0.40814 errors 12\n",
      "Iteration 1000: loss 0.61919 errors 16\n",
      "Iteration 2000: loss 0.61796 errors 16\n",
      "Iteration 3000: loss 0.61755 errors 16\n",
      "Iteration 4000: loss 0.61739 errors 16\n",
      "Iteration 5000: loss 0.61732 errors 16\n",
      "Iteration 1000: loss 0.55250 errors 16\n",
      "Iteration 2000: loss 0.53833 errors 16\n",
      "Iteration 3000: loss 0.53411 errors 16\n",
      "Iteration 4000: loss 0.53295 errors 16\n",
      "Iteration 5000: loss 0.53250 errors 16\n",
      "Iteration 6000: loss 0.53228 errors 16\n",
      "Iteration 7000: loss 0.53217 errors 16\n",
      "Iteration 8000: loss 0.53209 errors 16\n",
      "Best for data1.txt H=4: error=0, iterations=15955\n",
      "Iteration 1000: loss 0.69315 errors 18\n",
      "Iteration 2000: loss 0.69315 errors 18\n",
      "Iteration 1000: loss 0.13547 errors 0\n",
      "Iteration 2000: loss 0.08552 errors 0\n",
      "Iteration 3000: loss 0.06395 errors 0\n",
      "Iteration 4000: loss 0.05187 errors 0\n",
      "Iteration 5000: loss 0.04410 errors 0\n",
      "Iteration 6000: loss 0.03866 errors 0\n",
      "Iteration 7000: loss 0.03462 errors 0\n",
      "Iteration 8000: loss 0.03150 errors 0\n",
      "Iteration 9000: loss 0.02899 errors 0\n",
      "Iteration 10000: loss 0.02694 errors 0\n",
      "Iteration 11000: loss 0.02522 errors 0\n",
      "Iteration 12000: loss 0.02376 errors 0\n",
      "Iteration 13000: loss 0.02249 errors 0\n",
      "Iteration 14000: loss 0.02139 errors 0\n",
      "Iteration 15000: loss 0.02042 errors 0\n",
      "Iteration 16000: loss 0.01955 errors 0\n",
      "Iteration 17000: loss 0.01878 errors 0\n",
      "Iteration 18000: loss 0.01808 errors 0\n",
      "Iteration 19000: loss 0.01744 errors 0\n",
      "Iteration 20000: loss 0.01686 errors 0\n",
      "Iteration 1000: loss 0.49714 errors 10\n",
      "Iteration 2000: loss 0.49312 errors 10\n",
      "Iteration 3000: loss 0.49113 errors 10\n",
      "Iteration 4000: loss 0.48942 errors 9\n",
      "Iteration 5000: loss 0.48790 errors 9\n",
      "Iteration 6000: loss 0.48654 errors 9\n",
      "Iteration 7000: loss 0.48531 errors 9\n",
      "Iteration 8000: loss 0.48419 errors 9\n",
      "Iteration 9000: loss 0.48316 errors 9\n",
      "Iteration 10000: loss 0.48222 errors 9\n",
      "Iteration 11000: loss 0.48136 errors 9\n",
      "Iteration 12000: loss 0.48061 errors 9\n",
      "Iteration 13000: loss 0.48008 errors 9\n",
      "Iteration 14000: loss 0.47971 errors 9\n",
      "Iteration 15000: loss 0.47942 errors 9\n",
      "Iteration 16000: loss 0.47920 errors 9\n",
      "Iteration 17000: loss 0.47902 errors 9\n",
      "Iteration 18000: loss 0.47888 errors 9\n",
      "Iteration 19000: loss 0.47876 errors 9\n",
      "Iteration 20000: loss 0.47866 errors 9\n",
      "Iteration 1000: loss 0.69315 errors 18\n",
      "Iteration 2000: loss 0.69315 errors 18\n",
      "Iteration 1000: loss 0.69315 errors 18\n",
      "Iteration 2000: loss 0.69315 errors 18\n",
      "Best for data2.txt H=2: error=0, iterations=386\n",
      "Iteration 1000: loss 0.16150 errors 0\n",
      "Iteration 2000: loss 0.10047 errors 0\n",
      "Iteration 3000: loss 0.07370 errors 0\n",
      "Iteration 4000: loss 0.05885 errors 0\n",
      "Iteration 5000: loss 0.04940 errors 0\n",
      "Iteration 6000: loss 0.04285 errors 0\n",
      "Iteration 7000: loss 0.03803 errors 0\n",
      "Iteration 8000: loss 0.03433 errors 0\n",
      "Iteration 9000: loss 0.03139 errors 0\n",
      "Iteration 10000: loss 0.02900 errors 0\n",
      "Iteration 11000: loss 0.02701 errors 0\n",
      "Iteration 12000: loss 0.02534 errors 0\n",
      "Iteration 13000: loss 0.02390 errors 0\n",
      "Iteration 14000: loss 0.02265 errors 0\n",
      "Iteration 15000: loss 0.02156 errors 0\n",
      "Iteration 16000: loss 0.02059 errors 0\n",
      "Iteration 17000: loss 0.01972 errors 0\n",
      "Iteration 18000: loss 0.01895 errors 0\n",
      "Iteration 19000: loss 0.01825 errors 0\n",
      "Iteration 20000: loss 0.01761 errors 0\n",
      "Iteration 1000: loss 0.04770 errors 0\n",
      "Iteration 2000: loss 0.03078 errors 0\n",
      "Iteration 3000: loss 0.02335 errors 0\n",
      "Iteration 4000: loss 0.01906 errors 0\n",
      "Iteration 5000: loss 0.01624 errors 0\n",
      "Iteration 6000: loss 0.01423 errors 0\n",
      "Iteration 7000: loss 0.01271 errors 0\n",
      "Iteration 8000: loss 0.01153 errors 0\n",
      "Iteration 9000: loss 0.01057 errors 0\n",
      "Iteration 10000: loss 0.00979 errors 0\n",
      "Iteration 11000: loss 0.00913 errors 0\n",
      "Iteration 12000: loss 0.00856 errors 0\n",
      "Iteration 13000: loss 0.00808 errors 0\n",
      "Iteration 14000: loss 0.00765 errors 0\n",
      "Iteration 15000: loss 0.00727 errors 0\n",
      "Iteration 16000: loss 0.00694 errors 0\n",
      "Iteration 17000: loss 0.00664 errors 0\n",
      "Iteration 18000: loss 0.00637 errors 0\n",
      "Iteration 19000: loss 0.00613 errors 0\n",
      "Iteration 20000: loss 0.00590 errors 0\n",
      "Iteration 1000: loss 0.06102 errors 0\n",
      "Iteration 2000: loss 0.03521 errors 0\n",
      "Iteration 3000: loss 0.02564 errors 0\n",
      "Iteration 4000: loss 0.02053 errors 0\n",
      "Iteration 5000: loss 0.01733 errors 0\n",
      "Iteration 6000: loss 0.01511 errors 0\n",
      "Iteration 7000: loss 0.01348 errors 0\n",
      "Iteration 8000: loss 0.01223 errors 0\n",
      "Iteration 9000: loss 0.01122 errors 0\n",
      "Iteration 10000: loss 0.01039 errors 0\n",
      "Iteration 11000: loss 0.00970 errors 0\n",
      "Iteration 12000: loss 0.00912 errors 0\n",
      "Iteration 13000: loss 0.00861 errors 0\n",
      "Iteration 14000: loss 0.00817 errors 0\n",
      "Iteration 15000: loss 0.00778 errors 0\n",
      "Iteration 16000: loss 0.00743 errors 0\n",
      "Iteration 17000: loss 0.00712 errors 0\n",
      "Iteration 18000: loss 0.00685 errors 0\n",
      "Iteration 19000: loss 0.00659 errors 0\n",
      "Iteration 20000: loss 0.00636 errors 0\n",
      "Iteration 1000: loss 0.69335 errors 10\n",
      "Iteration 2000: loss 0.69323 errors 10\n",
      "Iteration 3000: loss 0.69319 errors 10\n",
      "Iteration 1000: loss 0.69315 errors 18\n",
      "Iteration 2000: loss 0.69315 errors 18\n",
      "Best for data2.txt H=4: error=0, iterations=506\n",
      "Iteration 1000: loss 0.22617 errors 8\n",
      "Iteration 2000: loss 0.21673 errors 8\n",
      "Iteration 3000: loss 0.21306 errors 8\n",
      "Iteration 4000: loss 0.21098 errors 8\n",
      "Iteration 5000: loss 0.20958 errors 8\n",
      "Iteration 6000: loss 0.20854 errors 8\n",
      "Iteration 7000: loss 0.20773 errors 8\n",
      "Iteration 8000: loss 0.20710 errors 8\n",
      "Iteration 9000: loss 0.20657 errors 8\n",
      "Iteration 10000: loss 0.20612 errors 8\n",
      "Iteration 11000: loss 0.20572 errors 8\n",
      "Iteration 12000: loss 0.20536 errors 8\n",
      "Iteration 13000: loss 0.20504 errors 8\n",
      "Iteration 14000: loss 0.20477 errors 8\n",
      "Iteration 15000: loss 0.20452 errors 8\n",
      "Iteration 16000: loss 0.20428 errors 8\n",
      "Iteration 17000: loss 0.20407 errors 8\n",
      "Iteration 18000: loss 0.20387 errors 8\n",
      "Iteration 19000: loss 0.20368 errors 8\n",
      "Iteration 20000: loss 0.20350 errors 8\n",
      "Iteration 1000: loss 0.35457 errors 14\n",
      "Iteration 2000: loss 0.34166 errors 14\n",
      "Iteration 3000: loss 0.32461 errors 12\n",
      "Iteration 4000: loss 0.30881 errors 12\n",
      "Iteration 5000: loss 0.29116 errors 12\n",
      "Iteration 6000: loss 0.27276 errors 11\n",
      "Iteration 7000: loss 0.25569 errors 9\n",
      "Iteration 8000: loss 0.24054 errors 8\n",
      "Iteration 9000: loss 0.22822 errors 7\n",
      "Iteration 10000: loss 0.21864 errors 7\n",
      "Iteration 11000: loss 0.21116 errors 7\n",
      "Iteration 12000: loss 0.20507 errors 6\n",
      "Iteration 13000: loss 0.20001 errors 6\n",
      "Iteration 14000: loss 0.19577 errors 6\n",
      "Iteration 15000: loss 0.19221 errors 6\n",
      "Iteration 16000: loss 0.18925 errors 5\n",
      "Iteration 17000: loss 0.18675 errors 5\n",
      "Iteration 18000: loss 0.18457 errors 5\n",
      "Iteration 19000: loss 0.18265 errors 5\n",
      "Iteration 20000: loss 0.18095 errors 5\n",
      "Iteration 1000: loss 0.20087 errors 10\n",
      "Iteration 2000: loss 0.19784 errors 10\n",
      "Iteration 3000: loss 0.19686 errors 10\n",
      "Iteration 4000: loss 0.19645 errors 9\n",
      "Iteration 5000: loss 0.19625 errors 9\n",
      "Iteration 6000: loss 0.19615 errors 9\n",
      "Iteration 7000: loss 0.19609 errors 9\n",
      "Iteration 1000: loss 0.69216 errors 43\n",
      "Iteration 2000: loss 0.69216 errors 43\n",
      "Iteration 1000: loss 0.69216 errors 43\n",
      "Iteration 2000: loss 0.69216 errors 43\n",
      "Best for data3.txt H=2: error=5, iterations=15257\n",
      "Iteration 1000: loss 0.20052 errors 10\n",
      "Iteration 2000: loss 0.19794 errors 10\n",
      "Iteration 3000: loss 0.19706 errors 10\n",
      "Iteration 4000: loss 0.19667 errors 9\n",
      "Iteration 5000: loss 0.19646 errors 9\n",
      "Iteration 6000: loss 0.19634 errors 9\n",
      "Iteration 7000: loss 0.19626 errors 9\n",
      "Iteration 1000: loss 0.20134 errors 10\n",
      "Iteration 2000: loss 0.19728 errors 10\n",
      "Iteration 3000: loss 0.19581 errors 10\n",
      "Iteration 4000: loss 0.19508 errors 9\n",
      "Iteration 5000: loss 0.19465 errors 9\n",
      "Iteration 6000: loss 0.19434 errors 9\n",
      "Iteration 7000: loss 0.19409 errors 9\n",
      "Iteration 8000: loss 0.19389 errors 9\n",
      "Iteration 9000: loss 0.19371 errors 9\n",
      "Iteration 10000: loss 0.19354 errors 9\n",
      "Iteration 11000: loss 0.19339 errors 9\n",
      "Iteration 12000: loss 0.19324 errors 9\n",
      "Iteration 13000: loss 0.19309 errors 9\n",
      "Iteration 14000: loss 0.19296 errors 9\n",
      "Iteration 15000: loss 0.19282 errors 9\n",
      "Iteration 16000: loss 0.19269 errors 9\n",
      "Iteration 17000: loss 0.19256 errors 9\n",
      "Iteration 18000: loss 0.19244 errors 9\n",
      "Iteration 19000: loss 0.19232 errors 9\n",
      "Iteration 20000: loss 0.19220 errors 9\n",
      "Iteration 1000: loss 0.20700 errors 9\n",
      "Iteration 2000: loss 0.20010 errors 10\n",
      "Iteration 3000: loss 0.19777 errors 10\n",
      "Iteration 4000: loss 0.19671 errors 10\n",
      "Iteration 5000: loss 0.19617 errors 10\n",
      "Iteration 6000: loss 0.19586 errors 9\n",
      "Iteration 7000: loss 0.19567 errors 9\n",
      "Iteration 8000: loss 0.19554 errors 9\n",
      "Iteration 9000: loss 0.19545 errors 9\n",
      "Iteration 1000: loss 0.31112 errors 13\n",
      "Iteration 2000: loss 0.26153 errors 10\n",
      "Iteration 3000: loss 0.23394 errors 9\n",
      "Iteration 4000: loss 0.22291 errors 8\n",
      "Iteration 5000: loss 0.21752 errors 7\n",
      "Iteration 6000: loss 0.21444 errors 7\n",
      "Iteration 7000: loss 0.21250 errors 7\n",
      "Iteration 8000: loss 0.21119 errors 7\n",
      "Iteration 9000: loss 0.21023 errors 7\n",
      "Iteration 10000: loss 0.20950 errors 7\n",
      "Iteration 11000: loss 0.20893 errors 7\n",
      "Iteration 12000: loss 0.20848 errors 7\n",
      "Iteration 13000: loss 0.20811 errors 7\n",
      "Iteration 14000: loss 0.20781 errors 7\n",
      "Iteration 15000: loss 0.20757 errors 7\n",
      "Iteration 16000: loss 0.20737 errors 7\n",
      "Iteration 17000: loss 0.20720 errors 7\n",
      "Iteration 18000: loss 0.20706 errors 7\n",
      "Iteration 19000: loss 0.20693 errors 7\n",
      "Iteration 20000: loss 0.20683 errors 7\n",
      "Iteration 1000: loss 0.69216 errors 43\n",
      "Iteration 2000: loss 0.69216 errors 43\n",
      "Best for data3.txt H=4: error=7, iterations=4062\n",
      "Iteration 1000: loss 0.22887 errors 3\n",
      "Iteration 2000: loss 0.16820 errors 3\n",
      "Iteration 3000: loss 0.14435 errors 3\n",
      "Iteration 4000: loss 0.13067 errors 3\n",
      "Iteration 5000: loss 0.12175 errors 3\n",
      "Iteration 6000: loss 0.11589 errors 3\n",
      "Iteration 7000: loss 0.11143 errors 3\n",
      "Iteration 8000: loss 0.10790 errors 3\n",
      "Iteration 9000: loss 0.10504 errors 3\n",
      "Iteration 10000: loss 0.10267 errors 3\n",
      "Iteration 11000: loss 0.10066 errors 3\n",
      "Iteration 12000: loss 0.09895 errors 3\n",
      "Iteration 13000: loss 0.09746 errors 3\n",
      "Iteration 14000: loss 0.09616 errors 3\n",
      "Iteration 15000: loss 0.09501 errors 3\n",
      "Iteration 16000: loss 0.09398 errors 3\n",
      "Iteration 17000: loss 0.09307 errors 3\n",
      "Iteration 18000: loss 0.09224 errors 3\n",
      "Iteration 19000: loss 0.09149 errors 3\n",
      "Iteration 20000: loss 0.09081 errors 3\n",
      "Iteration 1000: loss 0.26950 errors 7\n",
      "Iteration 2000: loss 0.17916 errors 4\n",
      "Iteration 3000: loss 0.11782 errors 0\n",
      "Iteration 4000: loss 0.08978 errors 0\n",
      "Iteration 5000: loss 0.07306 errors 0\n",
      "Iteration 6000: loss 0.06212 errors 0\n",
      "Iteration 7000: loss 0.05439 errors 0\n",
      "Iteration 8000: loss 0.04863 errors 0\n",
      "Iteration 9000: loss 0.04414 errors 0\n",
      "Iteration 10000: loss 0.04055 errors 0\n",
      "Iteration 11000: loss 0.03759 errors 0\n",
      "Iteration 12000: loss 0.03512 errors 0\n",
      "Iteration 13000: loss 0.03301 errors 0\n",
      "Iteration 14000: loss 0.03120 errors 0\n",
      "Iteration 15000: loss 0.02963 errors 0\n",
      "Iteration 16000: loss 0.02824 errors 0\n",
      "Iteration 17000: loss 0.02701 errors 0\n",
      "Iteration 18000: loss 0.02591 errors 0\n",
      "Iteration 19000: loss 0.02492 errors 0\n",
      "Iteration 20000: loss 0.02402 errors 0\n",
      "Iteration 1000: loss 0.49990 errors 25\n",
      "Iteration 2000: loss 0.48633 errors 25\n",
      "Iteration 3000: loss 0.47312 errors 25\n",
      "Iteration 4000: loss 0.45803 errors 25\n",
      "Iteration 5000: loss 0.40158 errors 16\n",
      "Iteration 6000: loss 0.27925 errors 3\n",
      "Iteration 7000: loss 0.23459 errors 3\n",
      "Iteration 8000: loss 0.20473 errors 3\n",
      "Iteration 9000: loss 0.18381 errors 3\n",
      "Iteration 10000: loss 0.16850 errors 3\n",
      "Iteration 11000: loss 0.15698 errors 3\n",
      "Iteration 12000: loss 0.14777 errors 3\n",
      "Iteration 13000: loss 0.14057 errors 3\n",
      "Iteration 14000: loss 0.13454 errors 3\n",
      "Iteration 15000: loss 0.12962 errors 3\n",
      "Iteration 16000: loss 0.12543 errors 3\n",
      "Iteration 17000: loss 0.12182 errors 3\n",
      "Iteration 18000: loss 0.11869 errors 3\n",
      "Iteration 19000: loss 0.11600 errors 3\n",
      "Iteration 20000: loss 0.11361 errors 3\n",
      "Iteration 1000: loss 0.63343 errors 25\n",
      "Iteration 2000: loss 0.63343 errors 25\n",
      "Iteration 1000: loss 0.63343 errors 25\n",
      "Iteration 2000: loss 0.63343 errors 25\n",
      "Best for data4.txt H=2: error=0, iterations=2686\n",
      "Iteration 1000: loss 0.07210 errors 0\n",
      "Iteration 2000: loss 0.04692 errors 0\n",
      "Iteration 3000: loss 0.03600 errors 0\n",
      "Iteration 4000: loss 0.02974 errors 0\n",
      "Iteration 5000: loss 0.02586 errors 0\n",
      "Iteration 6000: loss 0.02309 errors 0\n",
      "Iteration 7000: loss 0.02098 errors 0\n",
      "Iteration 8000: loss 0.01932 errors 0\n",
      "Iteration 9000: loss 0.01797 errors 0\n",
      "Iteration 10000: loss 0.01684 errors 0\n",
      "Iteration 11000: loss 0.01589 errors 0\n",
      "Iteration 12000: loss 0.01506 errors 0\n",
      "Iteration 13000: loss 0.01434 errors 0\n",
      "Iteration 14000: loss 0.01371 errors 0\n",
      "Iteration 15000: loss 0.01315 errors 0\n",
      "Iteration 16000: loss 0.01265 errors 0\n",
      "Iteration 17000: loss 0.01219 errors 0\n",
      "Iteration 18000: loss 0.01178 errors 0\n",
      "Iteration 19000: loss 0.01140 errors 0\n",
      "Iteration 20000: loss 0.01106 errors 0\n",
      "Iteration 1000: loss 0.04927 errors 0\n",
      "Iteration 2000: loss 0.03093 errors 0\n",
      "Iteration 3000: loss 0.02363 errors 0\n",
      "Iteration 4000: loss 0.01948 errors 0\n",
      "Iteration 5000: loss 0.01675 errors 0\n",
      "Iteration 6000: loss 0.01478 errors 0\n",
      "Iteration 7000: loss 0.01330 errors 0\n",
      "Iteration 8000: loss 0.01213 errors 0\n",
      "Iteration 9000: loss 0.01118 errors 0\n",
      "Iteration 10000: loss 0.01039 errors 0\n",
      "Iteration 11000: loss 0.00951 errors 0\n",
      "Iteration 12000: loss 0.00881 errors 0\n",
      "Iteration 13000: loss 0.00827 errors 0\n",
      "Iteration 14000: loss 0.00782 errors 0\n",
      "Iteration 15000: loss 0.00743 errors 0\n",
      "Iteration 16000: loss 0.00709 errors 0\n",
      "Iteration 17000: loss 0.00679 errors 0\n",
      "Iteration 18000: loss 0.00652 errors 0\n",
      "Iteration 19000: loss 0.00627 errors 0\n",
      "Iteration 20000: loss 0.00604 errors 0\n",
      "Iteration 1000: loss 0.07669 errors 0\n",
      "Iteration 2000: loss 0.04976 errors 0\n",
      "Iteration 3000: loss 0.03861 errors 0\n",
      "Iteration 4000: loss 0.03234 errors 0\n",
      "Iteration 5000: loss 0.02816 errors 0\n",
      "Iteration 6000: loss 0.02514 errors 0\n",
      "Iteration 7000: loss 0.02284 errors 0\n",
      "Iteration 8000: loss 0.02102 errors 0\n",
      "Iteration 9000: loss 0.01955 errors 0\n",
      "Iteration 10000: loss 0.01833 errors 0\n",
      "Iteration 11000: loss 0.01730 errors 0\n",
      "Iteration 12000: loss 0.01641 errors 0\n",
      "Iteration 13000: loss 0.01563 errors 0\n",
      "Iteration 14000: loss 0.01494 errors 0\n",
      "Iteration 15000: loss 0.01433 errors 0\n",
      "Iteration 16000: loss 0.01378 errors 0\n",
      "Iteration 17000: loss 0.01328 errors 0\n",
      "Iteration 18000: loss 0.01283 errors 0\n",
      "Iteration 19000: loss 0.01241 errors 0\n",
      "Iteration 20000: loss 0.01203 errors 0\n",
      "Iteration 1000: loss 0.35427 errors 3\n",
      "Iteration 2000: loss 0.28272 errors 3\n",
      "Iteration 3000: loss 0.23622 errors 3\n",
      "Iteration 4000: loss 0.20214 errors 3\n",
      "Iteration 5000: loss 0.17771 errors 3\n",
      "Iteration 6000: loss 0.16019 errors 3\n",
      "Iteration 7000: loss 0.14724 errors 3\n",
      "Iteration 8000: loss 0.13757 errors 3\n",
      "Iteration 9000: loss 0.12999 errors 3\n",
      "Iteration 10000: loss 0.12400 errors 3\n",
      "Iteration 11000: loss 0.11921 errors 3\n",
      "Iteration 12000: loss 0.11521 errors 3\n",
      "Iteration 13000: loss 0.11183 errors 3\n",
      "Iteration 14000: loss 0.10894 errors 3\n",
      "Iteration 15000: loss 0.10644 errors 3\n",
      "Iteration 16000: loss 0.10433 errors 3\n",
      "Iteration 17000: loss 0.10249 errors 3\n",
      "Iteration 18000: loss 0.10086 errors 3\n",
      "Iteration 19000: loss 0.09940 errors 3\n",
      "Iteration 20000: loss 0.09809 errors 3\n",
      "Iteration 1000: loss 0.63343 errors 25\n",
      "Iteration 2000: loss 0.63343 errors 25\n",
      "Best for data4.txt H=4: error=0, iterations=46\n",
      "Iteration 1000: loss 0.63741 errors 20\n",
      "Iteration 2000: loss 0.63690 errors 20\n",
      "Iteration 3000: loss 0.63672 errors 20\n",
      "Iteration 4000: loss 0.63663 errors 20\n",
      "Iteration 1000: loss 0.31019 errors 6\n",
      "Iteration 2000: loss 0.19310 errors 1\n",
      "Iteration 3000: loss 0.13097 errors 0\n",
      "Iteration 4000: loss 0.09876 errors 0\n",
      "Iteration 5000: loss 0.07944 errors 0\n",
      "Iteration 6000: loss 0.06669 errors 0\n",
      "Iteration 7000: loss 0.05768 errors 0\n",
      "Iteration 8000: loss 0.05099 errors 0\n",
      "Iteration 9000: loss 0.04582 errors 0\n",
      "Iteration 10000: loss 0.04172 errors 0\n",
      "Iteration 11000: loss 0.03837 errors 0\n",
      "Iteration 12000: loss 0.03559 errors 0\n",
      "Iteration 13000: loss 0.03324 errors 0\n",
      "Iteration 14000: loss 0.03123 errors 0\n",
      "Iteration 15000: loss 0.02948 errors 0\n",
      "Iteration 16000: loss 0.02796 errors 0\n",
      "Iteration 17000: loss 0.02661 errors 0\n",
      "Iteration 18000: loss 0.02541 errors 0\n",
      "Iteration 19000: loss 0.02433 errors 0\n",
      "Iteration 20000: loss 0.02336 errors 0\n",
      "Iteration 1000: loss 0.62203 errors 20\n",
      "Iteration 2000: loss 0.61773 errors 20\n",
      "Iteration 3000: loss 0.61084 errors 20\n",
      "Iteration 4000: loss 0.59600 errors 20\n",
      "Iteration 5000: loss 0.57053 errors 20\n",
      "Iteration 6000: loss 0.54488 errors 20\n",
      "Iteration 7000: loss 0.52634 errors 20\n",
      "Iteration 8000: loss 0.51026 errors 20\n",
      "Iteration 9000: loss 0.49719 errors 20\n",
      "Iteration 10000: loss 0.48860 errors 20\n",
      "Iteration 11000: loss 0.48379 errors 20\n",
      "Iteration 12000: loss 0.47889 errors 20\n",
      "Iteration 13000: loss 0.47600 errors 20\n",
      "Iteration 14000: loss 0.47538 errors 20\n",
      "Iteration 15000: loss 0.47272 errors 20\n",
      "Iteration 16000: loss 0.47100 errors 20\n",
      "Iteration 17000: loss 0.47027 errors 20\n",
      "Iteration 18000: loss 0.46926 errors 20\n",
      "Iteration 19000: loss 0.46890 errors 20\n",
      "Iteration 20000: loss 0.46809 errors 20\n",
      "Iteration 1000: loss 0.63651 errors 20\n",
      "Iteration 2000: loss 0.63651 errors 20\n",
      "Iteration 1000: loss 0.50512 errors 20\n",
      "Iteration 2000: loss 0.48801 errors 20\n",
      "Iteration 3000: loss 0.47603 errors 20\n",
      "Iteration 4000: loss 0.47255 errors 20\n",
      "Iteration 5000: loss 0.47120 errors 20\n",
      "Iteration 6000: loss 0.46859 errors 20\n",
      "Iteration 7000: loss 0.46812 errors 20\n",
      "Iteration 8000: loss 0.46755 errors 20\n",
      "Iteration 9000: loss 0.46655 errors 20\n",
      "Iteration 10000: loss 0.46619 errors 20\n",
      "Iteration 11000: loss 0.46597 errors 20\n",
      "Iteration 12000: loss 0.46577 errors 20\n",
      "Iteration 13000: loss 0.46528 errors 20\n",
      "Iteration 14000: loss 0.46508 errors 20\n",
      "Iteration 15000: loss 0.46495 errors 20\n",
      "Iteration 16000: loss 0.46477 errors 20\n",
      "Iteration 17000: loss 0.46463 errors 20\n",
      "Iteration 18000: loss 0.46451 errors 20\n",
      "Iteration 19000: loss 0.46438 errors 20\n",
      "Iteration 20000: loss 0.46443 errors 20\n",
      "Best for data5.txt H=2: error=0, iterations=2044\n",
      "Iteration 1000: loss 0.56402 errors 20\n",
      "Iteration 2000: loss 0.54126 errors 20\n",
      "Iteration 3000: loss 0.53083 errors 20\n",
      "Iteration 4000: loss 0.52505 errors 20\n",
      "Iteration 5000: loss 0.52144 errors 20\n",
      "Iteration 6000: loss 0.51799 errors 20\n",
      "Iteration 7000: loss 0.51410 errors 20\n",
      "Iteration 8000: loss 0.50929 errors 20\n",
      "Iteration 9000: loss 0.50509 errors 20\n",
      "Iteration 10000: loss 0.50184 errors 20\n",
      "Iteration 11000: loss 0.49933 errors 20\n",
      "Iteration 12000: loss 0.49734 errors 20\n",
      "Iteration 13000: loss 0.49575 errors 20\n",
      "Iteration 14000: loss 0.49235 errors 20\n",
      "Iteration 15000: loss 0.48902 errors 20\n",
      "Iteration 16000: loss 0.48655 errors 20\n",
      "Iteration 17000: loss 0.48252 errors 20\n",
      "Iteration 18000: loss 0.47718 errors 20\n",
      "Iteration 19000: loss 0.47394 errors 20\n",
      "Iteration 20000: loss 0.47183 errors 20\n",
      "Iteration 1000: loss 0.38421 errors 12\n",
      "Iteration 2000: loss 0.30925 errors 11\n",
      "Iteration 3000: loss 0.22547 errors 4\n",
      "Iteration 4000: loss 0.15563 errors 0\n",
      "Iteration 5000: loss 0.12243 errors 0\n",
      "Iteration 6000: loss 0.10007 errors 0\n",
      "Iteration 7000: loss 0.08396 errors 0\n",
      "Iteration 8000: loss 0.07200 errors 0\n",
      "Iteration 9000: loss 0.06291 errors 0\n",
      "Iteration 10000: loss 0.05582 errors 0\n",
      "Iteration 11000: loss 0.05020 errors 0\n",
      "Iteration 12000: loss 0.04566 errors 0\n",
      "Iteration 13000: loss 0.04196 errors 0\n",
      "Iteration 14000: loss 0.03890 errors 0\n",
      "Iteration 15000: loss 0.03631 errors 0\n",
      "Iteration 16000: loss 0.03409 errors 0\n",
      "Iteration 17000: loss 0.03216 errors 0\n",
      "Iteration 18000: loss 0.03046 errors 0\n",
      "Iteration 19000: loss 0.02896 errors 0\n",
      "Iteration 20000: loss 0.02763 errors 0\n",
      "Iteration 1000: loss 0.32004 errors 2\n",
      "Iteration 2000: loss 0.19280 errors 1\n",
      "Iteration 3000: loss 0.13892 errors 0\n",
      "Iteration 4000: loss 0.10963 errors 0\n",
      "Iteration 5000: loss 0.09125 errors 0\n",
      "Iteration 6000: loss 0.07849 errors 0\n",
      "Iteration 7000: loss 0.06902 errors 0\n",
      "Iteration 8000: loss 0.06180 errors 0\n",
      "Iteration 9000: loss 0.05605 errors 0\n",
      "Iteration 10000: loss 0.05136 errors 0\n",
      "Iteration 11000: loss 0.04745 errors 0\n",
      "Iteration 12000: loss 0.04419 errors 0\n",
      "Iteration 13000: loss 0.04141 errors 0\n",
      "Iteration 14000: loss 0.03898 errors 0\n",
      "Iteration 15000: loss 0.03685 errors 0\n",
      "Iteration 16000: loss 0.03496 errors 0\n",
      "Iteration 17000: loss 0.03327 errors 0\n",
      "Iteration 18000: loss 0.03176 errors 0\n",
      "Iteration 19000: loss 0.03039 errors 0\n",
      "Iteration 20000: loss 0.02916 errors 0\n",
      "Iteration 1000: loss 0.63849 errors 20\n",
      "Iteration 2000: loss 0.63740 errors 20\n",
      "Iteration 3000: loss 0.63696 errors 20\n",
      "Iteration 4000: loss 0.63676 errors 20\n",
      "Iteration 5000: loss 0.63666 errors 20\n",
      "Iteration 6000: loss 0.63660 errors 20\n",
      "Iteration 1000: loss 0.54626 errors 20\n",
      "Iteration 2000: loss 0.50836 errors 20\n",
      "Iteration 3000: loss 0.48876 errors 20\n",
      "Iteration 4000: loss 0.48146 errors 20\n",
      "Iteration 5000: loss 0.47668 errors 20\n",
      "Iteration 6000: loss 0.47277 errors 20\n",
      "Iteration 7000: loss 0.47134 errors 20\n",
      "Iteration 8000: loss 0.46929 errors 20\n",
      "Iteration 9000: loss 0.46825 errors 20\n",
      "Iteration 10000: loss 0.46833 errors 20\n",
      "Best for data5.txt H=4: error=0, iterations=3334\n",
      "Iteration 1000: loss 0.69295 errors 392\n",
      "Iteration 2000: loss 0.69295 errors 392\n",
      "Iteration 1000: loss 0.44190 errors 158\n",
      "Iteration 2000: loss 0.43121 errors 151\n",
      "Iteration 3000: loss 0.42828 errors 148\n",
      "Iteration 4000: loss 0.42696 errors 146\n",
      "Iteration 5000: loss 0.42618 errors 147\n",
      "Iteration 6000: loss 0.42565 errors 146\n",
      "Iteration 7000: loss 0.42522 errors 146\n",
      "Iteration 8000: loss 0.42483 errors 146\n",
      "Iteration 9000: loss 0.42451 errors 146\n",
      "Iteration 10000: loss 0.42424 errors 146\n",
      "Iteration 11000: loss 0.42399 errors 146\n",
      "Iteration 12000: loss 0.42377 errors 146\n",
      "Iteration 13000: loss 0.42357 errors 146\n",
      "Iteration 14000: loss 0.42340 errors 146\n",
      "Iteration 15000: loss 0.42323 errors 147\n",
      "Iteration 16000: loss 0.42269 errors 146\n",
      "Iteration 17000: loss 0.42238 errors 145\n",
      "Iteration 18000: loss 0.42218 errors 145\n",
      "Iteration 19000: loss 0.42199 errors 145\n",
      "Iteration 20000: loss 0.42182 errors 146\n",
      "Iteration 1000: loss 0.63159 errors 226\n",
      "Iteration 2000: loss 0.56071 errors 195\n",
      "Iteration 3000: loss 0.47872 errors 160\n",
      "Iteration 4000: loss 0.45172 errors 145\n",
      "Iteration 5000: loss 0.44109 errors 142\n",
      "Iteration 6000: loss 0.43659 errors 142\n",
      "Iteration 7000: loss 0.43442 errors 142\n",
      "Iteration 8000: loss 0.43303 errors 142\n",
      "Iteration 9000: loss 0.43218 errors 142\n",
      "Iteration 10000: loss 0.43151 errors 143\n",
      "Iteration 11000: loss 0.43098 errors 142\n",
      "Iteration 12000: loss 0.43059 errors 142\n",
      "Iteration 13000: loss 0.43028 errors 142\n",
      "Iteration 14000: loss 0.43004 errors 142\n",
      "Iteration 15000: loss 0.42984 errors 142\n",
      "Iteration 16000: loss 0.42969 errors 142\n",
      "Iteration 17000: loss 0.42955 errors 141\n",
      "Iteration 18000: loss 0.42944 errors 141\n",
      "Iteration 19000: loss 0.42935 errors 141\n",
      "Iteration 1000: loss 0.69295 errors 392\n",
      "Iteration 2000: loss 0.69295 errors 392\n",
      "Iteration 1000: loss 0.69295 errors 392\n",
      "Iteration 2000: loss 0.69295 errors 392\n",
      "Iteration 1000: loss 0.43489 errors 145\n",
      "Iteration 2000: loss 0.43323 errors 145\n",
      "Iteration 3000: loss 0.43266 errors 145\n",
      "Iteration 4000: loss 0.43241 errors 145\n",
      "Iteration 5000: loss 0.43225 errors 145\n",
      "Iteration 6000: loss 0.43210 errors 144\n",
      "Iteration 7000: loss 0.43196 errors 144\n",
      "Iteration 8000: loss 0.43186 errors 144\n",
      "Iteration 1000: loss 0.53055 errors 214\n",
      "Iteration 2000: loss 0.52399 errors 210\n",
      "Iteration 3000: loss 0.51829 errors 201\n",
      "Iteration 4000: loss 0.51145 errors 195\n",
      "Iteration 5000: loss 0.50453 errors 194\n",
      "Iteration 6000: loss 0.49364 errors 185\n",
      "Iteration 7000: loss 0.45400 errors 154\n",
      "Iteration 8000: loss 0.44116 errors 150\n",
      "Iteration 9000: loss 0.43577 errors 150\n",
      "Iteration 10000: loss 0.43289 errors 148\n",
      "Iteration 11000: loss 0.43112 errors 150\n",
      "Iteration 12000: loss 0.42992 errors 148\n",
      "Iteration 13000: loss 0.42912 errors 151\n",
      "Iteration 14000: loss 0.42815 errors 149\n",
      "Iteration 15000: loss 0.42684 errors 149\n",
      "Iteration 16000: loss 0.42586 errors 148\n",
      "Iteration 17000: loss 0.42479 errors 147\n",
      "Iteration 18000: loss 0.42418 errors 146\n",
      "Iteration 19000: loss 0.42344 errors 148\n",
      "Iteration 20000: loss 0.42283 errors 148\n",
      "Iteration 1000: loss 0.43521 errors 144\n",
      "Iteration 2000: loss 0.43241 errors 144\n",
      "Iteration 3000: loss 0.43165 errors 144\n",
      "Iteration 4000: loss 0.43137 errors 144\n",
      "Iteration 5000: loss 0.43124 errors 145\n",
      "Iteration 6000: loss 0.43115 errors 145\n",
      "Iteration 1000: loss 0.43861 errors 153\n",
      "Iteration 2000: loss 0.43177 errors 147\n",
      "Iteration 3000: loss 0.42919 errors 137\n",
      "Iteration 4000: loss 0.42760 errors 137\n",
      "Iteration 5000: loss 0.42648 errors 136\n",
      "Iteration 6000: loss 0.42558 errors 136\n",
      "Iteration 7000: loss 0.42479 errors 134\n",
      "Iteration 8000: loss 0.42406 errors 135\n",
      "Iteration 9000: loss 0.42331 errors 135\n",
      "Iteration 10000: loss 0.42262 errors 135\n",
      "Iteration 11000: loss 0.42197 errors 134\n",
      "Iteration 12000: loss 0.42137 errors 134\n",
      "Iteration 13000: loss 0.42079 errors 137\n",
      "Iteration 14000: loss 0.42022 errors 137\n",
      "Iteration 15000: loss 0.41968 errors 137\n",
      "Iteration 16000: loss 0.41918 errors 138\n",
      "Iteration 17000: loss 0.41872 errors 137\n",
      "Iteration 18000: loss 0.41829 errors 137\n",
      "Iteration 19000: loss 0.41787 errors 137\n",
      "Iteration 20000: loss 0.41748 errors 137\n",
      "Iteration 1000: loss 0.43634 errors 149\n",
      "Iteration 2000: loss 0.43166 errors 143\n",
      "Iteration 3000: loss 0.43039 errors 144\n",
      "Iteration 4000: loss 0.42981 errors 141\n",
      "Iteration 5000: loss 0.42947 errors 141\n",
      "Iteration 6000: loss 0.42918 errors 140\n",
      "Iteration 7000: loss 0.42895 errors 140\n",
      "Iteration 8000: loss 0.42876 errors 140\n",
      "Iteration 9000: loss 0.42860 errors 140\n",
      "Iteration 10000: loss 0.42848 errors 140\n",
      "Iteration 11000: loss 0.42836 errors 140\n",
      "Iteration 12000: loss 0.42826 errors 140\n",
      "Iteration 13000: loss 0.42814 errors 140\n",
      "Iteration 14000: loss 0.42801 errors 140\n",
      "Iteration 15000: loss 0.42789 errors 140\n",
      "Iteration 16000: loss 0.42778 errors 140\n",
      "Iteration 17000: loss 0.42768 errors 140\n",
      "Best for noisy data H=2: error=134, iterations=6165\n",
      "Iteration 1000: loss 0.43598 errors 143\n",
      "Iteration 2000: loss 0.43289 errors 145\n",
      "Iteration 3000: loss 0.43218 errors 144\n",
      "Iteration 4000: loss 0.43191 errors 145\n",
      "Iteration 5000: loss 0.43179 errors 145\n",
      "Iteration 6000: loss 0.43171 errors 145\n",
      "Iteration 1000: loss 0.42565 errors 161\n",
      "Iteration 2000: loss 0.42196 errors 153\n",
      "Iteration 3000: loss 0.42057 errors 153\n",
      "Iteration 4000: loss 0.41980 errors 150\n",
      "Iteration 5000: loss 0.41931 errors 149\n",
      "Iteration 6000: loss 0.41895 errors 149\n",
      "Iteration 7000: loss 0.41864 errors 149\n",
      "Iteration 8000: loss 0.41839 errors 149\n",
      "Iteration 9000: loss 0.41820 errors 149\n",
      "Iteration 10000: loss 0.41804 errors 149\n",
      "Iteration 11000: loss 0.41790 errors 149\n",
      "Iteration 12000: loss 0.41775 errors 149\n",
      "Iteration 13000: loss 0.41760 errors 149\n",
      "Iteration 14000: loss 0.41745 errors 148\n",
      "Iteration 15000: loss 0.41727 errors 148\n",
      "Iteration 16000: loss 0.41712 errors 147\n",
      "Iteration 17000: loss 0.41701 errors 147\n",
      "Iteration 18000: loss 0.41693 errors 147\n",
      "Iteration 1000: loss 0.54006 errors 225\n",
      "Iteration 2000: loss 0.50136 errors 213\n",
      "Iteration 3000: loss 0.46723 errors 168\n",
      "Iteration 4000: loss 0.44862 errors 154\n",
      "Iteration 5000: loss 0.43942 errors 149\n",
      "Iteration 6000: loss 0.43441 errors 148\n",
      "Iteration 7000: loss 0.43124 errors 145\n",
      "Iteration 8000: loss 0.42868 errors 147\n",
      "Iteration 9000: loss 0.42693 errors 147\n",
      "Iteration 10000: loss 0.42563 errors 147\n",
      "Iteration 11000: loss 0.42455 errors 144\n",
      "Iteration 12000: loss 0.42363 errors 146\n",
      "Iteration 13000: loss 0.42294 errors 144\n",
      "Iteration 14000: loss 0.42210 errors 144\n",
      "Iteration 15000: loss 0.42143 errors 144\n",
      "Iteration 16000: loss 0.42088 errors 143\n",
      "Iteration 17000: loss 0.42045 errors 143\n",
      "Iteration 18000: loss 0.42007 errors 144\n",
      "Iteration 19000: loss 0.41975 errors 144\n",
      "Iteration 20000: loss 0.41948 errors 145\n",
      "Iteration 1000: loss 0.43642 errors 143\n",
      "Iteration 2000: loss 0.43295 errors 144\n",
      "Iteration 3000: loss 0.43216 errors 144\n",
      "Iteration 4000: loss 0.43184 errors 145\n",
      "Iteration 5000: loss 0.43156 errors 143\n",
      "Iteration 6000: loss 0.43137 errors 144\n",
      "Iteration 7000: loss 0.43124 errors 145\n",
      "Iteration 8000: loss 0.43115 errors 145\n",
      "Iteration 1000: loss 0.42558 errors 148\n",
      "Iteration 2000: loss 0.42518 errors 144\n",
      "Iteration 3000: loss 0.42515 errors 143\n",
      "Iteration 1000: loss 0.41636 errors 160\n",
      "Iteration 2000: loss 0.41280 errors 159\n",
      "Iteration 3000: loss 0.41102 errors 158\n",
      "Iteration 4000: loss 0.40966 errors 158\n",
      "Iteration 5000: loss 0.40620 errors 156\n",
      "Iteration 6000: loss 0.40488 errors 156\n",
      "Iteration 7000: loss 0.40352 errors 156\n",
      "Iteration 8000: loss 0.40225 errors 157\n",
      "Iteration 9000: loss 0.40093 errors 155\n",
      "Iteration 10000: loss 0.39980 errors 154\n",
      "Iteration 11000: loss 0.39872 errors 154\n",
      "Iteration 12000: loss 0.39763 errors 154\n",
      "Iteration 13000: loss 0.39658 errors 153\n",
      "Iteration 14000: loss 0.39559 errors 153\n",
      "Iteration 15000: loss 0.39463 errors 153\n",
      "Iteration 16000: loss 0.39370 errors 155\n",
      "Iteration 17000: loss 0.39272 errors 155\n",
      "Iteration 18000: loss 0.39169 errors 154\n",
      "Iteration 19000: loss 0.39083 errors 154\n",
      "Iteration 20000: loss 0.39003 errors 155\n",
      "Iteration 1000: loss 0.42945 errors 151\n",
      "Iteration 2000: loss 0.42582 errors 146\n",
      "Iteration 3000: loss 0.42396 errors 144\n",
      "Iteration 4000: loss 0.42280 errors 144\n",
      "Iteration 5000: loss 0.42180 errors 143\n",
      "Iteration 6000: loss 0.42066 errors 142\n",
      "Iteration 7000: loss 0.41939 errors 143\n",
      "Iteration 8000: loss 0.41809 errors 140\n",
      "Iteration 9000: loss 0.41676 errors 140\n",
      "Iteration 10000: loss 0.41546 errors 138\n",
      "Iteration 11000: loss 0.41416 errors 135\n",
      "Iteration 12000: loss 0.41258 errors 135\n",
      "Iteration 13000: loss 0.41071 errors 133\n",
      "Iteration 14000: loss 0.40861 errors 131\n",
      "Iteration 15000: loss 0.40647 errors 129\n",
      "Iteration 16000: loss 0.40413 errors 128\n",
      "Iteration 17000: loss 0.40191 errors 126\n",
      "Iteration 18000: loss 0.39989 errors 127\n",
      "Iteration 19000: loss 0.39807 errors 127\n",
      "Iteration 20000: loss 0.39640 errors 128\n",
      "Iteration 1000: loss 0.42874 errors 133\n",
      "Iteration 2000: loss 0.40324 errors 126\n",
      "Iteration 3000: loss 0.39295 errors 126\n",
      "Iteration 4000: loss 0.38720 errors 129\n",
      "Iteration 5000: loss 0.38327 errors 129\n",
      "Iteration 6000: loss 0.38070 errors 131\n",
      "Iteration 7000: loss 0.37887 errors 131\n",
      "Iteration 8000: loss 0.37750 errors 131\n",
      "Iteration 9000: loss 0.37643 errors 129\n",
      "Iteration 10000: loss 0.37559 errors 127\n",
      "Iteration 11000: loss 0.37478 errors 127\n",
      "Iteration 12000: loss 0.37418 errors 127\n",
      "Iteration 13000: loss 0.37372 errors 126\n",
      "Iteration 14000: loss 0.37335 errors 125\n",
      "Iteration 15000: loss 0.37306 errors 125\n",
      "Iteration 16000: loss 0.37282 errors 125\n",
      "Iteration 17000: loss 0.37261 errors 125\n",
      "Iteration 18000: loss 0.37244 errors 126\n",
      "Iteration 19000: loss 0.37229 errors 126\n",
      "Iteration 20000: loss 0.37216 errors 127\n",
      "Iteration 1000: loss 0.43215 errors 144\n",
      "Iteration 2000: loss 0.42703 errors 139\n",
      "Iteration 3000: loss 0.42558 errors 134\n",
      "Iteration 4000: loss 0.42467 errors 133\n",
      "Iteration 5000: loss 0.42399 errors 133\n",
      "Iteration 6000: loss 0.42340 errors 133\n",
      "Iteration 7000: loss 0.42282 errors 133\n",
      "Iteration 8000: loss 0.42224 errors 133\n",
      "Iteration 9000: loss 0.42162 errors 134\n",
      "Iteration 10000: loss 0.42092 errors 135\n",
      "Iteration 11000: loss 0.42016 errors 135\n",
      "Iteration 12000: loss 0.41930 errors 136\n",
      "Iteration 13000: loss 0.41837 errors 136\n",
      "Iteration 14000: loss 0.41715 errors 136\n",
      "Iteration 15000: loss 0.41561 errors 135\n",
      "Iteration 16000: loss 0.41364 errors 136\n",
      "Iteration 17000: loss 0.41105 errors 135\n",
      "Iteration 18000: loss 0.40803 errors 136\n",
      "Iteration 19000: loss 0.40528 errors 133\n",
      "Iteration 20000: loss 0.40271 errors 131\n",
      "Iteration 1000: loss 0.42289 errors 141\n",
      "Iteration 2000: loss 0.42180 errors 140\n",
      "Iteration 3000: loss 0.42068 errors 139\n",
      "Iteration 4000: loss 0.41946 errors 139\n",
      "Iteration 5000: loss 0.41811 errors 140\n",
      "Iteration 6000: loss 0.41661 errors 141\n",
      "Iteration 7000: loss 0.41496 errors 139\n",
      "Iteration 8000: loss 0.41318 errors 138\n",
      "Iteration 9000: loss 0.41126 errors 137\n",
      "Iteration 10000: loss 0.40923 errors 136\n",
      "Iteration 11000: loss 0.40709 errors 134\n",
      "Iteration 12000: loss 0.40487 errors 135\n",
      "Iteration 13000: loss 0.40261 errors 136\n",
      "Iteration 14000: loss 0.40033 errors 138\n",
      "Iteration 15000: loss 0.39805 errors 137\n",
      "Iteration 16000: loss 0.39579 errors 134\n",
      "Iteration 17000: loss 0.39357 errors 133\n",
      "Iteration 18000: loss 0.39143 errors 131\n",
      "Iteration 19000: loss 0.38940 errors 130\n",
      "Iteration 20000: loss 0.38747 errors 127\n",
      "Best for noisy data H=4: error=125, iterations=1533\n",
      "\n",
      "Summary Table (for LaTeX):\n",
      "data\tH value\titerations\terror rate\n",
      "data1.txt\t2\t185\t10\n",
      "data1.txt\t4\t15955\t0\n",
      "data2.txt\t2\t386\t0\n",
      "data2.txt\t4\t506\t0\n",
      "data3.txt\t2\t15257\t5\n",
      "data3.txt\t4\t4062\t7\n",
      "data4.txt\t2\t2686\t0\n",
      "data4.txt\t4\t46\t0\n",
      "data5.txt\t2\t2044\t0\n",
      "data5.txt\t4\t3334\t0\n",
      "noisy\t2\t6165\t134\n",
      "noisy\t4\t1533\t125\n",
      "\n",
      "Plots saved in 'plots/' directory.\n"
     ]
    }
   ],
   "source": [
    "# Neural Net Experiments for Homework 10, Question 5\n",
    "# --------------------------------------------------\n",
    "# This script automates the experiments described in Question 5.\n",
    "# It uses the provided helper functions and model structure, and\n",
    "# extends them for the two-hidden-layer case. Results are printed\n",
    "# for LaTeX table inclusion.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --------------------------\n",
    "# Helper Functions (from test.tex)\n",
    "# --------------------------\n",
    "\n",
    "def load_data(datafile):\n",
    "    data = np.loadtxt(datafile)\n",
    "    n, p = data.shape\n",
    "    rawx = data[:, 0:2]\n",
    "    rawy = data[:, 2]\n",
    "    x = torch.tensor(rawx, dtype=torch.float)\n",
    "    y = torch.reshape(torch.tensor((rawy + 1.0) / 2.0, dtype=torch.float), [n, 1])\n",
    "    return x, y\n",
    "\n",
    "def plot_data(x, y, show=True):\n",
    "    x_min = min(x[:, 0]) - 1\n",
    "    x_max = max(x[:, 0]) + 1\n",
    "    y_min = min(x[:, 1]) - 1\n",
    "    y_max = max(x[:, 1]) + 1\n",
    "    pos = (torch.squeeze(y) == 1)\n",
    "    neg = (torch.squeeze(y) == 0)\n",
    "    plt.plot(x[pos, 0], x[pos, 1], 'ro')\n",
    "    plt.plot(x[neg, 0], x[neg, 1], 'k^')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_boundary(x, y, model, title=None, show=True):\n",
    "    x_min = min(x[:, 0]) - 1\n",
    "    x_max = max(x[:, 0]) + 1\n",
    "    y_min = min(x[:, 1]) - 1\n",
    "    y_max = max(x[:, 1]) + 1\n",
    "    delta = 0.05\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, delta), np.arange(y_min, y_max, delta))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    gn, gp = grid.shape\n",
    "    Z = np.zeros(gn)\n",
    "    for i in range(gn):\n",
    "        pred = model(torch.tensor(grid[i, :], dtype=torch.float))\n",
    "        Z[i] = int(pred > 0.5)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.PRGn, vmin=-3, vmax=3)\n",
    "    pos = (torch.squeeze(y) == 1)\n",
    "    neg = (torch.squeeze(y) == 0)\n",
    "    plt.plot(x[pos, 0], x[pos, 1], 'ro')\n",
    "    plt.plot(x[neg, 0], x[neg, 1], 'k^')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def error_rate(y1, y2):\n",
    "    sum = 0.0\n",
    "    for i in range(0, y1.size()[0]):\n",
    "        sum += ((y1[i] - 0.5) * (y2[i] - 0.5) <= 0.0)\n",
    "    return int(sum)\n",
    "\n",
    "# --------------------------\n",
    "# Model Definitions\n",
    "# --------------------------\n",
    "\n",
    "def make_one_hidden_layer_model(input_dim, hidden_dim):\n",
    "    # Returns a model with one hidden layer of size hidden_dim\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_dim, hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_dim, 1),\n",
    "        torch.nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "def make_two_hidden_layer_model(input_dim, hidden_dim):\n",
    "    # Returns a model with two hidden layers, each of size hidden_dim\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_dim, hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(hidden_dim, 1),\n",
    "        torch.nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "# --------------------------\n",
    "# Training Function\n",
    "# --------------------------\n",
    "\n",
    "def train_model(model, x, y, max_iters=20000, tol=1e-4, learning_rate=0.5, print_every=1000):\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    prev_loss = 1.0\n",
    "    done = False\n",
    "    t = 1\n",
    "    best_model_state = None\n",
    "    best_error = y.size()[0]\n",
    "    best_iter = 0\n",
    "    while not done and t < max_iters:\n",
    "        y_pred = model(x)\n",
    "        t += 1\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        err = error_rate(y_pred, y)\n",
    "        if err < best_error:\n",
    "            best_error = err\n",
    "            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "            best_iter = t\n",
    "        if t % print_every == 0:\n",
    "            print(f\"Iteration {t}: loss {loss.item():.5f} errors {err}\")\n",
    "            if abs(prev_loss - loss.item()) < tol:\n",
    "                done = True\n",
    "            prev_loss = loss.item()\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * (1.0 / np.sqrt(t)) * param.grad\n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model, best_iter, best_error\n",
    "\n",
    "# --------------------------\n",
    "# Experiment 1: Five Datasets, One Hidden Layer\n",
    "# --------------------------\n",
    "\n",
    "def experiment_one_hidden_layer(data_files, H_values, runs_per_setting=5, plot_dir=\"plots\"):\n",
    "    results = []\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    for datafile in data_files:\n",
    "        x, y = load_data(datafile)\n",
    "        for H in H_values:\n",
    "            best_error = y.size()[0]\n",
    "            best_model = None\n",
    "            best_iter = 0\n",
    "            for run in range(runs_per_setting):\n",
    "                torch.manual_seed(run)  # For reproducibility\n",
    "                model = make_one_hidden_layer_model(2, H)\n",
    "                model, iters, err = train_model(model, x, y)\n",
    "                if err < best_error:\n",
    "                    best_error = err\n",
    "                    best_model = model\n",
    "                    best_iter = iters\n",
    "            # Plot and save the best model for this setting\n",
    "            plot_path = os.path.join(plot_dir, f\"{os.path.splitext(datafile)[0]}_H{H}.png\")\n",
    "            plot_boundary(x, y, best_model, title=f\"{datafile}, H={H}\", show=False)\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            results.append({\n",
    "                \"data\": datafile,\n",
    "                \"H\": H,\n",
    "                \"iterations\": best_iter,\n",
    "                \"error_rate\": best_error,\n",
    "                \"plot_path\": plot_path\n",
    "            })\n",
    "            print(f\"Best for {datafile} H={H}: error={best_error}, iterations={best_iter}\")\n",
    "    return results\n",
    "\n",
    "# --------------------------\n",
    "# Experiment 2: Noisy Data, Two Hidden Layers\n",
    "# --------------------------\n",
    "\n",
    "def generate_noisy_data(n=800, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    X_train = np.random.rand(n, 2)\n",
    "    x1 = X_train[:, 0]\n",
    "    x2 = X_train[:, 1]\n",
    "    y_train = ((np.exp(-((x1 - 0.5) * 6) ** 2) * 2 * ((x1 - 0.5) * 6) + 1) / 2 - x2) > 0\n",
    "    idx = np.random.choice(range(n), size=(int(n * 0.03),))\n",
    "    y_train[idx] = ~y_train[idx]\n",
    "    x = torch.tensor(X_train, dtype=torch.float) * 10\n",
    "    y = torch.reshape(torch.tensor(y_train, dtype=torch.float), [n, 1])\n",
    "    return x, y\n",
    "\n",
    "def experiment_two_hidden_layers(H, runs=10, error_threshold=100, plot_dir=\"plots\"):\n",
    "    x, y = generate_noisy_data()\n",
    "    best_error = y.size()[0]\n",
    "    best_model = None\n",
    "    best_iter = 0\n",
    "    for run in range(runs):\n",
    "        torch.manual_seed(run)\n",
    "        model = make_two_hidden_layer_model(2, H)\n",
    "        model, iters, err = train_model(model, x, y)\n",
    "        if err < best_error:\n",
    "            best_error = err\n",
    "            best_model = model\n",
    "            best_iter = iters\n",
    "        if best_error < error_threshold:\n",
    "            break\n",
    "    plot_path = os.path.join(plot_dir, f\"noisy_H{H}.png\")\n",
    "    plot_boundary(x, y, best_model, title=f\"Noisy Data, H={H}\", show=False)\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Best for noisy data H={H}: error={best_error}, iterations={best_iter}\")\n",
    "    return {\n",
    "        \"data\": \"noisy\",\n",
    "        \"H\": H,\n",
    "        \"iterations\": best_iter,\n",
    "        \"error_rate\": best_error,\n",
    "        \"plot_path\": plot_path\n",
    "    }\n",
    "\n",
    "# --------------------------\n",
    "# Main Execution\n",
    "# --------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Experiment 1: Five datasets, two H values each\n",
    "    data_files = [f\"data{i}.txt\" for i in range(1, 6)]\n",
    "    H_values = [2, 4]  # Example values; adjust as needed\n",
    "    results_one = experiment_one_hidden_layer(data_files, H_values, runs_per_setting=5)\n",
    "\n",
    "    # Experiment 2: Noisy data, two hidden layers\n",
    "    results_two = []\n",
    "    for H in [2, 4]:\n",
    "        res = experiment_two_hidden_layers(H, runs=10, error_threshold=100)\n",
    "        results_two.append(res)\n",
    "\n",
    "    # Print summary for LaTeX table\n",
    "    print(\"\\nSummary Table (for LaTeX):\")\n",
    "    print(\"data\\tH value\\titerations\\terror rate\")\n",
    "    for r in results_one + results_two:\n",
    "        print(f\"{r['data']}\\t{r['H']}\\t{r['iterations']}\\t{r['error_rate']}\")\n",
    "\n",
    "    print(\"\\nPlots saved in 'plots/' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': 'data1.txt', 'H': 2, 'iterations': 185, 'error_rate': 10, 'plot_path': 'plots/data1_H2.png'}, {'data': 'data1.txt', 'H': 4, 'iterations': 15955, 'error_rate': 0, 'plot_path': 'plots/data1_H4.png'}, {'data': 'data2.txt', 'H': 2, 'iterations': 386, 'error_rate': 0, 'plot_path': 'plots/data2_H2.png'}, {'data': 'data2.txt', 'H': 4, 'iterations': 506, 'error_rate': 0, 'plot_path': 'plots/data2_H4.png'}, {'data': 'data3.txt', 'H': 2, 'iterations': 15257, 'error_rate': 5, 'plot_path': 'plots/data3_H2.png'}, {'data': 'data3.txt', 'H': 4, 'iterations': 4062, 'error_rate': 7, 'plot_path': 'plots/data3_H4.png'}, {'data': 'data4.txt', 'H': 2, 'iterations': 2686, 'error_rate': 0, 'plot_path': 'plots/data4_H2.png'}, {'data': 'data4.txt', 'H': 4, 'iterations': 46, 'error_rate': 0, 'plot_path': 'plots/data4_H4.png'}, {'data': 'data5.txt', 'H': 2, 'iterations': 2044, 'error_rate': 0, 'plot_path': 'plots/data5_H2.png'}, {'data': 'data5.txt', 'H': 4, 'iterations': 3334, 'error_rate': 0, 'plot_path': 'plots/data5_H4.png'}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>1. Various helper functions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads in a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datafile):\n",
    "    data = np.loadtxt(datafile)\n",
    "    n,p = data.shape\n",
    "    rawx = data[:,0:2]\n",
    "    rawy = data[:,2]\n",
    "    x = torch.tensor(rawx, dtype=torch.float)\n",
    "    y = torch.reshape(torch.tensor((rawy+1.0)/2.0, dtype=torch.float), [n,1])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x,y):\n",
    "    x_min = min(x[:,0]) - 1\n",
    "    x_max = max(x[:,0]) + 1\n",
    "    y_min = min(x[:,1]) - 1\n",
    "    y_max = max(x[:,1]) + 1\n",
    "    pos = (torch.squeeze(y) == 1)\n",
    "    neg = (torch.squeeze(y) == 0)\n",
    "    plt.plot(x[pos,0], x[pos,1], 'ro')\n",
    "    plt.plot(x[neg,0], x[neg,1], 'k^')\n",
    "    plt.xlim(x_min,x_max)\n",
    "    plt.ylim(y_min,y_max)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function plots a decision boundary as well as the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(x,y,model):\n",
    "    \n",
    "    x_min = min(x[:,0]) - 1\n",
    "    x_max = max(x[:,0]) + 1\n",
    "    y_min = min(x[:,1]) - 1\n",
    "    y_max = max(x[:,1]) + 1\n",
    "\n",
    "    delta = 0.05\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, delta), np.arange(y_min, y_max, delta))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    gn, gp = grid.shape\n",
    "    Z = np.zeros(gn)\n",
    "    for i in range(gn):\n",
    "        pred = model(torch.tensor(grid[i,:], dtype=torch.float))\n",
    "        Z[i] = int(pred > 0.5)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.PRGn, vmin=-3, vmax=3)\n",
    "\n",
    "    # Plot also the training points\n",
    "    pos = (torch.squeeze(y) == 1)\n",
    "    neg = (torch.squeeze(y) == 0)\n",
    "    plt.plot(x[pos,0], x[pos,1], 'ro')\n",
    "    plt.plot(x[neg,0], x[neg,1], 'k^')\n",
    "\n",
    "    plt.xlim(x_min,x_max)\n",
    "    plt.ylim(y_min,y_max)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the error rate of the predicted labels `y1` given the true labels `y2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(y1, y2):\n",
    "    sum = 0.0\n",
    "    for i in range(0,y1.size()[0]):\n",
    "        sum += ((y1[i]-0.5) * (y2[i]-0.5) <= 0.0)\n",
    "    return int(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>2. Experiments with toy data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in one of the data sets and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = load_data('data1.txt')\n",
    "plot_data(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train a feedforward net on it. This takes many iterations of gradient descent (backpropagation). We'll print the status every 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train a neural net\n",
    "#\n",
    "# d is input dimension\n",
    "# H is hidden dimension\n",
    "d = 2\n",
    "H = 4\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use binary cross entropy (BCE) as our loss function.\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "prev_loss = 1.0\n",
    "learning_rate = 0.25\n",
    "done = False\n",
    "t = 1\n",
    "tol = 1e-4\n",
    "while not(done):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "    t = t+1\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 1000 == 0:\n",
    "        print('Iteration %d: loss %0.5f errors %d' % \n",
    "              (t, loss.item(), error_rate(y_pred, y)))\n",
    "        if (prev_loss - loss.item() < tol):\n",
    "            done = True\n",
    "        prev_loss = loss.item()\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * (1.0/np.sqrt(t)) * param.grad\n",
    "print(\"Number of training errors:\", error_rate(model(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what kind of a boundary we got!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(x,y,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>3. A different data set</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell generates a data set of 800 points in which the labels are noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 800\n",
    "np.random.seed(0)\n",
    "X_train = np.random.rand(n,2)\n",
    "x1 = X_train[:,0]\n",
    "x2 = X_train[:,1]\n",
    "y_train = ((np.exp(-((x1-0.5)*6)**2)*2*((x1-0.5)*6)+1)/2-x2)>0 \n",
    "\n",
    "idx = np.random.choice(range(n),size=(int(n*0.03),))\n",
    "y_train[idx] = ~y_train[idx]\n",
    "x = torch.tensor(X_train, dtype=torch.float) * 10\n",
    "y = torch.reshape(torch.tensor(y_train, dtype=torch.float), [n,1])\n",
    "plot_data(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='magenta'>Define a neural net with <b>two</b> hidden layers, each containing the same number of nodes. <em>Hint:</em> Start with the code above and just make a small tweak to it.</font>\n",
    "\n",
    "<font color='magenta'>Train the net a few times, and print the decision boundary for the best (lowest-error) model that you find.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
