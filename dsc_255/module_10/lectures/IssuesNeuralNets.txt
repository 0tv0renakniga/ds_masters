(light music) - We have been creating neural net training as an optimization problem. We're looking for a model that minimizes a loss function. Now in the case of neural nets, this loss function is unfortunately highly non-convex. And as a result, people have developed all sorts of tricks to help it guide the optimization towards a more satisfactory solution. So let's take a look at some of these. So the first couple of tricks have to do with generalization. Anytime we are trying to learn a model in which there is a huge number of parameters, generalization is a very important issue. Okay, we've seen this, for example, when we were discussing decision trees or random forests. And it turns out that some of the insights and tricks from those earlier models have also been carried over to neural nets, although in a slightly different form. For instance, when we were talking about decision trees we saw this graph over here. What it says is that as you are growing a decision tree, as you're adding more and more nodes to the tree, as the tree is getting bigger, the training error goes down and eventually goes down to zero. But what we actually care about is the true error. And the true error will go down initially but at some point it starts to go up. So rather than grow the tree all the way to the end, we would actually like to stop growing the tree somewhere around here. How do we do that? Well, one way of doing it is to have a separate validation set and to keep track of the validation error and to stop growing the tree when the validation error starts to go up. Now, in the case of neural nets, there is a similar issue that arises and it has to do with the number of rounds of training. As you train more and more, as you train for more and more iterations, you can potentially drive the training error down further and further. But this doesn't necessarily mean that the true error is going down. So one way of dealing with this is, again, to have a separate validation set and to periodically check the validation error, say every 100 iterations of training. If you check the validation error and you find that it's actually higher than it was 100 iterations ago, than what you can do is to simply revert to that previous model, okay? It's quite simple. A second trick for improving generalization is called dropout and this is highly reminiscent of random forests. So what happens in dropout is, let's say that we are in the middle of training and our next data point is (x,y). So we introduce X at the bottom but now what we do is that we randomly kill off half of the internal nodes of the net. So for each node with probability a half, we remove it. So we kill off a large number of these nodes. And other than that, the computation proceeds exactly as before. We take our input, we do a forward pass, and then we do a backward pass in which we compute gradients. So the effect of this, the effect of dropping out half of the nodes at random is that it forces the network to have many alternative parts towards the correct answer. It builds in a sort of robustness. And at test time of course, we don't do this anymore. This is only something that we do during training. At test time, we keep the full network. Okay, so those are two tricks that help with generalization. There's also a bunch of tricks that facilitate the optimization that help guide the optimization process towards a better local optimum. One of these is called batch normalization. So to understand this, let's say we have a deep network. Let me just draw it as a box, and there's some internal layer over here. Let's look at a particular layer of the network. And let's think about the inputs to this layer. What are the inputs to this layer? Well, it's basically the previous layer. We apply a linear function followed by a ReLU. That's the input into this layer. Now, during the training process, the parameters of the network are continuously changing. And as a result, the distribution of these inputs into that particular layer is also changing. It's a kind of covariate shift. It's a kind of covariate shift that is internal to the network. So even though the distribution of actual inputs might remain the same, the inputs into this layer, into this internal layer, have a shifting distribution because the parameters are changing. So how can we compensate for this? One way of doing it is batch normalization. So if, as is typical, we are using batch stochastic gradient descent, let's say we have mini batches of 100 points. So then we look at the 100 points that are going into this layer, the 100 inputs going into the layer. So let's say that we have p inputs x1 through xp. If we have a batch of size 100, we have 100 values of these, okay? So 100 x1's, 100 x2's, 100 xp's. What we can do is to compute the mean and variance of each xi and normalize so that each of the xi's has mean zero in variance one. The formula for doing that is very simple, subtract the mean and divide by the variance. We include this little epsilon over here because it's possible that the variance is zero or very close to zero. So this normalizes the inputs going into each layer so that each batch has mean zero in variance one. And it turns out to be a very effective trick in the optimization process. Another thing to be aware of is that there are many variants of stochastic gradient descent that are popular when training neural nets. So let's just look at a couple of these. So suppose we have a particular parameter, some parameter in the network, and let's say our loss function looks like this. So it depends on the parameter and on the particular data point. The usual SGD update is of this form. The parameter at time t plus one is the parameter at time t minus some step size times the gradient. So g of t is the gradient of the loss at time t, on the data point that appears at time t. So this is the normal stochastic gradient update. One variant on this is called momentum. So what happens in momentum is that the parameter at time t plus one is the parameter at time t minus some value, v of t. But this value is not just the gradient at time t. It's the gradient at time t plus some accumulation of recent gradients. So this value v sub t is the gradient at time t times the step size plus some amount of other recent gradients. Some amount of the gradients in recent time steps. And that's why it's called the momentum term. Another optimization method is known as AdaGrad and this addresses the issue of setting the learning rates for different parameters. So one thing we know about learning rates, that is to say the step sizes, the eta sub t over here, is that it's common to start with a fairly large step size, and then, once the parameter has been tuned quite a lot, to gradually let the step size decrease. But when you have a large number of parameters, it's very often the case that some of them have already been changed quite a bit whereas others have not changed very much. And so that would really suggest that we'd want different step sizes for these different parameters. And that's what AdaGrad gives. It says that the step size is some constant eta divided by this term. And what this term is is the sum of the squared gradient so far. So if parameter at theta j has been updated a lot, in other words, if these past gradients are fairly large, then we're dividing by a large number and so the step size becomes small. If on the other hand, the parameter hasn't been updated much at all, in other words, these gradients are all pretty close to zero, then we get a much larger step size. So it's a nice simple way of adjusting step sizes to accommodate these different regimes for the various parameters. Okay, so that's quite a lot about neural network training. At this point, we've talked quite a bit about neural networks, we've provided a lot of background. And probably, the best way to learn more is to simply start experimenting, is to take some data and start playing around with PyTorch. 