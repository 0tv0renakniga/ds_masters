(upbeat music) - Okay, we will now turn to the backpropagation algorithm for training neural nets. So here, again, is a feed forward neural net. As we've seen, this can potentially have an astronomical number of parameters, but it's a very flexible model class that can approximate any continuous function arbitrarily well. How do we learn one of these things? As usual, we are gonna treat learning as an optimization problem. So let's say that the parameters of the entire net are W. This is a list of all the weights anywhere in the net. There could easily be millions of such weights. Okay? What we are going to do is to write down a loss function involving W, and then attempt to find the W that minimizes this loss. So what loss function shall we use? Well, that depends on whether we are solving a classification or regression problem. Let's say that we are doing classification and that we have k labels. In that case, what the network gives us for any data point x is probabilities of each of the labels. Okay, it tells us what is the probability of label one given x? What is the probability of label two given x, and so on? Okay, just like logistic regression. And like logistic regression we can simply use likelihood as our loss function. So let's say that we have a data set, x1 y1 through xn yn. What we want to do is define the parameters W that maximize the likelihood of this data. Okay, so what we want to maximize is the probability under the model W of the correct label of the first data point times the probability under W of the correct label of the second data point, times the probability of the correct label of the third data point, and so on. Okay, so that's the thing we want to maximize. Now, this is a product, so we can just hit it with a logarithm to make it into a sum, and to make it a minimization problem we can stick a minus in front. And so that's how we get this loss function over here. Okay, so this is the loss function we wanna minimize, and it's a loss function that's often called the cross entropy. So we wanna find the network parameters W that minimize the cross entropy loss. Now, in the case of logistic regression, this loss function was convex. What happens with neural nets, is it still convex? Unfortunately not. It is very, very far from convex. So we are not in this situation at all. We are very much in this situation. And so we cannot really hope to find the global optimum solution. We are resigned to getting stuck in some local optimum. And our main hope is really that this function class is so flexible that even one of these suboptimal solutions is actually pretty good. Now, in order to arrive at a local optimum, in order to try minimizing this loss function, as usual we'll use gradient descent or one of its variants. Okay, so what does this involve? Well, we start by initializing the parameters at random and then we keep tweaking the parameters. We keep moving the parameters in the negative direction of the gradient. Now, earlier we talked about three variants of gradient descent. There's gradient descent itself, where each update to W involves the entire training set. Now, in the neural net setting, data sets are often enormous and what this means is that gradient descent is really not viable. Each of these updates would just become too time consuming. Okay, so a much more attractive option is stochastic gradient dissent where each update involves just one data point. Now, the downside of SGD is that these updates, because they involve just a single point, are very noisy. So a common compromise is mini batch SGD, where each update just involves a fixed but modest number of data points. And that's the method that's typically used in training neural nets. Okay, now, regardless of which of these methods we use we do need to compute gradients. So what are the gradients we need, exactly? Well, let's look at our architecture again. Let's say we have a particular data point, x, y. So what we can do is to put x in the bottom over here. And what we'll get at the top are probabilities of the different labels, okay? And so what we can do is to look at the negative log probability of the correct label, okay? And that's our loss on this particular data point. So given x, y, we can easily do a forward pass in which we compute all the values of the intermediate nodes. We compute the values at the output layer. And we compute the loss as well. Okay, so it's easy to compute the loss. What we need, however, is the derivative of the loss with respect to all of the parameters. Okay? So with respect to the weight along this edge and the weight along this edge, and even the weight along these edges down here. How do we compute these derivatives? Well, at the top layer those derivatives are very easy to compute. The top layer is essentially just multi-class logistic regression, and that's something for which we computed derivatives explicitly earlier on, and the same derivatives continue to hold. But how do you compute the derivative of the loss with respect to some weight down here? How do you compute the effect of that weight on the loss all the way up here? In order to do this, we make heavy use of the chain rule of calculus. So let's take a look at what that is. So let's say that we have a function h that's actually a composition of two functions f and g, okay? So to compute h of x what you do is you first compute f of x, okay? And then you take whatever the answer is and you apply g to it. Okay, so you take x, you apply f and then you apply g to the answer. If that's the case, then the derivative of h is the derivative of g evaluated at f of x times the derivative of f evaluated at x. This is the chain rule of calculus. Another way to think of it is as follows. Okay, so let's say we have some value x, and we have y that is a function of x. Okay, so let's say y is f of x. And then we have z that's a function of y. So let's say z is equal to g of y. Okay, so now what we want to know is what is the derivative of z with respect to x? Okay, what's the derivative of z with respect to x? And here we have z being g of f of x, okay? So if we look at our chain rule, what that tells us is that the derivative of z with respect to x is just the derivative of z with respect to y times the derivative of y with respect to x. Okay, so these are two different ways to think of the same thing, the chain rule of calculus. This is the rule that we are gonna be applying over and over again in order to compute derivatives in a neural net. So let's go through the logic of backpropagation a little bit carefully, okay? And in order to do that, let's simplify things a little bit. Let's say that we have a neural net in which there are several layers, many layers, but each layer just contains a single node. Okay, so the input layer consists of x, which is just a single number, it's a one-dimensional input. And the next layer, the first hidden layer has just one node in it, h1, and the second hidden layer has just one node in it, h2, and the third hidden layer has just one node in it, h3, and so on. Okay, now I've drawn this net horizontally rather than vertically just to show this chain structure. Okay, so each node h sub i is just a linear function of its parent h sub i minus one. Okay, so h sub three, for example, is just a linear function, w3 times h2 plus some offset, and then we apply this non-linearity. Great. So once we have x we can just do a forward computation in which we compute all of these hidden values, h1, h2, h3. And at the end we get a prediction and we can compute the loss easily, okay? So that's the easy part. What we want are the derivatives of the loss. So we get the loss at the end. We want the derivatives of the loss with respect to these weights over here. How do we do that? Okay, so we're gonna do this in two steps. The first step is to say that in order to compute the derivative of the loss with respect to a particular weight w sub i, it's enough if we have the derivative of the loss with respect to h sub i, okay? Which is one of the hidden units. Why is this? So this follows from the chain rule, okay? So the derivative of the loss with respect to wi, that's something that we need in order to figure out how to update w sub i. Once we have this derivative, we are gonna take w sub i and move it in the negative direction of this derivative, okay? We are gonna subtract some multiple of this derivative from w sub i, okay? That's what we do in gradient descent. So we definitely need this derivative. Now, using the chain rule, we can say that the derivative of the loss with respect to wi is the derivative of the loss with respect to h sub i times the derivative of h sub i with respect to wi. And the second thing we can compute just from the formula for h sub i, okay? So given the formula for h sub i, the derivative of h sub i with respect to w sub i is the derivative of this thing over here with respect to w sub i. So that's the derivative of sigma evaluated at the value inside the sigma times the derivative of this thing inside with respect to w sub i, and that's just a linear function. So the derivative of that is h sub I minus one. Okay, so all these things over here are values that we have. We've computed all these h's in the forward pass. So if we have the derivative of the loss with respect to h sub i, that will automatically give us the derivative of the loss with respect to w sub i. Okay, so we've simplified our problem somewhat, or at least we've transformed it. Instead of asking for the derivatives with respect to w sub i, we are now looking for the derivatives of the loss with respect to the h sub i's. Okay? How do we get those? And what we are gonna do is to get those in a single backward pass, okay? We will start by computing the derivative of the loss with respect to this h sub l, then with respect to this h, then with respect to this h, and so on. Okay, so let's see how we get this. Okay, so let's say that we have already computed the derivative of the loss with respect to hi plus one. How do we get the derivative of the loss with respect to h sub i? Okay, well, let's write down the formula for hi sub one with respect to h sub i. h sub i plus one is after all just a linear function of h sub i along with this non-linear activation, okay? And so the derivative of the loss with respect to h sub i is, by the chain rule, the derivative of the loss with respect to h sub i plus one times the derivative of h sub i plus one with respect to h sub i. And this term we can get directly from this formula. It's the derivative of sigma evaluated at the stuff inside the parentheses, times the derivative of the stuff inside the parentheses with respect to h sub i which is just wi plus one. Okay? And once again, all of these terms are terms that we already have. We've computed all the h sub i's in our forward pass. So what this tells us is that as long as we know the derivative of the loss with respect to hi plus one, we automatically can get the derivative of the loss with respect to h sub i. And that's how we do a backward pass. So this gives us all of these derivatives, the derivative of the loss with respect to each of the h's. And what we saw before is that in turn, this automatically gives us the derivative of the loss with respect to each of the w's. So this is the backpropagation algorithm on a neural net that just looks like a single chain, okay? A net that has several layers, but in which each layer has just one node. Now, of course, the nets that we are actually gonna use are not gonna look like this. There will be many nodes in each layer. But it turns out that the logic remains exactly the same. It's just the notation that gets a little bit more cumbersome. Okay, so now you've seen how back propagation works. And in principle what you could do is given a particular net you could figure out all the derivatives and code up a stochastic gradient descent algorithm from scratch. Luckily, you probably will never need to do this. In PyTorch and TensorFlow and similar systems there is a built-in automatic differentiation. Okay, so all you need to do is to define the structure of the network and say what function is being computed at each node and what the overall loss function is. And the system will automatically figure out all the derivatives by itself and create the gradients in this way. Okay, so that's it for back propagation. This is an algorithm that was developed in the 1980s and is the method for training neural nets. 