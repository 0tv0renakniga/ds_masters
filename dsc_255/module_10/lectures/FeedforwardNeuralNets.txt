(light music) (swoosh effect) - Welcome back, everybody. Our next topic is neural networks or deep learning. Neural nets are prediction models that are loosely inspired by the brain. So this here is what a feedforward neural net looks like. Basically, you put the input in the bottom. It does a bunch of computations on it in a single forward pass. And then the answer comes out of the top, okay? So to be a little bit more specific, let's say that we have a d dimensional input. So the input consists of d features, x1 through xd. Then the input layer would have d nodes. So this node over here would contain the first feature, x1. This node would be x2, and this last node would be xd. Once we have the values at this input layer, we can compute the values in the next layer, the first hidden layer. And once we have those values, we can compute the second hidden layer, and so on. At any given point, we've reached a certain layer and we're computing the next layer. Okay, so what are these computations? Well, let's look at a particular internal node, say this one. Okay, let's call it h. How do we get the value at h? Well, h is connected to a few nodes at the previous layer, okay? We can think of those nodes as the parents of h. This particular h has only two parents. These, right here, okay? And the value of h is a simple function of the values of its parents. So as soon as we have those parent values, we can compute h as well. And this is the way in which the computation proceeds from bottom to top. So what are these functions? What are the functions at these individual nodes in the network? Let's pick a particular node h, okay? And let's say that it has m parents z1 through zm. So h is gonna be some sort of simple function of the values of these parents. What is this function exactly? So here's what the function looks like. First, we compute a linear function of the parents, okay? And this is a straightforward linear function, something of the form, w1, z1 plus w2, z2, and so on, plus an offset term b at the end, okay? So we compute this linear function. And if you like, you can think of w1 as being the weight associated with this edge. So we take z1 and we multiply it by w1. And w2 is the weight along this edge and wm is the weight along this edge, okay? And in addition, we have this offset term. Okay. So we compute this linear function of the parents, and then we apply a non-linear activation function, sigma. Now, there's several choices for this activation function. A very popular option is the ReLU function, okay? So ReLU, which stands for rectified linear unit. And this is what the ReLU is. So the ReLU takes a real value. So you've computed this linear function of the parents. That's some real number. If the number is positive, it just leaves it alone, okay? Just takes the number as it is. If it's negative, it makes it zero. So here's what it looks like. We have some real number U, which is the linear function of the parents. And this is what ReLU does to that number. If it's positive, it just keeps the number, so it looks like this. And if it's negative, then it zeros it out. So this is the shape of the ReLU activation function. Now, there's several other common activation functions as well. Perhaps the simplest is just the threshold function. What that does is that it takes the value, takes a real value, z. If it's negative, it replaces it with zero and if it's positive, it replaces it with one. So it looks like a step function like this. Another option is the Sigmoid function that we saw when we were talking about logistic regression. So it takes the value z, and it converts it to some number in the range zero to one, okay? And it looks like this roughly. The hyperbolic tangent is similar to the Sigmoid, except that instead of returning a value in the range zero to one, it returns a value in the range minus one to one. So it looks like this. So here's one, here's minus one, and returns a value like this. And then there's the ReLU that we mentioned before. So here are four common activation functions. So one question is, why do we need these at all? Why do we need non-linearities? Let's go back to our picture of the feedforward network and let's see what would happen if all of the functions we were using were purely linear functions without any non-linearities at all, okay? So we have our input layer x. Our next layer h1 would then just be a linear function of x, say W1x. And the next layer h2 would be a linear function of the previous layer, and so on. What's wrong with this? Well, if this is what we have, then we see that we can compose these layers. So for instance, h2 is equal to W2 times W1 times x. So in other words, h2 is just a linear function of x. So if all we are using is linear functions at the intermediate nodes, then the entire computation just gives us a linear function. Okay? It's a very simple class of functions. And if that's all we are doing, we don't need any hidden nodes at all, okay? So in order to get more sophisticated functions, in order to get a more expressive model class, we really need these non-linearities at the internal nodes of the net. Okay. So we've talked a lot about these internal nodes. What happens at the output layer? So that really depends upon the type of prediction problem we are trying to solve. Is it regression or classification? Let's say for concreteness that we wanna solve a classification problem in which there are k labels, okay? If that's what we are after, then typically, we would have k nodes in the output layer. And each of these nodes would correspond to one of the labels, okay? So perhaps the most common setting is one where we would want k probabilities, the probability of label one, the probability of label two. And of course, these values would have to sum to one. How do we get these probabilities? Well, we get them in exactly the same way as we saw for multi-class logistic regression, okay? So what we'll do is that each of these y values will simply be a linear function of the parents z1 through zm. And then, okay, that just gives us y values that are real values. In order to convert them into probabilities, we will use a softmax, okay? So this is what it looks like. The y's themselves are simple linear functions of the z's. And then we convert them into probabilities by saying that the probability of label j is proportional to e to the yj, okay? And so once you normalize, the probability of label j is e to the yj, divided by e to the y1 plus e to the y2, and so on, okay? And so in this way we get k probabilities that add up to one, exactly like multi-class logistic regression. Okay. Now let's talk a little bit about the complexity of these models. How many parameters do we have? Well, let's see. So for each internal node, we have a linear function of its parents. And as we've said, we can think of each of these edges as having an associated weight, okay? So if we ignore the offset terms, the number of parameters is basically just the number of edges in this graph, okay? The actual number is a little bit more because of the offsets. So how many edges are there over here? Well, let's look at two successive layers. Let's say that this layer has a thousand nodes, and let's say that this layer also has a thousand nodes, just for simplicity. And let's say that the layers are fully connected, okay? So all pairs of nodes are connected. How many edges are there? So if they're fully connected, the number of edges is a thousand times a thousand, namely a million, okay? So you can see that as we get more and more layers and as the layers get wider, the number of parameters in these networks can become truly astronomical, okay? What are the implications of this? Well, first of all, it means that these are very expressive models. They can express decision boundaries that are really quite complicated. The second consequence of having so many parameters is that probably we need very large data sets in order to fit models like this. So what kind of functions can we actually model using a neural net? So it turns out that there is complete clarity on this. If you look at any continuous function, f, there is some neural net that approximates f arbitrarily well. And what's more, the neural net has just one hidden layer, okay? So this is a very strong result. Any function, any continuous function can be approximated to arbitrary accuracy by a neural net with just one hidden layer. So this seems like good news but it also raises another question. Why do we ever use more than one hidden layer, okay, if we can model basically everything with just one hidden layer. So it turns out that if we use just one hidden layer, that layer might need to be really wide. It might need many, many, many nodes, okay? Whereas if we use a deeper network, it might be possible to have the individual layers of fairly modest size, okay? So one of the reasons for using a deeper network is that it might permit us to have an overall model that is more compact. Okay. So we've talked a little bit about feedforward neural nets, what they are, how they compute, and what kinds of functions they're able to represent. What we'll move to next is how we train these models. 