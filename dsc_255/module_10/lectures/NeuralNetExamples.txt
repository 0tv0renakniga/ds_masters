(light electronic music) (air whooshing) - So far, we've talked about neural nets at a fairly abstract level. Now let's look at some concrete examples. So over here, we have a data set with two labels. What sort of neural net would we want to use to classify this data? Well, let's see, so first of all, there are just two features, so we need two input nodes. So we'll create two nodes in the input layer. Let's call them x1 and x2. There are two labels, so we could use two output nodes, or we could just use one. You know, what we care about is the probability of the two labels, and since they add up to one, we just need the probability of label one, say. Okay, so let's say we just have one output node. How many hidden nodes do we need a hidden layer? Well, this boundary certainly is not linear, okay? So, we definitely need a hidden layer, but it's unclear how many hidden nodes we need. So let's just leave that variable for now. Let's say we have H hidden nodes. So H nodes in a hidden layer. And we could have more hidden layers, but let's just use one, since we do know that any continuous function can be arbitrarily well approximated with just one hidden layer. Okay, so we have H nodes in a hidden layer. And so, this is what our neural net will look like. Okay, and for simplicity, we'll just make it fully connected between each pair of layers. Okay, so that's the architecture of the net. Now we have to talk about the functions at each node. So we have an input layer with two nodes, one hidden layer with H nodes, an output layer with one node. From the input to hidden layer, we can just do the standard linear function with ReLU activation. For the output layer, since we just have one output node, we can use a sigmoid activation, since that gives us a probability in the range zero to one, okay, which is exactly what we want. So this then is the complete specification of the net up to the number of hidden nodes, H, okay? And so, that's something we now need to decide. So, how many hidden nodes should we use? Well, let's see. If we recall, each hidden node is a linear function of the input followed by a ReLU. So we could have a hidden node that basically does this, okay? It computes this linear function and then it uses the ReLU to zero out this side and to give a positive value on this side. So that's an example of a hidden node that we might have. And that would be really useful, since it would allow us to correctly classify this group of points. Similarly, we could have another hidden node that computed this linear function and used ReLU to zero out everything on this side and to give a positive value on this side. And that would allow us to correctly classify these nodes. So it looks like two hidden units should be enough. Let's try that. Sure enough, it works. Now, this was our first attempt. Let's try again. And this time, it hasn't worked. How can that be? Why is it the case that you try it two times and you get different answers, one good, one bad? Well, there's a lot of randomization in the training of the net. First of all, the parameters of the net are initialized at random. Second, the order of the data points is often randomized. Now, the loss function itself is highly non-convex. There are lots of local optima. And with different random choices, we will typically end up in different local optima, okay? And so, this is why you can get rather different answers for different attempts at learning. Okay, so this is what we got. We got one good, one bad. Let's move to another example. Okay, so now, once again, we have a nonlinear decision boundary. Let's use the same architecture. And the question again is, how many hidden units do we need? Okay? Now, using the same logic as last time, over here, it seems like it would be enough to have, say, four hidden units, okay? So you could have one hidden unit that basically was positive on that side, one that's positive over there, one that's positive over here, and one that's positive over here. So four hidden units should be more than enough. Maybe three is fine too. So let's try that. So here's our first attempt with four hidden units. It's pretty bad. In fact, it looks like it's actually just using two units. It's computing this linear function over here, and it's computing this linear function. And the other two hidden units are sort of being wasted in a sense, okay? So this has not worked out well. Let's try again. Once again, not very good. Okay, it seems like, again, it's only using two hidden units. And it's really not doing a good job of separating the classes. Let's try again. Another failure, okay? Let's try a fourth time. Finally, it's doing something sensible. So, in this case, it really took us several attempts to get a good answer. Now let's try with extra hidden units, okay? So, we're trying now with eight hidden units. We say this is the overparameterized setting because we have way more parameters than we need. And here, just on the first try, it does something reasonable. Okay, so overparameterization is often helpful in neural net training. Now let's look at this data set. This is a much more complex boundary. How many hidden units do we need this time? Well, it's hard to tell. So let's just try a bunch of different values. Okay, so first, we try with four, and it's completely useless. It basically finds a linear boundary, okay? So it's like it's using only one of the hidden units. Okay, let's double it and do eight. Slightly better, okay, but very close to linear. Let's double again and go to 16. Oh, this is regressed in fact. This has got worse. This is like the four case. So let's try 16 again and now we get something a little bit better, okay? It's still getting a lot of the data wrong. It's getting these points wrong, and it's getting these wrong. But it's a little better, okay? At least it's got a significant amount of nonlinearity. This is another attempt with 16. And again, it's getting about four of the points wrong. Let's go up to 32. Okay, this is a little bit like the 16 case. Another attempt with 32. Let's try again. Now we're getting really close. Now it only has, is making only one mistake. Now let's try with 64. Not good. Another attempt with 64. And a final attempt with 64. And finally, it's able to classify all of the training points correctly. So, all of these examples were very easy to code up using PyTorch, okay? And so let me just show you briefly two little snippets. So the first snippet over here defines what the network looks like, okay? So in our case, we had an input, which was two-dimensional, and we had a number of hidden nodes that we were experimenting with, okay? Over here, it says eight, but we experimented with lots of different values. Now the model consisted of a bunch of operations, okay, a bunch of operations performed on the input. The first operation was a linear mapping from d dimensions to H dimensions, the mapping from the input layer to the hidden layer, followed by a ReLU, okay? And then we had a linear mapping from the hidden layer to the output, followed by a sigmoid, in order to convert it into a probability. So that completely specifies the architecture of the net, as well as the kind of functions that are being used at each of the individual nodes, okay? So this completely defines the model. Now what we also need to define is the loss function. And over here, we're using the binary cross-entropy, the BCE loss, okay? Great, so this declares and initializes the network. Now, over here is the training procedure. So when we have a particular point x,y, how do we take a gradient step? So what we do is we first run x through the model and we get a prediction. This is the probability at the output. Then we compute the loss, okay? So we know what the true label is. We compute the loss, which is the binary cross-entropy loss. Great, so we have the loss. Now we zero out the derivatives, 'cause we're starting fresh. And we propagate the loss backwards. This is the back propagation step that propagates the loss backwards and gets the derivatives of the loss with respect to all of the parameters. And now we actually update the parameters. So for each parameter in the model, okay, so model.parameters is the list of all the parameters in the model, we take the parameter and move it in the negative direction of the gradient of that parameter. And that's it, okay? So that's the learning algorithm. So hopefully these few examples have given you a sense of both the expressiveness of neural nets and of some of the ways in which we train them using PyTorch. 