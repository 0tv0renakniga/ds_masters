\documentclass[11pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\usepackage{enumitem}
\geometry{margin=1in}

\title{Comprehensive Review: Neural Networks Handout}
\author{DSC 255 - Machine Learning Fundamentals}
\date{}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Overview of Feedforward Neural Networks}

A feedforward neural network is a layered model where inputs are passed forward through hidden layers to produce an output. Each hidden unit applies a nonlinear function to a weighted sum of its inputs.

\section{Network Architecture}

\begin{itemize}
    \item Input layer: Receives feature vector $x$
    \item Hidden layers: Apply transformations with weights and biases
    \item Output layer: Produces prediction $y$
\end{itemize}

Each hidden unit computes:
\[
h = \sigma(w_1 z_1 + w_2 z_2 + \cdots + w_m z_m + b)
\]

\section{Common Activation Functions}

\begin{enumerate}[label=(\alph*)]
    \item \textbf{Threshold (Heaviside)}:
    \[
    \sigma(z) = 
    \begin{cases}
    1 & \text{if } z \geq 0 \\
    0 & \text{otherwise}
    \end{cases}
    \]

    \item \textbf{Sigmoid}:
    \[
    \sigma(z) = \frac{1}{1 + e^{-z}}
    \]

    \item \textbf{Tanh}:
    \[
    \sigma(z) = \tanh(z)
    \]

    \item \textbf{ReLU (Rectified Linear Unit)}:
    \[
    \sigma(z) = \max(0, z)
    \]
\end{enumerate}

\section{Why Use Nonlinear Activations?}

Without nonlinearities, the entire network reduces to a single linear transformation:
\[
h_2 = W_2 W_1 x
\]
Nonlinear functions introduce expressive power and allow modeling of complex relationships.

\section{Output Layer and Softmax}

For classification with $k$ labels:
\begin{itemize}
    \item The final layer produces real-valued scores $y_1, \ldots, y_k$
    \item The softmax function converts them to probabilities:
    \[
    \text{Pr(label } j) = \frac{e^{y_j}}{\sum_{i=1}^k e^{y_i}}
    \]
\end{itemize}

\section{Universal Approximation Theorem}

Let $f: \mathbb{R}^d \to \mathbb{R}$ be a continuous function. Then there exists a neural network with one hidden layer that approximates $f$ arbitrarily well.

\begin{itemize}
    \item One hidden layer may require many nodes
    \item Using multiple layers can reduce the number of required nodes per layer
\end{itemize}

\section{Loss Function for Training}

For classification with $k$ labels, define:
\[
L(W) = -\sum_{i=1}^n \log \text{Pr}_W(y^{(i)} \mid x^{(i)})
\]

\begin{itemize}
    \item This is the \textbf{cross-entropy loss}
    \item $W$ are all weights and biases in the network
\end{itemize}

\section{Optimization Techniques}

\subsection*{Gradient Descent Variants}

\begin{itemize}
    \item \textbf{Batch Gradient Descent}: Full dataset per update
    \item \textbf{Stochastic Gradient Descent (SGD)}: Single data point per update
    \item \textbf{Mini-batch SGD}: A fixed-size subset per update (widely used)
\end{itemize}

\section{Chain Rule for Gradients}

\begin{itemize}
    \item If $h(x) = g(f(x))$, then:
    \[
    h'(x) = g'(f(x)) \cdot f'(x)
    \]
    \item For nested functions:
    \[
    \frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}
    \]
\end{itemize}

\section{Backpropagation in a Simple Chain}

In a network with one node per hidden layer:
\[
h_i = \sigma(w_i h_{i-1} + b_i)
\]

\begin{itemize}
    \item To update $w_i$, compute:
    \[
    \frac{dL}{dw_i} = \frac{dL}{dh_i} \cdot \sigma'(w_i h_{i-1} + b_i) \cdot h_{i-1}
    \]

    \item For previous layer:
    \[
    \frac{dL}{dh_i} = \frac{dL}{dh_{i+1}} \cdot \sigma'(w_{i+1} h_i + b_{i+1}) \cdot w_{i+1}
    \]
\end{itemize}

\section{Backpropagation Algorithm}

\begin{enumerate}
    \item \textbf{Forward Pass}: Compute $h_i$ for all layers
    \item \textbf{Backward Pass}: Use the chain rule to compute $\frac{dL}{dh_i}$ from output layer to input
    \item Update each parameter using its gradient
\end{enumerate}

\section{PyTorch Code Example}

\subsection*{Model Initialization}
\begin{verbatim}
d, H = 2, 8
model = torch.nn.Sequential(
    torch.nn.Linear(d, H),
    torch.nn.ReLU(),
    torch.nn.Linear(H, 1),
    torch.nn.Sigmoid()
)
lossfn = torch.nn.BCELoss()
\end{verbatim}

\subsection*{Training Step}
\begin{verbatim}
ypred = model(x)
loss = lossfn(ypred, y)
model.zero_grad()
loss.backward()
with torch.no_grad():
    for param in model.parameters():
        param -= eta * param.grad
\end{verbatim}

\section{Summary}

\begin{itemize}
    \item Feedforward networks compute layer-wise transformations using nonlinearities.
    \item They can approximate any continuous function with enough capacity.
    \item Softmax is used for output probabilities in classification.
    \item Training involves minimizing cross-entropy loss using variants of gradient descent.
    \item Gradients are computed efficiently using backpropagation.
\end{itemize}

\end{document}
