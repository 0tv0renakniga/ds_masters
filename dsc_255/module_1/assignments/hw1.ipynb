{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 255: Machine Learning\n",
    "\n",
    "## Homework 1\n",
    "\n",
    "### Nearest Neighbor Classification\n",
    "\n",
    "1. *Casting an image into vector form.* A $10 \\times 10$ greyscale image is mapped to a $d$-dimensional vector, with one pixel per coordinate. What is $d$?\n",
    "\n",
    "   **Solution:** \n",
    "\n",
    "   Given: $l = 10$ ; $w = 10$ ; $d = l \\times w$\n",
    "\n",
    "   *Apply $d = l \\times w$ and substitute $l$ and $w$*\n",
    "\n",
    "      $d = l \\times w$\n",
    "\n",
    "      $d = 10 \\times 10$\n",
    "\n",
    "      $d = 100$\n",
    "   \n",
    "   $\\therefore$ the $10 \\times 10$ greyscale image is mapped to a $100$-dimensional vector\n",
    "\n",
    "2. *The length of a vector.* The Euclidean (or $L_2$) length of a vector $x \\in \\mathbb{R}^d$ is\n",
    "\n",
    "   $$\n",
    "   \\|x\\| = \\sqrt{\\sum_{i=1}^{d} x_i^2}\n",
    "   $$\n",
    "\n",
    "   where $x_i$ is the $i$-th coordinate of $x$. This is the same as the Euclidean distance between $x$ and the origin. What is the length of the vector which has a 1 in every coordinate? Your answer may be a function of $d$.\n",
    "\n",
    "   **Solution:** \n",
    "\n",
    "   Given: $\\exists x \\in \\mathbb{R}^d : x_i = 1 \\ \\forall i \\in \\mathbb{N}^d$\n",
    "\n",
    "   It follows that,\n",
    "\n",
    "   $\\|x\\| = \\sqrt{\\sum_{i=1}^{d} x_i^2}$ $\\rightarrow$ *Definiton of $L_2$*\n",
    "\n",
    "   $\\|x\\| = \\sqrt{\\left(x_1^{2}+x_2^{2}+...+x_d^{2}\\right)}$ $\\rightarrow$ *Expand summation*\n",
    "\n",
    "   $\\|x\\| = \\sqrt{\\left(1^{2}+1^{2}+...+1^{2}\\right)}$ $\\rightarrow$ $x_i = 1 \\ \\forall i \\in \\mathbb{N}^d$  \n",
    "\n",
    "   $\\|x\\| = \\sqrt{\\left(1+1+...+1\\right)}$ $\\rightarrow$ $1^2 = 1$  \n",
    "\n",
    "   $\\|x\\| = \\sqrt{d}$ $\\rightarrow$ $\\left(1+1+...+1\\right) = d$ , *Since there are d ones*\n",
    "\n",
    "   $\\therefore$ the length of the vector which has a 1 in every coordinate is $\\|x\\| = \\sqrt{d}$\n",
    "\n",
    "3. *Euclidean distance.* What is the Euclidean distance between the following two points in $\\mathbb{R}^3$?\n",
    "\n",
    "   $$\n",
    "   \\begin{bmatrix}\n",
    "   1 \\\\\n",
    "   2 \\\\\n",
    "   3\n",
    "   \\end{bmatrix},\n",
    "   \\begin{bmatrix}\n",
    "   3 \\\\\n",
    "   2 \\\\\n",
    "   1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   **Solution:** \n",
    "\n",
    "   Let, $x = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$ , $y = \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "   It follows that,\n",
    "   $x_1 = 1$ , $x_2 = 2$ , $x_3 = 3$ , $y_1 = 3$ , $y_2 = 2$ , $y_3 = 1$\n",
    "\n",
    "   The Euclidean distance in an n-dimensional space is defined as the following:\n",
    "\n",
    "   $$\\|x-y\\| = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$$\n",
    "\n",
    "   x and y are 3-dimensional vectors. Hence, $n=3$ and the equation above becomes:\n",
    "\n",
    "   $$\\|x-y\\| = \\sqrt{\\sum_{i=1}^{3} (x_i - y_i)^2}$$\n",
    "\n",
    "   After, expanding the summation and substituting $x_1$... $y_3$ the Euclidean distance formula for a 3-dimensional space is:\n",
    "\n",
    "   $\\|x-y\\| = \\sqrt{\\left((1 - 3)^2+(2 - 2)^2+(3 - 1)^2\\right)}$\n",
    "   \n",
    "   $\\|x-y\\| = \\sqrt{\\left(4+0+4\\right)}$\n",
    "   \n",
    "   $\\|x-y\\| = \\sqrt{\\left((1 - 3)^2+(2 - 2)^2(3 - 1)^2\\right)}$\n",
    "\n",
    "   Hence, $\\|x-y\\| = \\sqrt{8}$\n",
    "\n",
    "   $\\therefore$ the Euclidean distance between $x = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$ , $y = \\begin{bmatrix} 3 \\\\ 2 \\\\ 1 \\end{bmatrix}$ in $\\mathbb{R}^3$ is $\\|x-y\\| = \\sqrt{8}$\n",
    "\n",
    "4. *Accuracy of a random classifier.* A particular data set has 4 possible labels, with the following frequencies:\n",
    "   \n",
    "   <center>\n",
    "\n",
    "   | Label | Frequency |\n",
    "   | :---: | :-------: |\n",
    "   | $A$   | $50\\%$    |\n",
    "   | $B$   | $20\\%$    |\n",
    "   | $C$   | $20\\%$    |\n",
    "   | $D$   | $10\\%$    |\n",
    "\n",
    "   <center>\n",
    " \n",
    "\n",
    "   (a) What is the error rate of a classifier that picks a label $(A, B, C, D)$ at random, each with probability $\\frac{1}{4}$?\n",
    "\n",
    "   **Solution:** \n",
    "\n",
    "   The Probability of event E is defined as:\n",
    "\n",
    "   $$P(E_i) = f_i \\times p_i$$\n",
    "      - $f_i$ is the frequency of each event $E_i$ $\\forall i \\in \\mathbb{N}$\n",
    "      - $p_i$ is the probability of each event $E_i$ $\\forall i \\in \\mathbb{N}$\n",
    "   \n",
    "   The Probability of selecting event E or event F is defined as:\n",
    "   $$P(E,F) = P(E)+P(F)$$\n",
    "\n",
    "   First, solve for $P(A)$ , $P(B)$ , $P(C)$ , $P(D)$.\n",
    "\n",
    "   Given: $p_a = p_b = p_c = p_d = \\frac{1}{4} = 0.25$ and $f_a = 0.5$ , $f_b = 0.2$ , $f_c= 0.2$ , $f_d = 0.1$  \n",
    "\n",
    "   $$P(A) = f_a \\times p_a = 0.5 \\times 0.25 = 0.125$$\n",
    "\n",
    "   $$P(B) = f_b \\times p_b = 0.2 \\times 0.25 = 0.050$$\n",
    "\n",
    "   $$P(C) = f_c \\times p_c = 0.2 \\times 0.25 = 0.050$$\n",
    "\n",
    "   $$P(D) = f_d \\times p_d = 0.1 \\times 0.25 = 0.025$$\n",
    "\n",
    "   Hence, $P(A)=0.125$ , $P(B)=0.05$ , $P(C)=0.05$ , $P(D)=0.025$\n",
    "\n",
    "   Now, solve for $P(A,B,C,D)$.\n",
    "\n",
    "   $$P(A,B,C,D) = P(A) + P(B) + P(C) + P(D) = 0.125+0.05+0.050.025 = 0.25$$\n",
    "\n",
    "   Hence, $P(A,B,C,D)$ or accuracy is $0.25$\n",
    "\n",
    "   The error rate($ER$) is defined as: $ER = 1 - accuracy$\n",
    "\n",
    "   Lastly, solve for the error rate.\n",
    "\n",
    "   $$ER = 1 - accuracy = 1 - 0.25 = 0.75$$\n",
    "\n",
    "   $\\therefore$ the error rate of a classifier that picks a label $(A, B, C, D)$ at random, each with probability $\\frac{1}{4}$ is $75$%.\n",
    "\n",
    "   (b) One very simple type of classifier just returns the same label, always.\n",
    "   - What label should it return?\n",
    "\n",
    "      **Solution:** \n",
    "\n",
    "   - What will its error rate be?\n",
    "\n",
    "      **Solution:** \n",
    "\n",
    "5. In the picture below, there are nine training points, each with label either square or star. These will be used to guess the label of a query point at $(3.5, 4.5)$, indicated by a circle.\n",
    "<center>\n",
    "\n",
    "   ![Training Points](dsc_255_hw1_5.png )\n",
    "\n",
    "</center>\n",
    "   Suppose Euclidean distance is used.\n",
    "   (a) How will the point be classified by 1-NN? The options are square, star, or ambiguous.\n",
    "   (b) By 3-NN?\n",
    "   (c) By 5-NN?\n",
    "\n",
    "6. We decide to use 4-fold cross-validation to figure out the right value of $k$ to choose when running $k$-nearest neighbor on a data set of size 10,000. When checking a particular value of $k$, we look at four different training sets. What is the size of each of these training sets?\n",
    "\n",
    "7. An extremal type of cross-validation is $n$-fold cross-validation on a training set of size $n$. If we want to estimate the error of $k$-NN, this amounts to classifying each training point by running $k$-NN on the remaining $n-1$ points, and then looking at the fraction of mistakes made. It is commonly called leave-one-out cross-validation (LOOCV).\n",
    "\n",
    "   Consider the following simple data set of just four points:\n",
    "<center>\n",
    "\n",
    "   ![Simple Data Set](dsc_255_hw1_7.png)\n",
    "\n",
    "</center>\n",
    "   What is the LOOCV error for 1-NN? For 3-NN?\n",
    "\n",
    "### Programming Exercises\n",
    "\n",
    "Before attempting this problem, make sure that Python 3 and Jupyter are installed on your computer.\n",
    "\n",
    "8. **Nearest neighbor on MNIST.** For this problem, download the archive `hw1.zip`, available from the course website, and open it. The Jupyter notebook `nn-mnist.ipynb` implements a basic 1-NN classifier for a subset of the MNIST data set. It uses a separate training and test set. Begin by going through this notebook, running each segment and taking care to understand exactly what each line is doing.\n",
    "\n",
    "   Now do the following:\n",
    "   (a) For test point 100, print its image as well as the image of its nearest neighbor in the training set. Put these images in your writeup. Is this test point classified correctly?\n",
    "   (b) The confusion matrix for the classifier is a $10 \\times 10$ matrix $N_{ij}$ with $0 \\leq i, j \\leq 9$, where $N_{ij}$ is the number of test points whose true label is $i$ but which are classified as $j$. Thus, if all test points are correctly classified, the off-diagonal entries of the matrix will be zero.\n",
    "\n",
    "   - Compute the matrix $N$ for the 1-NN classifier and print it out.\n",
    "   - Which digit is misclassified most often? Least often?\n",
    "\n",
    "   (c) For each digit $0 \\leq i \\leq 9$: look at all training instances of image $i$, and compute their mean. This average is a 784-dimensional vector. Use the `show_digit` routine to print out these 10 average-digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML Classification: Interpretation\n",
    "* ML Classification: Do classes and subclasses in the sdss dataset have a unique signatures, comprised of numerical variable combinations?*\n",
    "* How well do machine learning classification algorithm predict class and subclass categorical variables Is there a difference in model performance when utilizing calculated features versus not(values include in original dataset)? From the model that performs best, can we predict what subclass the Mystery QSO and Mystery Galaxy belong to?*\n",
    "- Accuracy results for K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM) models ranged from ~76.9% to 98.6%. Time and time again the Random Forest model outperformed the SVM model and KNN model. Calculated features, color index (B-V) and distance had the largest impact on the KNN model accuracy. The KNN model accuracy increased 3.6% when incorporating calculated features. According to the model accuracy results classes and subclasses can be classified by leveraging numerical features in this dataset.\n",
    "\n",
    "#####  ML Classification: Bias\n",
    "- The calculated variables are just estimates and could artificially inflate model accuracy results. Model selection was influenced by the results in [Janga et al. 2023](https://www.mdpi.com/2072-4292/15/16/4112) and it is possible that better models exist for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML Classification: Interpretation\n",
    "* ML Classification: Do classes and subclasses in the sdss dataset have a unique signatures, comprised of numerical variable combinations?*\n",
    "* How well do machine learning classification algorithm predict class and subclass categorical variables Is there a difference in model performance when utilizing calculated features versus not(values include in original dataset)? From the model that performs best, can we predict what subclass the Mystery QSO and Mystery Galaxy belong to?*\n",
    "- Accuracy results for K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM) models ranged from ~76.9% to 98.6%. Time and time again the Random Forest model outperformed the SVM model and KNN model. Calculated features, color index (B-V) and distance had the largest impact on the KNN model accuracy. The KNN model accuracy increased 3.6% when incorporating calculated features. According to the model accuracy results classes and subclasses can be classified by leveraging numerical features in this dataset.\n",
    "\n",
    "#####  ML Classification: Bias\n",
    "- The calculated variables are just estimates and could artificially inflate model accuracy results. Model selection was influenced by the results in [Janga et al. 2023](https://www.mdpi.com/2072-4292/15/16/4112) and it is possible that better models exist for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML Classification: Interpretation\n",
    "* ML Classification: Do classes and subclasses in the sdss dataset have a unique signatures, comprised of numerical variable combinations?*\n",
    "* How well do machine learning classification algorithm predict class and subclass categorical variables Is there a difference in model performance when utilizing calculated features versus not(values include in original dataset)? From the model that performs best, can we predict what subclass the Mystery QSO and Mystery Galaxy belong to?*\n",
    "- Accuracy results for K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM) models ranged from ~76.9% to 98.6%. Time and time again the Random Forest model outperformed the SVM model and KNN model. Calculated features, color index (B-V) and distance had the largest impact on the KNN model accuracy. The KNN model accuracy increased 3.6% when incorporating calculated features. According to the model accuracy results classes and subclasses can be classified by leveraging numerical features in this dataset.\n",
    "\n",
    "#####  ML Classification: Bias\n",
    "- The calculated variables are just estimates and could artificially inflate model accuracy results. Model selection was influenced by the results in [Janga et al. 2023](https://www.mdpi.com/2072-4292/15/16/4112) and it is possible that better models exist for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML Classification: Interpretation\n",
    "* ML Classification: Do classes and subclasses in the sdss dataset have a unique signatures, comprised of numerical variable combinations?*\n",
    "* How well do machine learning classification algorithm predict class and subclass categorical variables Is there a difference in model performance when utilizing calculated features versus not(values include in original dataset)? From the model that performs best, can we predict what subclass the Mystery QSO and Mystery Galaxy belong to?*\n",
    "- Accuracy results for K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM) models ranged from ~76.9% to 98.6%. Time and time again the Random Forest model outperformed the SVM model and KNN model. Calculated features, color index (B-V) and distance had the largest impact on the KNN model accuracy. The KNN model accuracy increased 3.6% when incorporating calculated features. According to the model accuracy results classes and subclasses can be classified by leveraging numerical features in this dataset.\n",
    "\n",
    "#####  ML Classification: Bias\n",
    "- The calculated variables are just estimates and could artificially inflate model accuracy results. Model selection was influenced by the results in [Janga et al. 2023](https://www.mdpi.com/2072-4292/15/16/4112) and it is possible that better models exist for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML Classification: Interpretation\n",
    "* ML Classification: Do classes and subclasses in the sdss dataset have a unique signatures, comprised of numerical variable combinations?*\n",
    "* How well do machine learning classification algorithm predict class and subclass categorical variables Is there a difference in model performance when utilizing calculated features versus not(values include in original dataset)? From the model that performs best, can we predict what subclass the Mystery QSO and Mystery Galaxy belong to?*\n",
    "- Accuracy results for K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM) models ranged from ~76.9% to 98.6%. Time and time again the Random Forest model outperformed the SVM model and KNN model. Calculated features, color index (B-V) and distance had the largest impact on the KNN model accuracy. The KNN model accuracy increased 3.6% when incorporating calculated features. According to the model accuracy results classes and subclasses can be classified by leveraging numerical features in this dataset.\n",
    "\n",
    "#####  ML Classification: Bias\n",
    "- The calculated variables are just estimates and could artificially inflate model accuracy results. Model selection was influenced by the results in [Janga et al. 2023](https://www.mdpi.com/2072-4292/15/16/4112) and it is possible that better models exist for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML Classification: Interpretation\n",
    "* ML Classification: Do classes and subclasses in the sdss dataset have a unique signatures, comprised of numerical variable combinations?*\n",
    "* How well do machine learning classification algorithm predict class and subclass categorical variables Is there a difference in model performance when utilizing calculated features versus not(values include in original dataset)? From the model that performs best, can we predict what subclass the Mystery QSO and Mystery Galaxy belong to?*\n",
    "- Accuracy results for K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM) models ranged from ~76.9% to 98.6%. Time and time again the Random Forest model outperformed the SVM model and KNN model. Calculated features, color index (B-V) and distance had the largest impact on the KNN model accuracy. The KNN model accuracy increased 3.6% when incorporating calculated features. According to the model accuracy results classes and subclasses can be classified by leveraging numerical features in this dataset.\n",
    "\n",
    "#####  ML Classification: Bias\n",
    "- The calculated variables are just estimates and could artificially inflate model accuracy results. Model selection was influenced by the results in [Janga et al. 2023](https://www.mdpi.com/2072-4292/15/16/4112) and it is possible that better models exist for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML Classification: Interpretation\n",
    "* ML Classification: Do classes and subclasses in the sdss dataset have a unique signatures, comprised of numerical variable combinations?*\n",
    "* How well do machine learning classification algorithm predict class and subclass categorical variables Is there a difference in model performance when utilizing calculated features versus not(values include in original dataset)? From the model that performs best, can we predict what subclass the Mystery QSO and Mystery Galaxy belong to?*\n",
    "- Accuracy results for K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM) models ranged from ~76.9% to 98.6%. Time and time again the Random Forest model outperformed the SVM model and KNN model. Calculated features, color index (B-V) and distance had the largest impact on the KNN model accuracy. The KNN model accuracy increased 3.6% when incorporating calculated features. According to the model accuracy results classes and subclasses can be classified by leveraging numerical features in this dataset.\n",
    "\n",
    "#####  ML Classification: Bias\n",
    "- The calculated variables are just estimates and could artificially inflate model accuracy results. Model selection was influenced by the results in [Janga et al. 2023](https://www.mdpi.com/2072-4292/15/16/4112) and it is possible that better models exist for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML Classification: Interpretation\n",
    "* ML Classification: Do classes and subclasses in the sdss dataset have a unique signatures, comprised of numerical variable combinations?*\n",
    "* How well do machine learning classification algorithm predict class and subclass categorical variables Is there a difference in model performance when utilizing calculated features versus not(values include in original dataset)? From the model that performs best, can we predict what subclass the Mystery QSO and Mystery Galaxy belong to?*\n",
    "- Accuracy results for K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM) models ranged from ~76.9% to 98.6%. Time and time again the Random Forest model outperformed the SVM model and KNN model. Calculated features, color index (B-V) and distance had the largest impact on the KNN model accuracy. The KNN model accuracy increased 3.6% when incorporating calculated features. According to the model accuracy results classes and subclasses can be classified by leveraging numerical features in this dataset.\n",
    "\n",
    "#####  ML Classification: Bias\n",
    "- The calculated variables are just estimates and could artificially inflate model accuracy results. Model selection was influenced by the results in [Janga et al. 2023](https://www.mdpi.com/2072-4292/15/16/4112) and it is possible that better models exist for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML Classification: Interpretation\n",
    "* ML Classification: Do classes and subclasses in the sdss dataset have a unique signatures, comprised of numerical variable combinations?*\n",
    "* How well do machine learning classification algorithm predict class and subclass categorical variables Is there a difference in model performance when utilizing calculated features versus not(values include in original dataset)? From the model that performs best, can we predict what subclass the Mystery QSO and Mystery Galaxy belong to?*\n",
    "- Accuracy results for K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM) models ranged from ~76.9% to 98.6%. Time and time again the Random Forest model outperformed the SVM model and KNN model. Calculated features, color index (B-V) and distance had the largest impact on the KNN model accuracy. The KNN model accuracy increased 3.6% when incorporating calculated features. According to the model accuracy results classes and subclasses can be classified by leveraging numerical features in this dataset.\n",
    "\n",
    "#####  ML Classification: Bias\n",
    "- The calculated variables are just estimates and could artificially inflate model accuracy results. Model selection was influenced by the results in [Janga et al. 2023](https://www.mdpi.com/2072-4292/15/16/4112) and it is possible that better models exist for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  ML Classification: Interpretation\n",
    "* ML Classification: Do classes and subclasses in the sdss dataset have a unique signatures, comprised of numerical variable combinations?*\n",
    "* How well do machine learning classification algorithm predict class and subclass categorical variables Is there a difference in model performance when utilizing calculated features versus not(values include in original dataset)? From the model that performs best, can we predict what subclass the Mystery QSO and Mystery Galaxy belong to?*\n",
    "- Accuracy results for K-Nearest Neighbors (KNN), Random Forest, and Support Vector Machine (SVM) models ranged from ~76.9% to 98.6%. Time and time again the Random Forest model outperformed the SVM model and KNN model. Calculated features, color index (B-V) and distance had the largest impact on the KNN model accuracy. The KNN model accuracy increased 3.6% when incorporating calculated features. According to the model accuracy results classes and subclasses can be classified by leveraging numerical features in this dataset.\n",
    "\n",
    "#####  ML Classification: Bias\n",
    "- The calculated variables are just estimates and could artificially inflate model accuracy results. Model selection was influenced by the results in [Janga et al. 2023](https://www.mdpi.com/2072-4292/15/16/4112) and it is possible that better models exist for this application."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
