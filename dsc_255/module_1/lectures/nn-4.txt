(upbeat music) - Welcome back. Our next topic is cross-validation, which is a crucially important technique across all of machine learning. The basic question it answers is how can you measure the error of a classifier if you only have the training set, if you don't have a separate test set? So let's return to our earlier example. We had built K-nearest neighbor classifiers for the MNIST data set, and we found that the best choice of K is K equals three. This is best in the sense that it leads to the lowest test error. And we said the test error is really a good measure of the quality of a classifier, unlike training error which is overly optimistic. Now, in real life, there often is no separate test set. We often just have a training set. So what would we do in this case? How would we choose the best value of K? Okay, this seems like a difficult problem. There are many different solution strategies. One idea, if you just have a training set, is to take a piece of the training set and just keep it separate to sort of create your own test set. So the idea here is that you have this large training set. Let's say this is our training data, all of it, that we call S. In the case of MNIST, this has 60,000 points in it. And we take a small portion of it that we set aside. We sort of create our own test set. Let's call this V where V stands for validation set. Okay, So maybe S has 60,000 images in it, whereas V has say, 5,000. In order to evaluate different choices of K what we do is that we treat V as the test set and we do K-nearest neighbor classification using the remaining 55,000 points, okay. So we try out different values of K. We choose the one that works best. And once we've chosen a value of K, then we revert to using all of this as the training set, okay. So this little trick of creating our own test set is solely for the purposes of choosing K. Once we've chosen K, we use the entire 60,000 images as the training set. So, this is a reasonably effective way of choosing a value of K. The one downside here is that when we are choosing K, we are only using 55,000 of the training points. And it's not clear that the best K for a training set with 55,000 points is the same as the best K for a training set of 60,000 points, okay. There's a little bit that gets lost in translation. So, we often look at alternative methods. One such alternative, which is particularly effective in the case of nearest neighbor, is something called leave-one-out cross-validation. So here's the idea. You take your training set of 60,000 points. To evaluate the performance of K-nearest neighbor you look at the very first point and classify it by doing K-nearest neighbor on the remaining 59,999 points. Then you take your second data point and classify it by doing K-nearest neighbor on the remaining 59,999 points. And in this way, you classify every single training point using the remaining points, okay. You choose the best value of K and then you revert to using the entire data set. In this case, the transition from 59,999 to 60,000 is a very small transition. And so it's very likely that the error rate that you get in this way is a very good estimate of the error rate for using the entire data set of 60,000 points. So leave-one-out cross-validation is very effective in the context of nearest neighbor, but for many of the models, for many of the types of classification that we'll be looking at later on in the class it'll turn out that although leave-one-out cross-validation yields very accurate estimates, it can be computationally very inefficient. And in those kinds of settings, what we typically use is something called M-fold cross-validation. Here, M is an integer that could be two or five or 10 or so on, okay. So to illustrate what this is, let's look at the particular case of 10-fold cross-validation, okay. How do we estimate the error using this method? Okay, so let's say we wanna evaluate K-nearest neighbor for some choice of K, for example, three nearest neighbor, okay. In order to do this using 10-fold cross-validation what we do is to take our training set of let's say 60,000 points, and since it's 10-fold cross-validation we'll divide it into 10 equal pieces. So one, two, three, and so on. And so each piece has got 6,000 points. Now what we'll do is that we'll treat this first chunk as the test set and the remainder as the training set. So we'll classify those first 6,000 points using the remaining 54,000 points, okay. So we'll classify these 6,000 points by running K-nearest neighbor on the remaining 54,000. Next, we'll classify the next chunk of 6,000 points using the remaining 54,000 points. Okay, so that consists of a training set with these points and the remaining points down here and so on, okay. And so what we'll end up doing is that for each of these chunks of 6,000 points at some point we will treat it as the test set and everything else as the training set. Okay, so we get 10 different error estimates, one for each of the chunks, and we average all of them and that gives us an overall error estimate for K-nearest neighbor, okay, and we'll try this for different values of K and pick the very best one. So this technique is something that's very widely used across machine learning and we'll see it over and over again. 