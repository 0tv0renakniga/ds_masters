(upbeat music) - [Sanjoy] Welcome back. So one of the things we've talked about a lot is nearest neighbor classification. It's simple, effective, widely used. A downside with nearest neighbor though, is that it can take a long time to find the nearest neighbor. So what we'll do now is just address this issue and see ways of mitigating it. So the naive way of finding the nearest neighbor is to compute the distance to the very first point in the training set, and then the distance to the second point, and then the distance to the third point, and so on and then to return the smallest distance. Okay. If the training set contains N points, then this takes time proportional to N. Which can be extremely slow if N is large. Something like a million or 10 million. In those cases, it would simply not be practical to do nearest neighbor search if this is the way we were gonna do it. Luckily, there are lots of clever data structures for speeding up nearest neighbor search. These have names like locality-sensitive hashing, k-d trees, ball trees. There are lots of these methods. They are well established. They've been around a long time, and they're now part of standard packages like scikit-learn and python. So you can just use them easily. And in practice they make a dramatic difference. Let's look at one of these just to get a sense of how they work. Okay, so let's look at k-d trees, which are certainly one of the most popular ways of speeding us up nearest neighbor. So let's say that you have a data set. Let's say you have a training set S and the data is d-dimensional. In the case of emnist, S contains 60,000 points, and D the dimension is 784. Now, for visualization purposes, I've just drawn a small data set here consisting of those points and here we have 16 points in two dimensional space. Okay, so what is a k-d tree? Well what you do is you start with your dataset S in this case, 16 points, and you put a box around them. Okay, so that's the outside bounding box. Then you choose a coordinate. Okay, so let's say we choose the X one coordinate and we look at the data points just along that coordinate. Okay, so we just look at the values of the first coordinate of all the points, and we find the median, the middle value and we split along that value. So here's the split that we get. So this divides the points into two groups. Each containing eight points. So now we've divided the data into these two halves. And now we recursively subdivide each of these two halves. Okay, so for example, on the right half the next time let's say we pick this coordinate and we split at the median and so on. Okay, so you can visualize this as a tree structure where at the top of the tree, at the root of the tree you have a single cell containing all of the data. You have a single rectangle containing all 16 points. At the next level of the tree, you now have two cells. You have two nodes, two rectangles each containing eight points. At the next level, you have four rectangles, and so on. So at each level of the tree we have these rectangular cells, okay? And we keep going until the cells get to a certain size. Here we've stopped when the cells contain two points. Okay, so this is the data structure that we build on top of the training set. Now, how do we use this to actually answer nearest neighbor queries? Okay, so one technique called defeautist search is a quick and dirty method that returns a point, returns a training point. That's not necessarily the exact nearest neighbor, but it's generally something pretty close. And for many practical purposes, this is good enough. Okay, so here's how it works. You have a query. Let's say we have a query Q over here. Okay, so let me just denote it by a little X over there. And you move the query down the tree and you look at the leaf cell in which it falls. Okay, so it ends up falling in this box. In this leaf of the tree. And then you just find its nearest neighbor within the leaf. So you just need to find its nearest neighbor in this case between these two points, and you return that. Okay, and that's your answer. Now, this is not always the exact nearest neighbor. So for example, let's say your query was this point over here. Let's say this was your query Q. Okay, I'll draw it by an X. You would end up returning this as the nearest neighbor, even though the actual nearest neighbor is this point right there. Okay, so to prevent that from happening, there's a slightly more careful search strategy called comprehensive search, which backs up the tree and looks at neighboring cells just to check whether the nearest neighbor lies in there. Okay, so this is just to give you a little bit of a flavor of what these nearest neighbor data structures are like. At a high level, what's important is that these are widely available and you can just use them anytime you want to do nearest neighbor search. 