(upbeat music) - Welcome back, everybody. We'll now talk about some generic ways of improving the performance of a nearest neighbor classifier. So let's return to the example from last time. We built a nearest neighbor classifier for the MNIST dataset and we found that it actually did pretty well. Its error rate on the separate test set was just 3.09%. Let's look at some of the mistakes that it made. So the test set had 10,000 points. An error rate of 3.09% means that it got 309 of these points wrong, okay? And here is a random smattering of the mistakes that it made. So in the first case, this was the point it was asked to classify, this is the image it was asked to classify. It was supposed to be a six. The nearest neighbor was this image, and so it predicted a four, it got that wrong. The second example, this is the image it was supposed to classify, clearly a two, but the nearest neighbor was this and so it thought, it predicted zero. So in all these cases and the other three, as well, the mistakes are really quite understandable. If you look at these first two images here, they're very close together in pixel space. Their pixels overlap significantly. And it's not surprising, therefore, that this four was the nearest image. It was the nearest neighbor of the six up here. So the mistakes are understandable but still it would be nice if there was some way of reducing these kinds of mistakes. What can we do? So there are two generic things that tend to improve the performance of nearest neighbor. The first and more important is to simply choose a better distance function. And the second is to move from looking at the very nearest neighbor to looking at the K nearest neighbors. Okay, so let's start with the distance function. So we were computing the distance between two images by simply looking at Euclidean distance. And it turns out that this is not a great way to measure distances between pictures. Okay, so let's look at these two images, for example. These are actually exactly the same image, except that the second one has been scrolled very slightly to the right, so almost exactly the same picture. And yet, if you were to measure the Euclidean distance between them, it would be significant because their pixels, the white areas of these images, almost don't overlap at all. Okay. So this is a clear sign that we need a different distance function for images, okay? And people have tried very hard to come up with such distance functions. So ideally what we'd like is some distance that's invariant to small translations of this type or maybe even to small rotations. An example of such a distance function that was discovered in the 90s by the computer vision community, is tangent distance. Thereafter, they went even further and they said, "Well, can we find a distance function that's also "invariant under small deformations of an image, "like taking a line and making it slightly curved?" And they found a distance function of that type, as well, called shape context. What happens when you use these better distance functions for nearest neighbor classification? Well, it turns out that, not surprisingly, the error rate goes down. So when using Euclidean distance, we found an error rate of 3.09%. With tangent distance, it goes down to 1.1%. And with shape context distance, it goes down to just 0.63 of a percent, okay? So in this case, in the first case, it got 309 test points wrong. In the second case, it got only 110 of them wrong. And then, with shape context, it only made mistakes on 63 of the test points. So the general message over here is that the most direct way and maybe the most impactful way of improving nearest neighbor performance is simply to choose a better distance function, one that is really adapted to the domain in which the data lie. So the second technique for improving nearest neighbor is to move from looking at the very closest neighbor to the few closest neighbors. For example, instead of just looking at the nearest neighbor, you can look at the three nearest neighbors in the training set and take a majority vote amongst their labels, or maybe the five nearest neighbors, or the 10 nearest neighbors. Let's see what happens with MNIST. Okay, so with one nearest neighbor, which is the technique we talked about last time, the error rate was 3.09%. But if we look at the three nearest neighbors and use the majority vote amongst their labels, the error rate goes down a little bit. It goes down to 2.94%. And when we move to five nearest neighbors or seven nearest neighbors, the error rate actually starts to go back up. So in this case, the best choice would be to use three nearest neighbors, okay? So the improvement is not dramatic. It's not as great as the improvement that we got by choosing a different distance function, but still it's something. Now, one tricky question over here is that in order to figure out which of these choices of K is best, we looked at the test error, but it turns out that in many practical settings, there isn't that separate test set, it's just not available. Okay? There is no separate test set of 10,000 points or of any size. In that case, how do we assess the best value of K? Okay, we certainly don't want to use training error because we know that that's misleading. Okay, so what do we do? And it turns out that this is a problem that appears over and over again in machine learning contexts. And what we'll do next time is to come up with a way of solving it. 