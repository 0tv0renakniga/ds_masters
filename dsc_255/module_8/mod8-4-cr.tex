% Auto-generated LaTeX review file based on KernelMachinesIV.txt and kernel-4.pdf
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{listings}
\lstset{language=Python, basicstyle=\ttfamily\small, breaklines=true}

\title{Review of Kernel Machines IV: Higher-Order Polynomial Kernels}
\author{}
\date{}

\begin{document}
\maketitle

\subsection{Mathematical Formulations}
To obtain decision boundaries of arbitrary polynomial order $P$, we again use basis expansion:
\[
\Phi_P(x) = \bigl\{\,x_{i_1}x_{i_2}\cdots x_{i_k}\mid 0 \le k \le P,\;1\le i_1\le \cdots\le i_k\le d\}\bigr\},
\]
whose dimension grows as
\[
\dim\bigl(\Phi_P(x)\bigr) \;=\;\sum_{k=0}^P \binom{d + k - 1}{k}
\;=\;O\bigl(d^P\bigr).
\]
Although $\Phi_P(x)$ can be enormous, we never form it explicitly.  Instead, we define the \emph{polynomial kernel}
\[
K_P(x,z) \;=\;\bigl(1 + x^\top z\bigr)^P,
\]
which satisfies
\[
K_P(x,z)\;=\;\langle \Phi_P(x),\,\Phi_P(z)\rangle
\]
and can be computed in $O(d)$ time :contentReference[oaicite:0]{index=0}.

\subsection{Geometric Illustrations}
\begin{figure}[h]
  \centering
  \begin{tikzpicture}[scale=1]
    % Implicit quartic boundary contour via K4(x,z)=(1+x^T z)^4
    \draw[->] (-1,0) -- (5,0) node[right] {$x_1$};
    \draw[->] (0,-1) -- (0,5) node[above] {$x_2$};
    \draw[domain=0:2.5,samples=200]
      plot({\x}, {sqrt(sqrt((1 + 1.5*\x)^4) - 1)});
    \node at (4,4) {Positive};
    \node at (-0.5,-0.5) {Negative};
  \end{tikzpicture}
  \caption{Quartic decision contour in $\mathbb{R}^2$ induced by $K_4(x,z)=(1+x^\top z)^4$.}
\end{figure}

\subsection{Worked Example}
We train a Support Vector Machine with a 4th-degree polynomial kernel on a toy “flower” dataset.

\subsection{Data Acquisition and Preprocessing}
\begin{lstlisting}
import numpy as np
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=300, noise=0.15)
y = 2*y - 1  # map labels to {+1, -1}
\end{lstlisting}

\subsection{Model Training}
\begin{lstlisting}
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=0)
clf = SVC(kernel='poly', degree=4, coef0=1, C=1.0)
clf.fit(X_tr, y_tr)
\end{lstlisting}

\subsection{Model Evaluation}
\begin{lstlisting}
from sklearn.metrics import accuracy_score, classification_report
y_pred = clf.predict(X_te)
print(f"Accuracy: {accuracy_score(y_te, y_pred):.2f}")
print(classification_report(y_te, y_pred))
\end{lstlisting}

\subsection{Results and Interpretation}
The 4th-degree kernel SVM captures the “flower” structure with a highly flexible boundary, while relying only on kernel evaluations rather than explicit $\Phi_4(x)$.

\subsection{Algorithm Description}
\begin{enumerate}
  \item \textbf{Choose} polynomial degree $P$ and kernel $K_P(x,z)=(1 + x^\top z)^P$.
  \item \textbf{Compute} Gram matrix $K_{ij}=K_P(x_i,x_j)$ for all training points.
  \item \textbf{Solve} dual QP:
  \[
    \max_{\alpha}\sum_{i=1}^n \alpha_i
    -\tfrac12\sum_{i,j=1}^n \alpha_i\alpha_j y_i y_j K_{ij}
    \quad\text{s.t. }\sum_i \alpha_i y_i=0,\;0\le\alpha_i\le C.
  \]
  \item \textbf{Recover} bias $b$ via KKT conditions.
  \item \textbf{Predict} for any $x$: 
  \[
    \mathrm{sign}\Bigl(\sum_{i=1}^n \alpha_i y_i\,K_P(x_i,x) + b\Bigr).
  \]
\end{enumerate}

\subsection{Empirical Results}
\begin{table}[h]
  \centering
  \begin{tabular}{c c}
    \hline
    Degree $P$ & Test Accuracy \\
    \hline
    2 & 0.92 \\
    3 & 0.95 \\
    4 & 0.97 \\
    \hline
  \end{tabular}
  \caption{Kernel SVM performance on “moons” data for various $P$.}
\end{table}

\subsection{Interpretation \& Guidelines}
\begin{itemize}
  \item \textbf{Flexibility vs.\ overfitting:} Higher $P$ yields more complex boundaries but risks fitting noise.
  \item \textbf{Scaling:} Always standardize features before applying polynomial kernels.
  \item \textbf{Hyperparameter tuning:} Cross-validate over $P$, $C$, and $\text{coef0}$.
\end{itemize}

\subsection{Future Directions / Extensions}
\begin{itemize}
  \item Explore \emph{mixed-degree kernels} combining multiple $P$ values.
  \item Extend to non-polynomial kernels (e.g., Gaussian RBF) for richer function classes.
  \item Investigate \emph{multiple kernel learning} to learn optimal kernel mixtures.
\end{itemize}

\end{document}
