(upbeat music) - Welcome back, last time we talked about the hard margin support vector machine or SVM. What we'll do now is to look at the version of the SVM that is the most commonly used in practice. It's sometimes called the soft margin SVM. So let's start by doing a quick recap of last time. So we have a training set of N data points, the points lie in D-dimensional space, and each point has a label of -1 or +1. We want to use these points to learn a linear classifier. The way we do that in the hard margin SVM, is to solve this convex optimization problem. So we have these n constraints over here, one per data point. And what this constraint says is that the ith data point should be correctly classified, okay? So ith point should be correctly classified and this is a hard constraint, okay? So we have a set of n constraints saying each of the n training points should be correctly classified. And other than that, what we wanna do is to minimize the length of w squared. And what we saw last time is this is the same as maximizing the margin, okay? And the reason for that is that the margin is simply 1 over the length of w. Now, this particular optimization problem is easy to solve. It can be solved efficiently, we get the optimal solution. And the solution we get is the linear classifier with the largest margin. Now in addition, it turns out that the solution w is a linear combination of just a few of the training points, the ones that lie exactly on the margin. These are called the support vectors. So, this is all well and good. The only problem over here is that this formulation assumes that the data can be perfectly separated, that the training points are linearly separable. This is often not true in practice, okay? And so what we are gonna look at now is what happens when we have data that are not linearly separable. If that's the case, we can no longer insist that these n constraints all hold. It will simply not be possible to satisfy all of them, okay? So what we need to do is to somehow allow some of these constraints to be violated. Now, the way we are gonna allow violations is by introducing slack variables. So for each of the data points, we will introduce a slack variable that controls the amount of violation on that point. So we can think of this as being the amount of violation on data point xi. So let's look at the modified convex optimization problem. Our constraints look a little bit different now. So previously, our constraints looked like this. We said that for the ith data point, we need yi times w.xi + b to be greater than or equal to 1, okay? Now we allow that to be violated, okay? We say, "Well, it's okay if it's less than 1. And this slack variable Xi will take up the slack, we'll account for the amount by which it's less than 1." So we introduce these extra slack variables one per data point. Now, the optimization criterion is partly the same. We have this term over here which asks for a large margin. But now we have a second term which says we don't want to use too much slack, we don't want too much violation. So there are these two terms that we are balancing. Okay, so to get an example of what this slack variable means, let's look at this picture. So here we have a linear boundary, a hyperplane, w.x + b, and this boundary over here is the parallel hyperplane, w.x + b = 1, and this one over here is another parallel hyperplane on the other side, w.x + b = -1, okay? As before, the margin is given by this quantity, gamma, and as usual, it's 1 over the length of w. What's different now is that some of the points violate the constraints. In fact, two of them do, this point over here and this point over here. So let's first look at this blue point. The blue point should be all the way on this side, okay? But it looks like for the blue point, w.x + b is something like .5, since it lies between these two hyperplanes, it's something like .5. We want it to be -1, so it's far away from where it should be. It's off, it should be -1, it's .5, so it's off by 1.5, and that's why the slack value for that point is 1.5. If we look at this point over here. It looks like for that point, w.x + b is something like .25. It should be 1, and so it's off by .75. All other points are not violations, and so they have a slack value of 0. So in this particular example we have allowed two violations, we've allowed two of the data points, one of them blue and one of them red, to violate the constraints, okay? And those are exactly the two points for which the slack variables are set to non-zero values. So let's see how this works. Here we have a data set that's taken from the winery data that we saw earlier, okay? Now the part of it that we've pulled out is a two-dimensional data set which has two labels, the black triangles and the red circles, and as you can see, this data is not linearly separable. So if we were to run the perceptron on this data, it would just keep going forever, it would never converge, okay? If we were to run the hard margin SVM on this data, it would simply return, "Error, constraints cannot be satisfied," okay? But, we can do the soft margin SVM on this dataset and this is what we get. So this is the boundary we get, it's a pretty good boundary. And the points that I've shown in bold are the support vectors. Now, in the case of the soft margin SVM, the support vectors are the points exactly on the margin, okay, just like before. So these are the points for which yi times w.xi + b is exactly equal to 1. But the support vectors also include anything on which slack is used, okay? In other words, any violated point. And in this particular case, there are lots of violations and therefore lots of support vectors, okay? So which is the point on which the violation is the largest? Well, let's see, it looks like this point over here, okay? This point, this black triangle, should be somewhere over here. So the violation on that, the slack value of that is probably something like 3, it's pretty far away from where it should be. So you can see that by allowing these slack variables, we are able to deal with data of this form. Okay, now here it says C = 1.0. What is that referring to? Well, let's go back to the formulation of the soft margin SVM. Okay, so here is our convex optimization problem and indeed there is this parameter C over here. In this case, we are optimizing two different quantities. There's this thing over here which controls the margin and there's this thing over here that controls the slack. And since we are optimizing both of them at the same time, we have this parameter that trades off between them. So let's look at the effect of different settings of C. Okay, so on one end of the spectrum, let's say we set C = 0. If we put C = 0, it basically means that slack is free, the second term just goes away, okay? So C = 0 means slack is free, which means we're welcome to use as much slack as we like, there is no penalty for it, we can violate all the data points, and as a result, we can make the margin as large as we like. So the margin can be arbitrarily large and that corresponds to w going to 0, okay? Because remember, the margin is 1 over the length of w. So C = 0 is slack is free. Now, on the other hand, what if we make C large? What if we let C go to infinity? In this case, we are saying that slack is very expensive, which means that we really do not want any violations at all. In which case, this starts to look like the hard margin SVM. So this is mathematically what changing the C parameter does. But now let's look at a pictorial depiction of the same thing. So here is a dataset from last time, and what we have here is data that is linearly separable. It's two-dimensional data, it's linearly separable, and let's start by picking C = 10, okay? So a relatively high value of C. Now returning to the previous slide, we saw that when C is large we get something that's like the large margin SVM. And indeed, when we do this, we get the same solution that we saw last time with the hard margin SVM, okay? So we get a solution that correctly classifies all the points. There are no violations, there's no slack used, and there are three support vectors, and this is the margin that we get. This is where the value of C = 10. Now, let's reduce the C value. So now we reduce it a little to 3. That means slack is a little bit cheaper, and indeed we are starting to see a little bit of slack over here. Not a lot, the slack on that point might just be .1, and similarly with these points, okay? So we've gone up to four support vectors now and the margin is a little bit larger than what we had before. This was the previous margin, this is the new margin, it's a little bit larger. Let's reduce C further. So now we go down to 2. Slack is a little bit cheaper, and now you can see that we are using quite a lot of slack on that point. The slack value at that point is probably something like .6 or .7, okay? And we've got a significantly larger margin as a result. Let's go down even further, let's make Slack even cheaper, and cheaper, and cheaper, and really cheap, okay? So at this point, slack is very cheap and we have violated all of the data points but in return, we've got a really large margin. So, we've seen the soft margin SVM, which is the most commonly used version of SVM in practice. It allows violations on individual training points, and in order to do this, it has this parameter C that has to be set carefully. What we'll do next time is to look at ways of setting C. 