(bright music) (whooshing) - Welcome back. Last time, we talked about the soft-margin SVM, the version of the SVM that is most commonly used in practice. Now, let's see an example of it in use. So let's start with a quick recap of the soft-margin SVM. We have some data set of- lets say endpoints, X1, Y1, to Xn, Yn. Each of the labels is plus one or minus one. The points themselves are vectors in the dimensional space. And what we wanna do is to fit a linear classifier to this data and we fit the linear classifier, WB, by solving this convex optimization problem. Okay? Now, the objective function which is the function we are trying to minimize has got two terms in it. There's this term W squared, minimizing the length of W squared corresponds to maximizing the margin. Okay? So what we want here is a large margin. That's what that term gives us. And then there's a second term over here. And what this tells us is that we don't want to violate too many of the training points. We wanna correctly classify the training points. We don't want too many violations on them. Okay? So not too many violations on training data. And for us the violations are controlled by slack variables. So there are these two considerations that we have to balance, and the way we trade them off is with this parameter C, and C is something that we get to choose and it's something that we need to choose carefully. Now, what we saw last time is that if C = zero what that means is that slack is free. And as a result, we are welcome to violate as many training points as we like. They could all be on the wrong side, it doesn't matter. And this allows us to get a really large margin, okay? As C gets larger, slack gets more and more expensive and we permit fewer and fewer violations. So you can think of C as the price of slack. Okay. So with that in mind, let's turn to the sentiment data that we saw earlier. And if you recall, this dataset consists of reviews from Amazon, Yelp, and IMDB. Each review is just a single sentence and is labeled as either positive or negative. So for example, this first one, "I wasted my money." This is a minus one. And the next one, "He was very impressed." That's a plus one, and so on. Okay? Now, as we did last time what we are gonna do here is to convert each of these sentences into a vector by using a bag of words representation. We'll use a vocabulary of 4,500 words as a result of which each sentence will become a vector in 4,500 dimensions. Okay? So in our terminology, the dimension D is 4,500. Now the data set is not very large. There are 2,500 training points. So N = 2,500. And there's also a separate test set that we can use to evaluate our final classifier. Now, in applying a Soft-Margin SVM to this data we have to choose the value C. We have to decide what it's going to be, and what I've done here is to show the effect of different choices of C starting from relatively small: 0.01, and geometrically increasing by factors of 10 up to 10,000. So let's see what's going on. When C is small- So remember, C is the price of slack. When C is small, it means slack is almost free. So we are welcome to violate a lot of the training points and that's reflected in the fact that the training error is very large, okay? As C gets larger, we pay more and more for violations and as a result, the training error goes down. So that's a very clear trend. What about the support vectors? Why does the number of support vectors go down as C increases? Well, if you recall, the support vectors are not just the points that are exactly on the margin. In the case of the soft SVM, they also include all of the violated points. So if you have fewer violated points that will tend to mean fewer support vectors as well. Okay, and so that's why the number of support vectors is going down. But ultimately what we care about is test error. And what's happening is we increase C- is that we are forcing more and more of the data of the training points to be satisfied. And as a result, the margin is getting smaller and smaller which means that our model might not generalize as well. And this is reflected in the test arrow which initially goes down, but later seems to be coming up. And so if one were to look just at test era it seems like the best choice of C is probably somewhere in here, somewhere in that range. So how do we choose C? Okay. Now, in general, we are not allowed to peak at the test era in choosing C how can we choose a good value of C using the training set alone? This is a common situation. We have a machine learning method that contains a mystery parameter. It's really important to set that parameter well. We want to set it in a way that gives us low error- that gives us a model with low error. And the most common way of doing this- a very easy way to do this, is to use cross-validation. So what we've done over here is to use 5-fold cross-validation. So we chose a whole bunch of different values of C. So these are different candidate values. And for each value what we wanted to know is what is an estimate of the error rate if we choose this value of C? And the way we got that estimate was by fivefold cross validation. Now, the data set itself is not very large. The training set consists of 2,500 points. So when we do 5-fold cross-validation we are creating chunks of 500 points, which means that the error bars on these numbers are not tiny. The error bars are significant, which is why the numbers do seem to be bouncing around a little bit. But still, it seems quite clear that the- that the best value is somewhere over here. And if we simply choose this- this one over here it turns out that that value is 0.32 and it leads to a test error of 15.6% which is not bad at all for this data. Okay. So we have now seen the support vector machine- the Soft-Margin support vector machine, in its full glory. It is really a very effective and competent form of linear classification. The amazing thing which we'll see soon is that this same method can be used to get boundaries that are far more complicated than just linear to get boundaries that are quadratic higher rotor polynomial, or even beyond. And the key to doing this is duality which is something that we'll touch upon next time. 