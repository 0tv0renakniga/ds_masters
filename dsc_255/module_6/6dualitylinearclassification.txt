(bright music) - Okay, so we have seen some very convenient methods for learning linear classifiers, the Perceptron and support vector machine. Now, it turns out that these same methods can be used to get boundaries that are far beyond linear, boundaries that are quadratic or polynomial or even beyond. The key to doing this is to view the Perceptron and support vector machine in their dual forms, okay? So, this probably sounds a little mysterious. Let's start with the dual of the Perceptron, and then we'll move on to the support vector machine. So, here's the Perceptron algorithm again. We want to learn a linear function given by w and b and we initialize it to zero, okay? And then we have our training set of end data points and we just keep cycling through this data over and over until we get all of the training points correctly classified, okay? So, we go through all of the points, and let's say we're currently processing point x, y. If we get the right label, if our prediction on x is correct, if it's equal to y, then we don't update w and b. But if we make a mistake, then we do this simple update. We add y, x to w and we add y to b, okay? So, let's look at what happens to w during this process. So initially, we have w equals zero. And then, the only change we ever make to it is if we arrive at some data point xi, yi, on which we make the wrong prediction. And in that case, what we do is we take w and we add yi xi to it, okay? That's the only way in which w ever changes. What this means is that at any given time, w is of a very specific form. It's exactly this form, okay? It is the sum over all the data points i, of yi xi times the number of times we updated on data point i, okay? So, if we never updated on data point i, if we got it correct from the get-go, then alpha i equals zero. If we updated once on it, then alpha i equals one. And if we updated 10 times on that same x of i, then alpha i equals 10, okay? So, alpha i is an integer, which is exactly the number of times we have updated on data point i. Okay, so w is always of this form. What this means is that we now have an alternative. We can either represent w as a 3-dimensional vector, like we were doing before, or we can just represent it by these coefficients, alpha one through alpha n. And that's equivalent because the minute we have the alphas we get w by just plugging into this formula over here, okay? 'Cause we have the training data, the xs and ys. So we now have two equivalent ways to write down w either as a 3-dimensional vector which is what we were doing before, or by specifying these coefficients, alpha one through alpha n the number of updates on data 0.1, which is equal to the number of times we got that point wrong the number of updates on data 0.2 and so on. As long as we keep track of these counts we can derive w from them. So we will think of this as the dual form of w. Another way to get get at the information in w. In fact what we can do is to take the Perceptron algorithm and rewrite it entirely using this dual form. So this is the original Perceptron algorithm up here and we are now gonna call it the primal form the original form. And this is the same algorithm we're familiar with, okay? The one we saw before. Now what we are gonna do is to write down the dual form of this algorithm where instead of keeping track of w we are just gonna be keeping track of the alpha I 'cause we've seen that that's equivalent. So over here for example, instead of initializing w to zero we are just gonna initialize all the alphas to zero, okay? And all that means is we haven't done any updates yet, okay? Alpha one through alpha n are all zero. Now once again, we cycle through the data and each time we get a point wrong, what do we do? Well, instead of this step over here where we add yi xi to w here all we need to do is to increment a counter saying that we have updated once more on data point i, okay? So to increment the counter for data point i and anytime we want to get w we just plug into this formula and we get the current w. So the second algorithm is doing exactly the same thing as the primal. The dual and primal are doing exactly the same thing, okay? And in some ways the dual is a little bit simpler. If you look at this update over here in the primal form the update is a vector edition. The w is a 2-dimensional vector. In the dual form this update is just incrementing a counter it's a simpler operation, okay? So in many respects, the dual form is a bit simpler. Okay? So that is the dual of the Perceptron algorithm. Now let's move on to the SVM. So if you recall we had two versions of the SVM the hard margin SVM, where we assume that the data is linearly separable and the soft margin SVM which is more general and is actually the version that would be used in practice. So let's start with the hard margin version. So as usual, we want to find a linear function given by w n b. And the way we get that function is by solving this optimization problem over here. So what are we doing? Well, we have a bunch of constraints, okay? One per data point. And the constraints just say that all the data points have to be correctly classified. Other than those constraints what we wanna do is to minimize the squared length of w which is the same as maximizing the margin. So essentially what we are saying is classify all the points correctly and maximize the margin at the same time. And this gives us the maximum margin linear separator. Now, this optimization problem is a convex optimization problem. And what that means is that the constraints that it has are all linear constraints, okay? And here they're linear in the things we're solving for. So they're linear in w n b. And the thing we are minimizing is a convex function, okay? w squared is a convex function. So we call this a convex optimization problem and that means basically that it's a well-behaved optimization problem. Now, there is a very rich theory of convex optimization which unfortunately we are not gonna have time to go into at all, but we will be using some of the conclusions of that theory. And one of the things that the theory tells us is that whenever you have a convex optimization problem like this, there is a dual problem, okay? So there's sort of a sibling problem that's called the dual, it's also an optimization problem. And if the original problem is a minimization the dual is a maximization problem, okay? And the two problems have the same solution they have the same numerical value for the solution and they're closely linked, okay? So this is doubtless pretty mysterious. So let me just go ahead and show you what the dual problem is for the hard margin SVM. And this is it right here. So this is the original problem, the primal and this is the dual problem. Now as you can see, the dual problem is a maximization and it's got a bunch of constraints that are linear. And in this case, instead of solving for w we are solving for the alphas just like in the case of the dual Perceptron, okay? So the dual of the SVM is also looking for those alphas, okay? And so it solves for alpha and then the solution it gets is gonna be the same linear combination that we used in the case of the Perceptron, okay? We are gonna take a linear combination of the data points of the yi xi and the coefficients are gonna be these alpha i alpha one through alpha n, okay? So once again, instead of solving for w we are solving for the alphas. And when we want w we are just gonna plug into this equation over here. Now interestingly, it turns out that so the alpha i are always gonna be greater than or equal to zero, and typically most of them are gonna be zero. The only time alpha I can be greater than zero is if this constraint is exactly met, which is which means that the point xi is exactly on the margin. So alpha I is only greater than zero for points xi that are exactly on the margin and those are the points that we called support vectors. Okay, so this is the dual form of the hard margin SVM. Now let's look at the soft margin SVM. So this is the familiar primal form of the soft margin SVM, and this is the dual form over here and it actually looks a whole lot like the dual for the hard margin SVM. What's the difference? Okay, so let's look back at the hard margin version. So we are maximizing this thing same as the soft margin case. We've got this constraint, same as the soft margin case. We've got this same as the soft margin case. The only additional thing over here is this upper bound. So in the case of the soft margin SVM the dual differs from the hard margin case just in this small upper bound on the alpha i, okay? At any rate, we use this dual problem to solve for the alpha i, and once again, if once we want w we just plug in to this equation over here. Now in this case, as we mentioned before the support vectors fall into two categories. We have the points that are exactly on the margin that's one type of support vector. But the other support vectors, the other points for which alpha i is non-zero are points that use slack. So all of these coefficients, alpha i are zero except for these two types of points, the points that are right on the margin and the points that are using slack. So that's the dual of the soft margin SVM. So we've seen the duals of the Perceptron the hard margin SVM, and the soft margin SVM. And the way to think about it for now is that in all of these cases, what we are doing is that we are taking this w that we want, our linear function w and instead we are writing it in a slightly different way. We are writing it as a linear combination of the data points. So the sum from i equals one to n. If there are n data points of some coefficient alpha i times yi times xi. We are writing w in this form, okay? So in the primal case, what we do is we solve for w and in the dual case what we do is to solve for alpha. Where alpha is the vector of these n numbers. Alpha one through alpha n, okay? So at a high level this is what's going on when we talk about the primal problem versus the dual problem. So far this seems like an interesting curiosity but later on this will take on larger significance because it will turn out that in order to take the Perceptron and SVM to the next level what we will need to do is to operate entirely in the dual space. 