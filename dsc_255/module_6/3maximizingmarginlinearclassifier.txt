(gentle music) (transition whooshes) - Welcome back. So we will now talk about support vector machines. Let's start by quickly recalling the perceptron algorithm. This is it in its entirety. It's an algorithm for learning a linear classifier and it's literally just these four lines of code, really easy to write up. Now, this algorithm has a strong mathematical guarantee. What's known is that, if it's given a training set that is linearly separable, it is guaranteed to converge and perfectly classify the training data. So what that means, for example, is that suppose we have a data set like this, which has four points in one class and four points in the other class, the perceptron will converge and find some linear separator that perfectly classifies the training points. And it might be, for example, this one or it might be this one, or it might be this one or one of infinitely other possibilities. So this is a very reassuring guarantee. It always finds a perfect model. But I wonder if it's possible to do something better. So these models all are perfect on the training data but are they all equally good? Perhaps it would be nice to be able to find the boundary that is the most central in some sense. Okay, could we, for example, find the separator that creates the largest margin between the classes? Indeed, we can, okay. In this case what we want is maybe a boundary like this. It's called the support vector machine. And we'll now see how to arrive at this linear classifier. Okay, so our learning problem is that, we have a bunch of data. We have a training set of endpoints, xi, yi. The data lies in d-dimensional space. The labels are either -1 or +1. And what we want to do is to find a linear model, w and b, that perfectly classifies all the data. Okay, so y times w dot x + b is always greater than zero. So y has the same sign as w dot x + b. Now we can take this condition and rewrite it a different way. Instead of asking for y times w dot x + b to be greater than zero, we can actually ask for it to be greater than or equal to one. And I'm claiming that these are actually equivalent. Hmm, in what sense? Okay, so let's look at these two conditions. Why are they equivalent? Well, certainly, if we find a model that satisfies the second condition, if we find some linear model, w and b, for which all of these y times w dot x + b are greater than or equal to one, then they're certainly greater than zero. Okay, so that direction is easy. But what about the other direction? What about showing that if we are able to satisfy this, we can also satisfy this? Okay, so let's say we have some model, w and b, that satisfies this first condition. So say we have w and b, for which yi times w dot xi + b is greater than zero. Now, this doesn't automatically mean that the second condition will be satisfied because some of these values might be less than one. Okay, so let's say that the smallest value, just for example, let's say that the smallest of these values is something less than one, maybe it's 0.1. How do we satisfy this condition? Well easy, we just multiply w and b by 10. So then 10w and 10b satisfies the second condition. So in this way, if we can find w and b that satisfies this first condition by simply scaling them up, by multiplying them by an appropriate constant, we can also satisfy this second condition. Okay and so when we talk about support vector machines, we will think of this second condition as defining what we mean by linear separability, by perfectly classifying the data points. Okay, so these two conditions are mathematically equivalent but why did we bother reformulating it in this second way? Well, it turns out that if you rewrite it in this way then it's very easy to characterize the margin between the two classes. So let's see how that works out. Okay, so here, let's say we are in d-dimensional space, w dot x + b equals zero is the decision boundary and this is gonna be our hyperplane in general, if D equals two, if we are in two-dimensional spaces, it's just a line. If D equals three, it's a plane. And in higher dimensions it's a hyperplane. Okay and so that's shown over here. What is w dot x + b equals 1? Well, that is a parallel hyperplane, so it's parallel to the decision boundary on one side of it. And what's w dot x + b equals -1? Well, that's another parallel hyperplane on the other side of the boundary. Okay, so that's what these three hyperplanes look like. We have the central hyperplane and then we have parallel hyperplanes on either side of it. Now, what this condition tells us is that all the +1 points, which are shown here as blue triangles, have to be to the right of this right hand boundary, w dot x + b equals 1. So they can either be on this margin itself or to the right of it. And all the negative points which are shown here as red circles have to either be on this left boundary or to the left of it. Now, the quantity that we are trying to maximize is the margin, which is shown here as a gamma, okay. We want to find the boundary, w dot x plus b equals 0, for which there is the largest margin, the largest gap between the boundary and the data. And so in this picture, that margin, that gap is given by this distance. So how can we compute this distance gamma? It turns out that a little bit of mathematical calculation shows that this margin is actually exactly one over the length of w, okay? And so if we want to maximize the margin, what we should do is to minimize the length of w. Okay, so here's what we will do. What we wanna do is to minimize the length of w, subject to these constraints over here, subject to the constraint that we are perfectly classifying all of the training points, okay? So previously all we wanted to do was to perfectly classify all the points. Now that's no longer enough for us. We wanna perfectly classify the points and we want to maximize the margin. And that's the same as minimizing the length of w. Now, it turns out that it's easier to deal with minimizing the squared length of w but that's the same thing. Minimizing w is the same as minimizing the squared length of w. So here then is our optimization problem. We wanna perfectly classify the training data and we wanna minimize the squared length of w while we are doing that. Minimizing the squared length of w is gonna be tantamount to maximizing the margin. Okay, so how do we solve this problem? Well, it turns out that this is a convex optimization problem. What that means is that it is actually easy to find the optimal solution to this problem. And in fact, there's a whole bunch of ready-made packages that we can use for that. Okay, so it's relatively a fairly easy problem to solve. The solution, the linear function that we get in this way, the linear classify that we get in this way is called the support vector machine or sometimes the hard margin support vector machine, okay? Why is it called that? What are support vectors? What is that referring to? Okay, so let's take a look. So it turns out that you can use some of the theory of convex optimization to characterize what the optimal solution will look like. It turns out that the solution is gonna look like this. The solution, the vector w, is gonna be a linear combination of the data points, okay? So the Xs are the training points, the Ys are just their labels, plus one or minus one and these are coefficients, you know, numbers like 0.3, 0.6. So these coefficients are all gonna be greater than or equal to zero. W is gonna be a linear combination of these data points. And moreover, it turns out that these coefficients, these alphas are mostly zero. In fact, the only data points for which they are not zero, okay? The only data points for which the alphas might not be zero are the points that are exactly on the margins. And these are the points that we call support vectors. So in this picture, we have a total of... It looks like 11 data points. Of these, four of them are exactly on the margin. Those are the four support vectors. And the solution w is a linear combination of just those four data points. The other data points do not enter into w. Okay, so w is a linear combination of these four support vectors. Okay, so this is why we call this a support vector machine. Okay, so now let's see this at work, okay? And here we are gonna run a support vector machine on the iris data set. Now, this data set has measurements from flowers corresponding to three different types of iris, iris setosa, iris versicolor and iris virginica. Okay, so three types of iris and for each flower four measurements were collected, the length and width of the petals and the length and width of the sepals. Okay, so four-dimensional data, four measurements per flower. And the goal is to take these four measurements and predict what type of iris it is. Which of these three types of iris is it? And there are 150 training points for this purpose. Now we're currently talking about binary classification. So what I did was to just pick out two of these classes. Okay, setosa and versicolor and they're shown over here. So the setosa are shown as red circles. The versicolor are shown as black triangles. And although there are four features, I just picked out two of them, the sepal width and petal width. And you can see the plot of the entire training data over here. Now, one thing you can tell at once, is that this data is linearly separable. What that means, for example, is that we could certainly run the perceptron algorithm on this data and the perceptron would definitely find a linear classifier that perfectly separated this data. For example, it might find this linear classifier or it might find this linear classifier or it might find this linear classifier, okay? It would find something that had perfect performance on the training data. Let's see what the SVM does. When we run our hard margin SVM, this is the model we get. This is the decision boundary, okay? This is the margin on one side, the margin on the other side and the margin we were able to achieve, the gamma, is this distance and this is the largest possible. There is no linear classifier that has a larger margin than this, okay? Now, in this model, there are only three support vectors. These points, there are only three training points that are exactly on the margin and therefore the final model w is a linear function of just these three support vectors. So as you can see from this picture, the support vector machine really gives us the central linear classifier. This is a very competent classifier. Okay, so that's it for the hard margin support vector machine. The one thing that we've had to assume over here is that the data is linearly separable. That's often not the case in real life. And for that, an alternative version of this, called the soft margin support vector machine was developed and that's what we'll turn to next time. 