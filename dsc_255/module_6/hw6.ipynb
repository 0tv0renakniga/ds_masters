{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a3a0cad",
   "metadata": {},
   "source": [
    "### question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0722de4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def predict(w, b, x):\n",
    "    \"\"\"\n",
    "    Predict the label for a data point using a linear classifier.\n",
    "    \n",
    "    Parameters:\n",
    "    w (numpy.ndarray): Weight vector\n",
    "    b (float): Bias term\n",
    "    x (numpy.ndarray): Data point\n",
    "    \n",
    "    Returns:\n",
    "    int: Predicted label (+1 or -1)\n",
    "    \"\"\"\n",
    "    # Calculate the dot product w·x + b\n",
    "    activation = np.dot(w, x) + b\n",
    "    \n",
    "    # Return the sign of the activation\n",
    "    return 1 if activation >= 0 else -1\n",
    "\n",
    "def perceptron_train(X, y, max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Train a binary Perceptron on the given data.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Array of data points (n_samples, n_features)\n",
    "    y (numpy.ndarray): Array of labels (+1 or -1)\n",
    "    max_iterations (int): Maximum number of iterations through the dataset\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (w, b, updates) - weight vector, bias term, and number of updates made\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0\n",
    "    \n",
    "    # Counter for updates\n",
    "    updates = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for _ in range(max_iterations):\n",
    "        # Randomly permute the data\n",
    "        X_shuffled, y_shuffled = shuffle(X, y)\n",
    "        \n",
    "        # Flag to check if any updates were made in this iteration\n",
    "        made_update = False\n",
    "        \n",
    "        # Go through all data points\n",
    "        for i in range(n_samples):\n",
    "            # Get current point and label\n",
    "            x_i = X_shuffled[i]\n",
    "            y_i = y_shuffled[i]\n",
    "            \n",
    "            # Make prediction\n",
    "            y_pred = predict(w, b, x_i)\n",
    "            \n",
    "            # Update if misclassified\n",
    "            if y_pred != y_i:\n",
    "                w = w + y_i * x_i\n",
    "                b = b + y_i\n",
    "                updates += 1\n",
    "                made_update = True\n",
    "        \n",
    "        # If no updates were made in this iteration, we've converged\n",
    "        if not made_update:\n",
    "            break\n",
    "    \n",
    "    return w, b, updates\n",
    "\n",
    "def plot_decision_boundary(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Plot the data points and the decision boundary.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Array of data points (n_samples, 2)\n",
    "    y (numpy.ndarray): Array of labels\n",
    "    w (numpy.ndarray): Weight vector\n",
    "    b (float): Bias term\n",
    "    \"\"\"\n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the data points\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', label='Class +1')\n",
    "    plt.scatter(X[y == -1, 0], X[y == -1, 1], color='red', label='Class -1')\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    # The decision boundary is where w·x + b = 0\n",
    "    # For 2D data, we can express this as x2 = (-w1*x1 - b) / w2\n",
    "    \n",
    "    # Get the min and max of x1\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    \n",
    "    # Calculate corresponding x2 values for the decision boundary\n",
    "    x2_boundary = lambda x1: (-w[0] * x1 - b) / w[1]\n",
    "    \n",
    "    # Create points for the line\n",
    "    x1_points = np.array([x1_min, x1_max])\n",
    "    x2_points = np.array([x2_boundary(x1_min), x2_boundary(x1_max)])\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.plot(x1_points, x2_points, 'g-', label='Decision Boundary')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Sepal Width (cm)')\n",
    "    plt.ylabel('Petal Width (cm)')\n",
    "    plt.title('Perceptron Decision Boundary')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('perceptron_decision_boundary.png')\n",
    "    plt.close()\n",
    "\n",
    "def run_multiple_trials(X, y, n_trials=20):\n",
    "    \"\"\"\n",
    "    Run the Perceptron algorithm multiple times and track the number of updates.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Array of data points\n",
    "    y (numpy.ndarray): Array of labels\n",
    "    n_trials (int): Number of trials to run\n",
    "    \n",
    "    Returns:\n",
    "    list: Number of updates made in each trial\n",
    "    \"\"\"\n",
    "    updates_list = []\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        _, _, updates = perceptron_train(X, y)\n",
    "        updates_list.append(updates)\n",
    "    \n",
    "    return updates_list\n",
    "\n",
    "def plot_updates_histogram(updates_list):\n",
    "    \"\"\"\n",
    "    Plot a histogram of the number of updates made by the Perceptron algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    updates_list (list): List of update counts\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(updates_list, bins=10, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Number of Updates')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Perceptron Updates (20 Trials)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('perceptron_updates_histogram.png')\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Load the Iris dataset\n",
    "    iris = datasets.load_iris()\n",
    "    X_full = iris.data\n",
    "    y_full = iris.target\n",
    "    \n",
    "    # Restrict to features 1 and 3 (sepal width and petal width)\n",
    "    X = X_full[:, [1, 3]]\n",
    "    \n",
    "    # Restrict to labels 0 and 1, and recode label 0 as -1\n",
    "    mask = (y_full == 0) | (y_full == 1)\n",
    "    X = X[mask]\n",
    "    y = y_full[mask]\n",
    "    y = np.where(y == 0, -1, 1)\n",
    "    \n",
    "    # Part (c): Train the Perceptron and plot the decision boundary\n",
    "    w, b, updates = perceptron_train(X, y)\n",
    "    print(f\"Trained Perceptron with {updates} updates\")\n",
    "    print(f\"Weight vector: {w}\")\n",
    "    print(f\"Bias term: {b}\")\n",
    "    \n",
    "    plot_decision_boundary(X, y, w, b)\n",
    "    \n",
    "    # Part (d): Run multiple trials and plot histogram\n",
    "    updates_list = run_multiple_trials(X, y)\n",
    "    print(f\"Updates across 20 trials: {updates_list}\")\n",
    "    print(f\"Average number of updates: {np.mean(updates_list):.2f}\")\n",
    "    print(f\"Standard deviation: {np.std(updates_list):.2f}\")\n",
    "    \n",
    "    plot_updates_histogram(updates_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fe631",
   "metadata": {},
   "source": [
    "### question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894e51a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def load_iris_data():\n",
    "    \"\"\"\n",
    "    Load the Iris dataset and extract features 0 and 2, and labels 1,2.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X, y) - feature matrix and labels\n",
    "    \"\"\"\n",
    "    # Load the Iris dataset\n",
    "    iris = datasets.load_iris()\n",
    "    X_full = iris.data\n",
    "    y_full = iris.target\n",
    "    \n",
    "    # Restrict to features 0 and 2 (sepal length and petal length)\n",
    "    X = X_full[:, [0, 2]]\n",
    "    \n",
    "    # Restrict to labels 1 and 2\n",
    "    mask = (y_full == 1) | (y_full == 2)\n",
    "    X = X[mask]\n",
    "    y = y_full[mask]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def check_linear_separability(X, y):\n",
    "    \"\"\"\n",
    "    Check if the data is linearly separable by training an SVM with a very large C value.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix\n",
    "    y (numpy.ndarray): Labels\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the data is linearly separable, False otherwise\n",
    "    \"\"\"\n",
    "    # Train an SVM with a very large C value (hard margin)\n",
    "    svm = SVC(kernel='linear', C=1e6)\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    # Predict on the training data\n",
    "    y_pred = svm.predict(X)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    # If accuracy is 1.0, the data is linearly separable\n",
    "    return accuracy == 1.0, accuracy\n",
    "\n",
    "def train_svm_with_different_c(X, y, c_values):\n",
    "    \"\"\"\n",
    "    Train SVM models with different C values and record training error and number of support vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix\n",
    "    y (numpy.ndarray): Labels\n",
    "    c_values (list): List of C values to try\n",
    "    \n",
    "    Returns:\n",
    "    list: List of dictionaries containing C value, training error, and number of support vectors\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for c in c_values:\n",
    "        # Train SVM with current C value\n",
    "        svm = SVC(kernel='linear', C=c)\n",
    "        svm.fit(X, y)\n",
    "        \n",
    "        # Predict on training data\n",
    "        y_pred = svm.predict(X)\n",
    "        \n",
    "        # Calculate training error\n",
    "        training_error = 1 - accuracy_score(y, y_pred)\n",
    "        \n",
    "        # Get number of support vectors\n",
    "        n_support_vectors = svm.n_support_.sum()\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'C': c,\n",
    "            'training_error': training_error,\n",
    "            'n_support_vectors': n_support_vectors\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_decision_boundary(X, y, c_value):\n",
    "    \"\"\"\n",
    "    Plot the data points and the decision boundary for a specific C value.\n",
    "    \n",
    "    Parameters:\n",
    "    X (numpy.ndarray): Feature matrix\n",
    "    y (numpy.ndarray): Labels\n",
    "    c_value (float): C value for the SVM\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Train SVM with the specified C value\n",
    "    svm = SVC(kernel='linear', C=c_value)\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the data points\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', label='Class 1 (Versicolor)')\n",
    "    plt.scatter(X[y == 2, 0], X[y == 2, 1], color='red', label='Class 2 (Virginica)')\n",
    "    \n",
    "    # Create a mesh grid to plot the decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Predict on the mesh grid\n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.contour(xx, yy, Z, colors='k', levels=[0.5, 1.5, 2.5], alpha=0.5,\n",
    "                linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # Fill the regions\n",
    "    plt.contourf(xx, yy, Z, colors=['#FFAAAA', '#AAAAFF'], alpha=0.2, levels=[0, 1, 2, 3])\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
    "                s=100, linewidth=1, facecolors='none', edgecolors='k', label='Support Vectors')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Sepal Length (cm)')\n",
    "    plt.ylabel('Petal Length (cm)')\n",
    "    plt.title(f'SVM Decision Boundary (C={c_value})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f'svm_decision_boundary_C{c_value}.png')\n",
    "    plt.close()\n",
    "\n",
    "def print_results_table(results):\n",
    "    \"\"\"\n",
    "    Print a table of results.\n",
    "    \n",
    "    Parameters:\n",
    "    results (list): List of dictionaries containing C value, training error, and number of support vectors\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(\"\\n{:<10} {:<20} {:<20}\".format('C', 'Training Error', 'Support Vectors'))\n",
    "    print(\"-\" * 50)\n",
    "    for result in results:\n",
    "        print(\"{:<10.2f} {:<20.4f} {:<20}\".format(\n",
    "            result['C'],\n",
    "            result['training_error'],\n",
    "            result['n_support_vectors']\n",
    "        ))\n",
    "\n",
    "def main():\n",
    "    # Load the Iris dataset\n",
    "    X, y = load_iris_data()\n",
    "    \n",
    "    # Part (a): Check if the data is linearly separable\n",
    "    is_separable, accuracy = check_linear_separability(X, y)\n",
    "    print(f\"Is the data linearly separable? {'Yes' if is_separable else 'No'}\")\n",
    "    print(f\"Accuracy with hard margin SVM: {accuracy:.4f}\")\n",
    "    \n",
    "    # Part (b): Train SVM with different C values\n",
    "    c_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "    results = train_svm_with_different_c(X, y, c_values)\n",
    "    \n",
    "    # Print results table\n",
    "    print_results_table(results)\n",
    "    \n",
    "    # Part (c): Find the best C value and plot decision boundary\n",
    "    # For simplicity, we'll choose the C value with the lowest training error\n",
    "    # In practice, you might want to use cross-validation\n",
    "    best_c = min(results, key=lambda x: x['training_error'])['C']\n",
    "    print(f\"\\nBest C value based on training error: {best_c}\")\n",
    "    \n",
    "    # Plot decision boundary for the best C value\n",
    "    plot_decision_boundary(X, y, best_c)\n",
    "    \n",
    "    # Also plot for a few other C values for comparison\n",
    "    plot_decision_boundary(X, y, 0.1)  # Low C\n",
    "    plot_decision_boundary(X, y, 100)  # Medium C\n",
    "    plot_decision_boundary(X, y, 10000)  # High C\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
