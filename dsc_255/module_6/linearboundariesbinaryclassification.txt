(bright music) - Okay, welcome back. We have spent a little bit of time talking about regression and conditional probability estimation. And in each case, we approach the learning problem by reformulating it as an optimization task. Now, let's circle back round to classification and bring the same optimization mindset to it. Let's start with a quick review of the geometry of linear classification. So here, we have a decision boundary in two dimensions. What is the formula for this boundary? Let's go ahead and derive that. So we have two coordinates, x1 and x2. Let's see, the y-intercept is four, and the slope of this line is minus 4/3. So the equation of the line is x2 equals minus 4/3 x1 plus four. We can rewrite this in the form 4x 1 plus 3x 2 minus 12 equals zero. Okay? And so this is the equation of this line. And what we will do is to always write the equation of a linear boundary in this form as some linear function of x equals zero. Okay, we'll always write it in the form linear function of x equals zero. That's gonna be our way of writing down a decision boundary. Now, on one side of the boundary, we are gonna predict plus, and on the other side, we'll predict minus. Okay? How exactly do we do this? Well, let's pick a point. Let's say we picked this one for example. What's that point? That is 2, 3. So let's plug 2, 3 into our linear function. Okay, so when X equals 2, 3, we have 4x 1 plus 3x 2 minus 12 equals, well, let's see, eight plus nine minus 12, which is five, which is greater than zero. And because it's greater than zero, we predict plus one. Let's look, let's do another one. This point over here is the point 1, 1. When X equals 1, 1, our linear function is 4x one plus 3x two minus 12. Okay, and that is equal to four plus three minus 12, which is negative five, which is less than zero. And therefore, at that point, we would predict minus one. So at any given point, the way we make a prediction is to just plug the point into this linear function. If the result is positive, we predict plus one. If the result is negative, we predict minus one. It's very simple. Okay, so this is in two dimensions. Let's see what this looks like when we move to greater generality, when we move to data in D dimensional space. So now we have data that lives in Rd, D dimensional vectors, and as before, the labels are binary. They're either minus one or plus one. A linear classifier is now given by a vector, W, a D dimensional vector, as well as an offset b. The form of the boundary is w . x plus b equals zero. Okay? A linear function of X equals zero. And in when we wanna make a prediction on a new point x, we simply compute the linear function, and then look at the sign of the outcome. If it's positive, we predict plus one. If it's negative, we predict minus one. Now, how can we check if our answer is correct? Well, let's say the true label is y. So we are correct if and only if one of two things happens. Either the true label is plus one, and our linear function evaluates to a positive number, or the true label is minus one. And our linear function evaluates to a negative number. These are the two possible cases. These are the two ways in which we can be correct. These are the only two ways in which we can be correct. A more compact way to write this is simply to say that the true label, times w . x plus b should be greater than zero. That is either both of these things are positive, or both of them are negative. So instead of having two conditions like we have over here, we just have a single compact condition, and we are gonna make heavy use of this particular formulation. Okay, so that's it for our quick review of the geometry of linear classification. What we'll be doing next is to use this formulation to write down one of the most beautiful and influential algorithms for linear classification, the perceptron. 