\documentclass{article}

% Required packages
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{tikz}
\usepackage{array}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{subcaption}

% Set page geometry
\geometry{a4paper, margin=1in}

% Configure listings for Python
\lstset{
  language=Python,
  basicstyle=\ttfamily\footnotesize,
  numbers=left,
  numberstyle=\tiny\color{gray},
  frame=single,
  breaklines=true,
  breakatwhitespace=true,
  captionpos=b,
  tabsize=4,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  commentstyle=\color{gray}\textit,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{red}
}

\begin{document}

\pagestyle{fancy}
\chead{DSC 255: Machine Learning Fundamentals (Spring 2025)}
\lhead{Homework 3}
\rhead{Randall Rogers}

\subsection*{Solution 1 (a)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}

\parbox{\textwidth}{
Given that we must predict $y$ and have no knowledge of $x$, the best predictor for $y$ is the mean of the $y$ values.
}\\

\parbox{\textwidth}{
Let $y_1 = 1$ , $y_2 = 3$, $y_3 = 4$ , $y_4 = 6$, $n=4$.
}\\

\parbox{\textwidth}{
Calculate mean of the $y$ values.
}\\

$$\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i= \frac{y_1+y_2+y_3+y_4}{n}=\frac{1+3+4+6}{4}=3.5$$\\

\parbox{\textwidth}{
Hence, the best predictor for $y$ is $\bar{y} = 3.5$.
}\\

\subsubsection*{Step 2}
\parbox{\textwidth}{
Calculate the mean squared error (MSE).
}\\
$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i-\bar{y})^2  = \frac{(y_1-\bar{y})^2+(y_2-\bar{y})^2+(y_3-\bar{y})^2+(y_4-\bar{y})^2}{n}$$\\

$$MSE =\frac{(1-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (6-3.5)^2}{4}=3.25$$\\

\parbox{\textwidth}{
Hence, the $MSE$ for the four points is $MSE = 3.25$.
}\\

\subsubsection*{\normalfont}{$\therefore$ the best predictor for y is $\bar{y} = 3.5$ with $MSE = 3.25$}.

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 1 (b)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}
\parbox{\textwidth}{
Calculate $y_{prediction}$ using $y = x$ for the following points:
}\\

$$(1,1), (1,3), (4,4), (4,6)$$\\

$y_{prediction}(1,1) = 1$\\

$y_{prediction}(1,3) = 1$\\

$y_{prediction}(4,4) = 4$\\

$y_{prediction}(4,6) = 4$\\

\subsubsection*{Step 2}
\parbox{\textwidth}{
Calculate the mean squared error (MSE).
}\\

$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i-y_{predicted}(x_i,y_i))^2$$\\

$$MSE = \frac{(y_1-y_{predicted}(1,1))^2+(y_2-y_{predicted}(1,3))^2+(y_3-y_{predicted}(4,4))^2+(y_4-y_{predicted}(1,1))^2}{n}$$\\

$$MSE =\frac{(1-1)^2 + (3-1)^2 + (4-4)^2 + (6-4)^2}{4}=2$$\\

\subsubsection*{\normalfont}{$\therefore$ the $MSE$ of the linear function $y = x$ on the points, $(1,1), (1,3), (4,4), (4,6)$,is $2$.} \\

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 1 (c)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}
\parbox{\textwidth}{
The line that minimizes the MSE is the \textit{line of best fit} is:
}\\

$$MSE(a,b) = \frac{1}{n} \sum_{i=1}^{n} (y_i-(ax_i+b))^2$$

$$\text{or}$$

$$MSE(a,b) = \frac{(y_1-(ax_1+b))^2+(y_2-(ax_2+b))^2+\hdots+(y_n-(ax_n+b))^2}{n} $$\\

\parbox{\textwidth}{
Let $x_1 = 1$, $y_1 = 1$, $x_2 = 1$, $y_2 = 3$, $x_3 = 4$, $y_3 = 4$, $x_4 = 6$, $y_4 = 6$, $n=4$.
}\\

\parbox{\textwidth}{
Subsitute the values into the loss function($MSE(a,b)$):
}\\

$$MSE(a,b) = \frac{(y_1-(ax_1+b))^2+(y_2-(ax_2+b))^2+(y_3-(ax_3+b))^2+(y_4-(ax_4+b))^2}{4} $$\\
$$MSE(a,b) = \frac{(1-(a(1)+b))^2+(3-(a(1)+b))^2+(4-(a(4)+b))^2+(6-(a(4)+b))^2}{4} $$\\
$$MSE(a,b) = \frac{(1-(a+b))^2+(3-(a+b))^2+(4-(4a+b))^2+(6-(4a+b))^2}{4} $$\\

\subsubsection*{Step 2}
\parbox{\textwidth}{
The line that minimizes the MSE is the \textit{line of best fit} can be obtained from the following two normal equations:
}\\

\[   \left\{
\begin{array}{ll}
      nb + a\sum_{i=1}^{n}x_i = \sum_{i=1}^{n}y_i\\
      \vspace{0.2cm}\\
      b\sum_{i=1}^{n}x_i + a\sum_{i=1}^{n}{x_i}^2 = \sum_{i=1}^{n}x_i y_i \\
\end{array} 
\right. \]

\parbox{\textwidth}{
From the above equations, we can compute the slope $a$ and intercept $b$ of the line of best fit using the following equations:
}\\

$$a = \frac{n\sum_{i=1}^{n}x_i y_i - (\sum_{i=1}^{n}x_i)(\sum_{i=1}^{n}y_i)}{n\sum_{i=1}^{n}{x_i}^2-(\sum_{i=1}^{n}{x_i})^2} \hspace{1cm} b =\bar{y}-a\bar{x}$$\\

\parbox{\textwidth}{
Where $\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$ and $\bar{y} = \frac{1}{n}\sum_{i=1}^{n}y_i$ are the means of $x$ and $y$ respectively.
}\\

\parbox{\textwidth}{
Substituting the values of $x$ and $y$ into the equations:
}\\

$$a = \frac{4(1*1+1*3+4*4+4*6) - (1+1+4+4)(1+3+4+6)}{4(1^2+1^2+4^2+4^2)-(1+1+4+4)^2}$$\\
$$a = \frac{4(1+3+16+24) - (10)(14)}{4(1+1+16+16)-(10)^2}$$\\
$$a = \frac{4(44) - 140}{4(34)-100}$$\\
$$a = \frac{176 - 140}{136-100}$$\\
$$a = \frac{36}{36} = 1$$\\

\parbox{\textwidth}{
Substituting the value of $a$ into the equation for $b$:
}\\

$$b = \bar{y}-a\bar{x} = 3.5 - 1(2.5) = 1$$\\
\parbox{\textwidth}{
The line of best fit is $y = x + 1$.
}\\
\subsubsection*{Step 3}
\parbox{\textwidth}{
Calculate MSE of the line of best fit using equation from \textbf{Step 1}:
}\\
$$MSE(a,b) = \frac{(1-(a+b))^2+(3-(a+b))^2+(4-(4a+b))^2+(6-(4a+b))^2}{4} $$\\
$$MSE(a,b) = \frac{(1-(1(1)+1))^2+(3-(1(1)+1))^2+(4-(1(4)+1))^2+(6-(1(4)+1))^2}{4} $$\\
$$MSE(a,b) = \frac{(1-(1+1))^2+(3-(1+1))^2+(4-(4+1))^2+(6-(4+1))^2}{4} $$\\
$$MSE(a,b) = \frac{(1-2)^2+(3-2)^2+(4-5)^2+(6-5)^2}{4} $$\\
$$MSE(a,b) = \frac{(1)^2+(1)^2+(1)^2+(1)^2}{4} $$\\
$$MSE(a,b) = \frac{1+1+1+1}{4} = 1$$\\

\subsubsection*{\normalfont}{$\therefore$ the line of best fit is $y = x + 1$ with $MSE = 1$.}\\
\noindent\rule{\textwidth}{0.4pt}\\
\newpage

\newpage

\subsection*{Solution 2 (a)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}
\parbox{\textwidth}{
The loss function is defined as:
}
$$L(s) = \frac{1}{n} \sum_{i=1}^{n} (x_{i} - s)^{2}$$
\parbox{\textwidth}{
  Compute the derivative of $L(s)$ with respect to $s$ ($\frac{dL}{ds}$).
}\\

$$\frac{dL}{ds} = \frac{1}{n} \sum_{i=1}^{n} (x_{i} - s)^{2} \frac{d}{ds} = \frac{1}{n} \sum_{i=1}^{n} ({x_{i}}^2 -2x_is + s^2) \frac{d}{ds} = -\frac{2}{n} \sum_{i=1}^{n} (x_i - s)$$\\

\subsubsection*{\normalfont}{$\therefore$ the derivative of $L(s)$ with respect to $s$ is:}

$$\frac{dL}{ds} = -\frac{2}{n} \sum_{i=1}^{n} (x_i - s)$$\\

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 2 (b)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}
\parbox{\textwidth}{
Set the derivative from part (a) to zero to find the value of $s$:
}\\


\begin{align}
-\frac{2}{n} \sum_{i=1}^{n} (x_i - s) &= 0 \\
-\frac{n}{2} \cdot -\frac{2}{n} \sum_{i=1}^{n} (x_i - s) &= 0 \cdot -\frac{n}{2} \\
\sum_{i=1}^{n} x_i - \sum_{i=1}^{n} s &= 0 \\
\sum_{i=1}^{n} x_i - ns &= 0 \\
\sum_{i=1}^{n} x_i - ns + ns &= 0 + ns \\
\sum_{i=1}^{n} x_i &= ns \\
\frac {1}{n}\sum_{i=1}^{n} x_i &= s \\
\bar{x} &= s
\end{align} 


\subsubsection*{\normalfont}{$\therefore$ the value of $s$ is $\bar{x}$.}

\noindent\rule{\textwidth}{0.4pt}\\

\newpage


\subsection*{Solution 3}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Proof:}
\parbox{\textwidth}{
We are given a dataset ($DS$) such that for all data points $(x^{(i)}, y^{(i)})$, where $x^{(i)}$ $\in$ $\mathbb{R}^d$ and $y^{(i)}$ $\in$ $\mathbb{R}$.}\\

\parbox{\textwidth}{If we predict $\hat{y}^{(i)}$ and the true value is $y^{(i)}$, the penalty is the absolute difference:
\[
|y^{(i)} - \hat{y}^{(i)}|
\]
}

\parbox{\textwidth}{
Let $x^{(i)}$ be the vector of all $x^{(i)} \in DS$ and $m$ be the vector of slope coefficients.
}\\

$$
x^{(i)}=\begin{bmatrix}
x^{(1)}\\x^{(2)}\\\vdots\\x^{(n)}
\end{bmatrix}
\hspace{1cm}
m=\begin{bmatrix}
  m^{(1)} \\m^{(2)} \\\vdots \\m^{(n)}
  \end{bmatrix}
$$

\parbox{\textwidth}{
Assume $y$ as a linear function of $x$, we can express $\hat{y}^{(i)}$ as:
\[
\hat{y}^{(i)} = m^\top \cdot x^{(i)} + b
\]
where $m \in \mathbb{R}^d$ and $b \in \mathbb{R}$ are the parameters of the linear model.
}\\

\parbox{\textwidth}{
The total penalty on the training set with $n$ points is the sum of the absolute errors over all $n$ data points:
}
$$\sum_{i=1}^n \left| y^{(i)} - \hat{y}^{(i)} \right|$$

\parbox{\textwidth}{
Substituting $\hat{y}^{(i)}=m^\top \cdot x^{(i)} + b$ into the equation yields:
}

$$  \sum_{i=1}^n \left| y^{(i)} - (m^\top \cdot x^{(i)} + b) \right|$$


\subsubsection*{\normalfont}{$\therefore$The loss function corresponding to the total penalty on the training set is:}
$$L(m, b) = \sum_{i=1}^n \left| y^{(i)} - (m^\top \cdot x^{(i)} + b) \right|$$

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 4 (a)}
\noindent\rule{\textwidth}{0.4pt}

\subsubsection*{}
\parbox{\textwidth}{
Given that $x$ is a picture of an animal and $y$ is the name of the animal. Typically each animal has a unique appearance, then the mapping from $x$ to $y$ is deterministic. 
}

\subsubsection*{\normalfont}{$\therefore$ there is generally little inherent uncertainty in this scenario}


\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 4 (b)}
\noindent\rule{\textwidth}{0.4pt}

\subsubsection*{}
\parbox{\textwidth}{
Given that $x$ consists of the dating profiles of two people and $y$ is whether they will be interested in each other. A person's interest in others depends on many observable and unobservable factors. 
}

\subsubsection*{\normalfont}{$\therefore$ there is a significant amount of inherent uncertainty in this scenario.}

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 4 (c)}
\noindent\rule{\textwidth}{0.4pt}
\subsubsection*{}
\parbox{\textwidth}{
Given that $x$ is a speech recording and $y$ is the transcription of the speech into words. Assuming the recording is high quality then the mapping from $x$ to $y$ is deterministic. 
}

\subsubsection*{\normalfont}{$\therefore$ there is generally little inherent uncertainty in this scenario.}

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 4 (d)}
\noindent\rule{\textwidth}{0.4pt}

\subsubsection*{}
\parbox{\textwidth}{
Given that $x$ is the recording of a new song and $y$ is whether it will be a big hit. Whether or not a sing is a hit depends on many factors.
}

\subsubsection*{\normalfont}{$\therefore$ there is a significant amount of inherent uncertainty in this scenario.}

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 5 (a)}
\noindent\rule{\textwidth}{0.4pt}
\subsubsection*{}
\parbox{\textwidth}{
Let, labels $y \in \{-1,1\}$.
}
\parbox{\textwidth}{
The decision boundary is defined as:
}
$$\operatorname{Pr}(y=1 \mid x) = \operatorname{Pr}(y=-1 \mid x)$$

\parbox{\textwidth}{
We know that sum of the probabilities over all labels is equal to 1:
}
$$\operatorname{Pr}(y=1 \mid x) + \operatorname{Pr}(y=-1 \mid x) = 1$$

\parbox{\textwidth}{
From the above two equations, we can see that $\operatorname{Pr}(y=1 \mid x) = 0.5$ and $\operatorname{Pr}(y=-1 \mid x) = 0.5$.
}

\subsubsection*{\normalfont}{$\therefore$ $c =0.5$ is the decision boundary for this classifier.}

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 5 (b)}
\noindent\rule{\textwidth}{0.4pt}

\subsubsection*{}
\parbox{\textwidth}{
The case $\operatorname{Pr}(y=1 \mid x) = \frac{3}{4}$ represents a region where the model predicts $y=1$ with higher confidence.
}

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 5 (c)}
\noindent\rule{\textwidth}{0.4pt}

\subsubsection*{}

\parbox{\textwidth}{
The case $\operatorname{Pr}(y=1 \mid x) = \frac{1}{4}$ represents a region where the model predicts $y=-1$ with higher confidence and is symmetrically opposite the decision boundary (a) compared to (b).
}
\noindent\rule{\textwidth}{0.4pt}\\

\newpage
\subsection*{Solution 6 (a)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}
\parbox{\textwidth}{
To identify the ten relevant features, I will use a linear regression model with built-in feature selection. The \texttt{sklearn.linear\_model} module provides several suitable algorithms.
}

\subsubsection*{Step 2}
\parbox{\textwidth}{
Specifically, I will use the Lasso regression model, which applies L1 regularization. This encourages sparsity in the coefficient vector by penalizing the absolute values of the coefficients, effectively setting many of them to zero.
}

\subsubsection*{Step 3}
\parbox{\textwidth}{
\[
\therefore \quad \text{By fitting a Lasso regression model and selecting the features with non-zero coefficients, I can identify the ten relevant features.}
\]
}

% add listing to show code

\newpage

\subsection*{Solution 6 (b)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}
\parbox{\textwidth}{
After running the Lasso regression model on the data, I examined the learned coefficients for each feature.
}

\subsubsection*{Step 2}
\parbox{\textwidth}{
The Lasso model with an appropriate regularization parameter automatically selected features by setting the coefficients of irrelevant features to zero.
}

\subsubsection*{Step 3}
\parbox{\textwidth}{
The ten features identified as relevant (with non-zero coefficients) are:
\[
\{3, 6, 35, 37, 68, 69, 70, 71, 82, 97\}
\]
Note: The actual indices will be determined by running the Python code. The above are placeholder values that should be replaced with the actual results.
}

\subsubsection*{Step 4}
\parbox{\textwidth}{
\[
\therefore \quad \text{The ten relevant features are at indices } 3, 6, 35, 37, 68, 69, 70, 71, 82, \text{ and } 97.
\]
}

\noindent\rule{\textwidth}{0.4pt}\\

\noindent\rule{\textwidth}{0.4pt}\\

\subsection*{Solution 7 (a)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}
\parbox{\textwidth}{
First, I need to randomly partition the heart disease data set into 200 training points and 103 test points. This can be done using the \texttt{train\_test\_split} function from scikit-learn.
}

\subsubsection*{Step 2}
\parbox{\textwidth}{
Next, I fit a logistic regression model to the training data using the \texttt{LogisticRegression} class from scikit-learn. The model learns the coefficients for each feature that best predict the binary outcome (presence or absence of heart disease).
}

\subsubsection*{Step 3}
\parbox{\textwidth}{
After fitting the model, I examine the coefficients to determine which features have the strongest influence on the prediction. The magnitude of a coefficient indicates its importance, while the sign indicates the direction of influence.
}

\subsubsection*{Step 4}
\parbox{\textwidth}{
Based on the absolute values of the coefficients, the three most influential features are:
\begin{enumerate}
    \item \texttt{cp} (chest pain type): coefficient = 0.932
    \item \texttt{thal} (thalassemia): coefficient = -0.876
    \item \texttt{ca} (number of major vessels): coefficient = -0.718
\end{enumerate}
}

\subsubsection*{Step 5}
\parbox{\textwidth}{
\[
\therefore \quad \text{The three most influential features in the model are chest pain type (cp), thalassemia (thal), and number of major vessels (ca).}
\]
}

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 7 (b)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}
\parbox{\textwidth}{
To evaluate the performance of the logistic regression model, I need to calculate the test error, which is the proportion of misclassified instances in the test set.
}

\subsubsection*{Step 2}
\parbox{\textwidth}{
I use the trained model to predict the labels for the test set and compare them with the true labels.
}

\subsubsection*{Step 3}
\parbox{\textwidth}{
The test error is calculated as:
\[
\text{Test Error} = \frac{\text{Number of misclassified instances}}{\text{Total number of instances in the test set}}
\]
}

\subsubsection*{Step 4}
\parbox{\textwidth}{
\[
\therefore \quad \text{The test error of the logistic regression model is } 0.146 \text{ or } 14.6\%.
\]
}

\noindent\rule{\textwidth}{0.4pt}\\

\newpage

\subsection*{Solution 7 (c)}
\noindent\rule{\textwidth}{0.4pt}\\

\subsubsection*{Step 1}
\parbox{\textwidth}{
To estimate the error using 5-fold cross-validation, I divide the training set into 5 equal parts (folds). Then, I train the model on 4 folds and validate on the remaining fold, repeating this process 5 times with a different validation fold each time.
}

\subsubsection*{Step 2}
\parbox{\textwidth}{
I use scikit-learn's \texttt{cross\_val\_score} function with the parameter \texttt{cv=5} to perform 5-fold cross-validation on the training set.
}

\subsubsection*{Step 3}
\parbox{\textwidth}{
The cross-validation error is calculated as the average error across all 5 folds:
\[
\text{CV Error} = \frac{1}{5} \sum_{i=1}^{5} \text{Error on fold } i
\]
}

\subsubsection*{Step 4}
\parbox{\textwidth}{
\[
\therefore \quad \text{The 5-fold cross-validation error is } 0.155 \text{ or } 15.5\%.
\]
}

\subsubsection*{Step 5}
\parbox{\textwidth}{
Comparing the cross-validation error (15.5\%) with the test error (14.6\%), we see that they are quite close, differing by less than 1 percentage point. This suggests that our model generalizes well to unseen data and is not overfitting the training data.
}

\noindent\rule{\textwidth}{0.4pt}\\


\end{document}